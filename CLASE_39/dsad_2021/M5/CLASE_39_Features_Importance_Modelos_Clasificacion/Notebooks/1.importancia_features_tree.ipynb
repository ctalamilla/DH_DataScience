{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PLkbZSX3H_s"
   },
   "source": [
    "# Importancia de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "\n",
    "## Tabla de Contenidos\n",
    "\n",
    "[Intro](#section_intro)\n",
    "\n",
    "[Dataset](#section_dataset)\n",
    "\n",
    "[Imports](#section_imports)\n",
    "\n",
    "[Importancia de features en un Árbol de decisión](#section_feature_importance_tree)\n",
    "\n",
    "[Importancia de features en modelos de Ensamble](#section_feature_importance_ensamble)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a> \n",
    "## Intro\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "Como científicos de datos, a menudo nos centramos en optimizar el rendimiento del modelo. \n",
    "\n",
    "Sin embargo, a veces eso es tan importante como comprender cómo las features de nuestro modelo contribuyen a la predicción. \n",
    "\n",
    "En esta práctica veremos cómo determinar la importancia de features en modelos CART y de ensamble.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_dataset\"></a> \n",
    "## Dataset\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Para ejemplificar el cálculo de importancia de features en modeos CART y de ensamble vamos a usar los datos de Car Evaluation Data Set disponibles en \n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/car+evaluation\n",
    "\n",
    "https://www.kaggle.com/elikplim/car-evaluation-data-set\n",
    "\n",
    "La variable target (acceptability) en este dataset tiene cuatro valores posibles:\n",
    "\n",
    "* unacc\n",
    "* acc\n",
    "* good\n",
    "* vgood\n",
    "\n",
    "Usando árboles de decisión y ensambles vamos a construir modelos que intenten predecir el valor de acceptability y vamos a evaluar la importancia de las features en cada uno de los modelos construídos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_imports\"></a> \n",
    "## Imports\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from IPython.display import Image\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oN-DY5bt3H_w"
   },
   "source": [
    "Vamos a comenzar con la lectura del dataset de aceptabilidad de autos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StupZU7-3H_y",
    "outputId": "4eb1e51f-205f-45d7-e5a9-24f8d49991a6"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/car.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores posibles de la variable target son 4, cada uno en la siguiente proporción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.acceptability.value_counts() / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JMXMfpMA3H_7"
   },
   "source": [
    "Esta vez codificaremos los features usando un esquema de codificación One Hot, es decir, los consideraremos como variables categóricas.\n",
    "\n",
    "Debido a que Scikit-Learn no entiende los strings, sólo números, también necesitaremos asignarle números a las etiquetas. Para eso usaremos el `LabelEncoder`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5NGPBEU3H_8",
    "outputId": "7154c0b1-4860-4b04-ef36-a19801ba8293"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['acceptability'])\n",
    "X = pd.get_dummies(df.drop('acceptability', axis=1))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTd4FgoB3H__"
   },
   "source": [
    "Vamos a entrenar un árbol de decisión. También vamos a limitar artificialmente el árbol a ser pequeño para que podamos visualizarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEAUGnLc3WvX"
   },
   "outputs": [],
   "source": [
    "# partimos en entrenamiento-prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_feature_importance_tree\"></a> \n",
    " ## Importancia de features en un Árbol de decisión\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4Z_98OY3IAC",
    "outputId": "93554e1d-dc48-4be4-8e1e-e53f2b699c63"
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 3, min_samples_split = 2, random_state = 11)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lldsqFH_3IAF"
   },
   "source": [
    "Ahora visualicemos el árbol usando el exportador de graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QSw3Hxuw3IAG",
    "outputId": "74fa4147-2a98-47a3-9634-9791492aeb71"
   },
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(dt, out_file=None,  \n",
    "                    feature_names=X.columns,  \n",
    "                    class_names=le.classes_,  \n",
    "                    filled=True, rounded=True,\n",
    "                    proportion=True,\n",
    "                    special_characters=True)  \n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Veamos qué información nos da este gráfico**\n",
    "\n",
    "Cada nodo que no es una hoja divide los datos en dos hijos. \n",
    "\n",
    "El nodo raíz contiene todos los datos (conjunto de entrenamiento).\n",
    "\n",
    "**Nodo raíz**:\n",
    "\n",
    "samples = 100% significa que el nodo contiene el total del conjunto de entrenamiento.\n",
    "\n",
    "valor = [0.224, 0.041, 0.701, 0.034] son las probabilidades de cada clase (acc, good, unacc, vgood)\n",
    "\n",
    "class indica la clase con probabilidad máxima en ese nodo (el máximo elemento de valor)\n",
    "\n",
    "gini = 0.456 es la impureza de gini del nodo. Describe cuánto se mezclan las clases.\n",
    "\n",
    "persons_2 <= 0.5 significa que el nodo se divide de modo que todas las muestras en las que el valor de persons_2 sea inferior o igual a 0.5 se dirijen al elemento secundario izquierdo y las muestras en las que el valor de la feature es superior a 0.5 se dirijen al elemento secundario derecho.\n",
    "\n",
    "**Nodos hoja**:\n",
    "\n",
    "Estos nodos no se dividen más, por lo que no hay necesidad de una condición.\n",
    "\n",
    "El valor de gini indica el nivel de impurezas restantes en los nodos hoja. Son más bajos que en el nodo padre, lo que significa que la división mejoró la separabilidad entre las clases, pero todavía queda algo de incertidumbre.\n",
    "\n",
    "**Split**\n",
    "\n",
    "Los nodos se dividen usando como condición aquella que minimice el promedio pesado de los scores Gini de los nodos hijos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-emVPJcV3IAS"
   },
   "source": [
    "---\n",
    "\n",
    "Ahora veamos la importancia de los features del árbol generado.\n",
    "Para lo cual utilizaremos la property `feature_importances_` de `DecisionTreeClassifier`\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_\n",
    "\n",
    "La importancia de features de `DecisionTreeClassifier` se calcula como la importancia de Gini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180"
   },
   "outputs": [],
   "source": [
    "importancia_features = pd.DataFrame(dt.feature_importances_, index = X_train.columns, columns=['importancia'])\n",
    "\n",
    "importancia_features_sort = importancia_features.sort_values('importancia', ascending=False)\n",
    "\n",
    "importancia_features_sort[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifiquemos que `dt.feature_importances_` nos devuelve la reducción total normalizada, debida a cada feature, del valor de Gini \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganancia_gini_persons_2    = 1.000 * 0.4576 - 0.651 * 0.583 - 0.349 * 0.0000\n",
    "ganancia_gini_safety_low   = 0.651 * 0.583 - 0.438 * 0.623 - 0.213 * 0.0000\n",
    "ganancia_gini_buying_vhigh = 0.438 * 0.623 - 0.333 * 0.612 - 0.108 * 0.456\n",
    "\n",
    "norm = ganancia_gini_persons_2 + ganancia_gini_safety_low + ganancia_gini_buying_vhigh\n",
    "\n",
    "print (\"persons_2:\", ganancia_gini_persons_2 / norm)\n",
    "print (\"safety_low:\", ganancia_gini_safety_low / norm)\n",
    "print (\"buying_vhigh:\", ganancia_gini_buying_vhigh / norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la feature de mayor importancia que nos devuelve `feature_importances_` no coincide con el nodo raíz de nuestro modelo, veamos por qué:\n",
    "\n",
    "Eligiendo como nodo raiz safety_low, la reducción en ese paso es menor que la que obtenemos con persons_2 como raíz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad_ocurrencias_clases = pd.Series(y_train).value_counts()\n",
    "total_observaciones = sum(cantidad_ocurrencias_clases )\n",
    "\n",
    "proporciones_clases = cantidad_ocurrencias_clases / total_observaciones\n",
    "\n",
    "print (\"Proporciones Clases\")\n",
    "print (proporciones_clases)\n",
    "\n",
    "gini_root = 1 - sum( proporciones_clases ** 2 )\n",
    "print (\"Gini root: \", gini_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la reducción del valor de Gini que obtenemos usando safety_low <= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_low_mask = X_train.safety_low <= 0.5\n",
    "#sum(safety_low_mask)\n",
    "\n",
    "y_train_left = y_train[safety_low_mask]\n",
    "y_train_right = y_train[np.logical_not(safety_low_mask)]\n",
    "\n",
    "prop_ocurrencias_clases_left = pd.Series(y_train_left).value_counts() / sum(safety_low_mask)\n",
    "print(\"proporciones clases left: \\n\", prop_ocurrencias_clases_left)\n",
    "\n",
    "prop_ocurrencias_clases_right = pd.Series(y_train_right).value_counts() / sum(np.logical_not(safety_low_mask))\n",
    "print(\"proporciones clases right: \\n\", prop_ocurrencias_clases_right)\n",
    "\n",
    "prop_left = sum(safety_low_mask) / len(y_train)\n",
    "prop_right = sum(np.logical_not(safety_low_mask)) / len(y_train)\n",
    "\n",
    "print(\"prop left \", prop_left)\n",
    "print(\"prop rigth \",prop_right)\n",
    "\n",
    "proporciones_clases = cantidad_ocurrencias_clases / total_observaciones\n",
    "\n",
    "print (\"Proporciones Clases\")\n",
    "print (proporciones_clases)\n",
    "\n",
    "gini_left = 1 - sum( prop_ocurrencias_clases_left ** 2 )\n",
    "print (\"Gini left: \", gini_left)\n",
    "\n",
    "\n",
    "gini_right = 1 - sum( prop_ocurrencias_clases_right ** 2 )\n",
    "print (\"Gini right: \", gini_right)\n",
    "\n",
    "\n",
    "ganancia = gini_root - gini_left * prop_left - gini_right * prop_right\n",
    "print(\"ganancia safety_low: \", ganancia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la reducción del valor de Gini que obtenemos usando persons_2 <= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons2_mask = X_train.persons_2 <= 0.5\n",
    "#sum(safety_low_mask)\n",
    "\n",
    "y_train_left = y_train[persons2_mask]\n",
    "y_train_right = y_train[np.logical_not(persons2_mask)]\n",
    "\n",
    "prop_ocurrencias_clases_left = pd.Series(y_train_left).value_counts() / sum(persons2_mask)\n",
    "print(\"proporciones clases left: \\n\", prop_ocurrencias_clases_left)\n",
    "\n",
    "prop_ocurrencias_clases_right = pd.Series(y_train_right).value_counts() / sum(np.logical_not(persons2_mask))\n",
    "print(\"proporciones clases right: \\n\", prop_ocurrencias_clases_right)\n",
    "\n",
    "prop_left = sum(persons2_mask) / len(y_train)\n",
    "prop_right = sum(np.logical_not(persons2_mask)) / len(y_train)\n",
    "\n",
    "print(\"prop left \", prop_left)\n",
    "print(\"prop rigth \",prop_right)\n",
    "\n",
    "proporciones_clases = cantidad_ocurrencias_clases / total_observaciones\n",
    "\n",
    "print (\"Proporciones Clases\")\n",
    "print (proporciones_clases)\n",
    "\n",
    "gini_left = 1 - sum( prop_ocurrencias_clases_left ** 2 )\n",
    "print (\"Gini left: \", gini_left)\n",
    "\n",
    "\n",
    "gini_right = 1 - sum( prop_ocurrencias_clases_right ** 2 )\n",
    "print (\"Gini right: \", gini_right)\n",
    "\n",
    "\n",
    "ganancia = gini_root - gini_left * prop_left - gini_right * prop_right\n",
    "print(\"ganancia persons_2: \", ganancia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, vemos que usando la condición safety_low <= 0.5 la reducción es del 6.8% y usando persons_2 <= 0.5 la reducción es del 7.6%\n",
    "\n",
    "Por eso aunque en `feature_importances_` `safety_low` tiene más importancia que `persons_2`, la segunda produce una ganancia mayor en el primero split y por eso el algoritmo la definió como raiz.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_feature_importance_ensamble\"></a> \n",
    "## Importancia de features en modelos de Ensamble\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "Vamos a construir ahora dos modelos en ensamble para resolver el mismo problema, y calcular la importancia de las features en esos modelos.\n",
    "\n",
    "Comencemos con con un modelo random forest\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos random forest exponen la importancia de los features que es calculada como la media de la importancia de los features de los árboles base. Vamos a verificar eso.\n",
    "\n",
    "Podemos acceder a los árboles base que componen el ensamble con  la property `estimators_` \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Para calcular la importancia de features de cada uno de los árboles base podemos usar una lista por comprensión:\n",
    "\n",
    "`[tree.feature_importances_ for tree in rf.estimators_]`\n",
    "\n",
    "Cada elemento de esta lista es otra lista donde cada uno de sus elementos es la importancia de esa variable en ese árbol.\n",
    "\n",
    "Vamos a construir un DataFrame que donde cada columna sea una de las variables predictoras de un árbol base, y cada fila sea un árbol base.\n",
    "\n",
    "Luego, calculamos la media por columna y obtenemos la importancia de cada feature en el modelo de ensamble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancias = rf.feature_importances_\n",
    "\n",
    "trees_feature_importance = [tree.feature_importances_ for tree in rf.estimators_]\n",
    "\n",
    "print(\"Cantidad de features en un árbol: \", len(trees_feature_importance[0]))\n",
    "\n",
    "importancia_trees_base = pd.DataFrame(data = trees_feature_importance)\n",
    "#print(importancia_trees_base.shape)\n",
    "#print(importancia_trees_base.head())\n",
    "\n",
    "feature_importance_calc = np.mean(importancia_trees_base, axis = 0)\n",
    "\n",
    "print(\"Importancia de features calculada: \", feature_importance_calc.values)\n",
    "\n",
    "# veamos si coinciden el cálculo feature_importance_calc con el resultado de feature_importances_ como atributo del ensamble:\n",
    "\n",
    "print(\"Coinciden? \", all(importancias == feature_importance_calc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos ahora el desvío estandar de la importancia de cada feature en el ensamble, usando el DataFrame `importancia_trees_base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_calc_std = np.std(importancia_trees_base, axis = 0)\n",
    "\n",
    "print(feature_importance_calc_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos los valores calculados para media y desvío estandar de la importancia de cada feature en el ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opcional:asigno como indice el nombre de las features antes de ordenar \n",
    "# porque coincide el orden de las columnas de train con el de feature_importance_calc y feature_importance_calc_std\n",
    "feature_importance_calc.index = X_train.columns\n",
    "feature_importance_calc_std.index = X_train.columns\n",
    "\n",
    "# oredenamos por importancia decreciente\n",
    "feature_importance_calc_sort = feature_importance_calc.sort_values(ascending=False)\n",
    "\n",
    "# ordenamos los desvíos con el orden definido por las importancias medias:\n",
    "orden_definido = feature_importance_calc_sort.index\n",
    "feature_importance_calc_std_sort = feature_importance_calc_std[orden_definido]\n",
    "\n",
    "# cantidad de features:\n",
    "cant_features = X_train.shape[1]\n",
    "nombres_features = feature_importance_calc_sort.index\n",
    "\n",
    "#graficamos medias y desvios como barras:\n",
    "plt.figure()\n",
    "plt.title(\"Importancia de los features en RandomForest\")\n",
    "plt.bar(range(cant_features), feature_importance_calc_sort, color = \"r\", yerr = feature_importance_calc_std_sort, align = \"center\")\n",
    "plt.xticks(range(cant_features), nombres_features, rotation=90)\n",
    "plt.xlim([-1, cant_features])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora repetimos el cálculo de la importancia de features en otro modelo de ensamble `ExtraTreesClassifier`\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "et.fit(X_train, y_train)\n",
    "\n",
    "importancias_et = et.feature_importances_\n",
    "\n",
    "trees_feature_importance_et = [tree.feature_importances_ for tree in et.estimators_]\n",
    "\n",
    "print(\"Cantidad de features en un árbol: \", len(trees_feature_importance_et[0]))\n",
    "\n",
    "importancia_trees_base_et = pd.DataFrame(data = trees_feature_importance_et)\n",
    "\n",
    "feature_importance_calc_et = np.mean(importancia_trees_base_et, axis = 0)\n",
    "\n",
    "print(\"Importancia de features calculada: \", feature_importance_calc_et.values)\n",
    "\n",
    "# veamos si coinciden el cálculo feature_importance_calc con el resultado de feature_importances_ como atributo del ensamble:\n",
    "\n",
    "print(\"Coinciden? \", all(importancias_et == feature_importance_calc_et))\n",
    "\n",
    "feature_importance_calc_std_et = np.std(importancia_trees_base_et, axis = 0)\n",
    "\n",
    "#print(feature_importance_calc_std)\n",
    "\n",
    "# opcional:asigno como indice el nombre de las features antes de ordenar \n",
    "# porque coincide el orden de las columnas de train con el de feature_importance_calc y feature_importance_calc_std\n",
    "feature_importance_calc_et.index = X_train.columns\n",
    "feature_importance_calc_std_et.index = X_train.columns\n",
    "\n",
    "# oredenamos por importancia decreciente\n",
    "feature_importance_calc_et_sort = feature_importance_calc_et.sort_values(ascending=False)\n",
    "\n",
    "# ordenamos los desvíos con el orden definido por las importancias medias:\n",
    "orden_definido = feature_importance_calc_et_sort.index\n",
    "feature_importance_calc_std_et_sort = feature_importance_calc_std_et[orden_definido]\n",
    "\n",
    "# cantidad de features:\n",
    "cant_features = X_train.shape[1]\n",
    "nombres_features = feature_importance_calc_et_sort.index\n",
    "\n",
    "#graficamos medias y desvios como barras:\n",
    "plt.figure()\n",
    "plt.title(\"Importancia de los features en ExtraTrees\")\n",
    "plt.bar(range(cant_features), feature_importance_calc_et_sort, color = \"r\", yerr = feature_importance_calc_std_et_sort, align = \"center\")\n",
    "plt.xticks(range(cant_features), nombres_features, rotation=90)\n",
    "plt.xlim([-1, cant_features])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente comparemos los 3 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancias = pd.DataFrame(\n",
    "    {'imp Árbol Decisión': dt.feature_importances_,\n",
    "     'imp Random Forest': rf.feature_importances_,\n",
    "     'imp Extra Trees': et.feature_importances_}).sort_values(['imp Random Forest'], ascending=False)\n",
    "\n",
    "# (opcional) hago esto para que aparezcan los nombres de las features en lufar del índice en columns\n",
    "feature_names = X_train.columns[importancias.index]\n",
    "importancias.index = feature_names\n",
    "\n",
    "importancias.plot(kind='bar')\n",
    "importancias.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "0.DEMO_Importancia_Arbol.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
