{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Evaluación de modelos e importancia de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "**Introducción**\n",
    "\n",
    "La Organización Mundial de la Salud ha estimado que 12 millones de muertes ocurren en todo el mundo, cada año debido a enfermedades del corazón. La mitad de las muertes en los Estados Unidos y otros países desarrollados se deben a enfermedades cardiovasculares. El pronóstico temprano de las enfermedades cardiovasculares puede ayudar a tomar decisiones sobre los cambios en el estilo de vida en pacientes de alto riesgo y, a su vez, reducir las complicaciones. Esta investigación tiene la intención de identificar los factores más relevantes / de riesgo de enfermedad cardíaca, así como predecir el riesgo general mediante distintos modelos de clasificación\n",
    "\n",
    "**Fuente**\n",
    "\n",
    "El conjunto de datos está disponible públicamente en el sitio web de Kaggle (https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset), y proviene de un estudio cardiovascular en curso en residentes de la ciudad de Framingham, Massachusetts. El objetivo de la clasificación es predecir si el paciente tiene riesgo de enfermedad coronaria (CHD) en los próximos 10 años. El conjunto de datos proporciona la información del paciente. Incluye más de 4000 registros y 15 atributos.\n",
    "\n",
    "**Variables**\n",
    "\n",
    "Cada atributo es un factor de riesgo potencial. Hay factores de riesgo demográficos, conductuales y médicos.\n",
    "\n",
    "Demográficos:\n",
    "\n",
    "• Sex: male or female(Nominal)\n",
    "\n",
    "• Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
    "Behavioral\n",
    "\n",
    "• Current Smoker: whether or not the patient is a current smoker (Nominal)\n",
    "\n",
    "• Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
    "\n",
    "Médicos (histórico):\n",
    "\n",
    "• BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n",
    "\n",
    "• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n",
    "\n",
    "• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n",
    "\n",
    "• Diabetes: whether or not the patient had diabetes (Nominal)\n",
    "\n",
    "Médicos (actual):\n",
    "\n",
    "• Tot Chol: total cholesterol level (Continuous)\n",
    "\n",
    "• Sys BP: systolic blood pressure (Continuous)\n",
    "\n",
    "• Dia BP: diastolic blood pressure (Continuous)\n",
    "\n",
    "• BMI: Body Mass Index (Continuous)\n",
    "\n",
    "• Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n",
    "\n",
    "• Glucose: glucose level (Continuous)\n",
    "\n",
    "Variable a predecir (target):\n",
    "\n",
    "• 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 - Preparación de datos\n",
    "\n",
    "1.1) Leamos los datos del archivo datasets_222487_478477_framingham.csv\n",
    "\n",
    "1.2) ¿Qué porcentaje de registros hay en cada una de las categorías target?\n",
    "\n",
    "1.3) ¿El dataset tiene datos faltantes?\n",
    "\n",
    "1.4) Usemos `dropna` para eliminar los registros con valores faltantes, y volvamos a calcular el porcentaje de registros hay en cada una de las categorías target \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('../Data/datasets_222487_478477_framingham.csv')\n",
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué procentaje de registros hay en cada una de las categorías target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en este caso, eliminando los registros que tienen algun valor nulo no cambiamos la proporción de registros en cada una de las categorías target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 - Train Test Split + StandardScaler\n",
    "\n",
    "Construir los conjuntos de entranamiento y test y usando StandardScaler normalizar las features\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3 - Opcional (pero recomendado)\n",
    "\n",
    "Escribir una función que reciba como parámetros \n",
    "\n",
    "* la instancia de un modelo\n",
    "* X_train, los registros con los valores de las features predictoras en el dataset de entrenamiento;\n",
    "* y_train, los registros con el valor del target en el dataset de entrenamiento;\n",
    "* X_test, los registros con los valores de las features predictoras en el dataset de testing;\n",
    "* y_test, los registros con el valor del target en el dataset de testing;\n",
    "* gridSearch_params, los parámetros para usar en grid search para la instancia del modelo\n",
    "* gridSearch_bagging_params, los parámetros para usar en grid search para bagging\n",
    "\n",
    "La función debe entrenar \n",
    "\n",
    "* El modelo que recibe como parámetro usando X_train, y_train\n",
    "\n",
    "* Grid Search Cross Validation KFold: `cv_KFold = KFold(n_splits=3, shuffle=True, random_state=371)`\n",
    "    \n",
    "\n",
    "* Grid Search CV Stratified KFold: `StratifiedKFold(n_splits=3, shuffle=True, random_state=371)`\n",
    "\n",
    "* Bagging\n",
    "\n",
    "* Bagging Grid Search Stratified Cross Validation: usando como base el mejor estimador devuelto en Grid Search CV Stratified KFold y `StratifiedKFold(n_splits=3, shuffle=True, random_state=371)`\n",
    "\n",
    "Esta función debe devolver alguna estructura de datos, por ejemplo un diccionario, donde después podamos consultar el score en test de cada uno de los modelos, y los valores de media y desvío del score en los entrenamientos con grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_instance, X_train, y_train, X_test, y_test, gridSearch_params, gridSearch_bagging_params):\n",
    "\n",
    "    #TODO\n",
    "    \n",
    "    return None\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Entrenar un modelo de clasificación basado en árboles, usando bagging y grid search cross validation estratificado sobre los parámetros de bagging, y calcular el score en test de cada uno de los modelos, y los valores de media y desvío del score en los entrenamientos con grid search cross validation.\n",
    "\n",
    "Los parámetros de GridSearchCV que vamos a probar son \n",
    "\n",
    "``params =  {'criterion': ['gini', 'entropy'],\n",
    "          'splitter': ['best', 'random'],\n",
    "          'max_depth': [None, 5, 10],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'min_samples_leaf': [1, 2, 3]}``\n",
    "\n",
    "Los parámetros de bagging que vamos a probar son\n",
    "\n",
    "<code>\n",
    "bagging_params = {'n_estimators': [10, 100],\n",
    "\n",
    "                  'max_samples': [0.01, 1.0],\n",
    "                  \n",
    "                  'max_features': [0.3, 1.0],\n",
    "                  \n",
    "                  'bootstrap_features': [True, False]}\n",
    "</code>\n",
    "\n",
    "Si resolvieron el ejercicio 3, pueden usar esa función. \n",
    "\n",
    "Si no, pueden hacer cada uno de los entrenamientos en la misma forma que venimos haciendo en las prácticas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5 _ Performance\n",
    "\n",
    "En este ejercicio vamos a repasar dos métodos para evaluar la performance de un modelo.\n",
    "\n",
    "* accuracy_score - Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "\n",
    "* classification_report\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "Vamos a evaluarlas sobre el modelo de bagging Stratified KFold cross validation que usa de base el mejor modelo de decision tree resultado gridsearch estratificado `StratifiedKFold(n_splits=3, shuffle=True, random_state=371)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6 - Feature importance\n",
    "\n",
    "Veamos qué importancia tiene cada una de las features del modelo entrenado en el ejercicio anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 7 - Extra\n",
    "\n",
    "Calculen el área bajo la curva como medida de performance de alguno de los clasificadores entrenados\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias \n",
    "\n",
    "Cross Validation\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "StratifiedKFold\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\n",
    "\n",
    "Grid Search\n",
    "https://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "\n",
    "ROC\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "\n",
    "BaggingClassifier\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
