{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "## Tabla de Contenidos\n",
    "\n",
    "[Intro](#section_intro)\n",
    "\n",
    "[Imports](#section_imports)\n",
    "\n",
    "[Dataset](#section_dataset)\n",
    "\n",
    "[Matriz de confusión](#section_confusion_matrix)\n",
    "\n",
    "[Error tipo I](#section_error_type_i)\n",
    "\n",
    "[Error tipo II](#section_error_type_ii)\n",
    "\n",
    "[Accuracy](#section_accuracy)\n",
    "\n",
    "[Recall / Sensitivity / TPR](#section_recall)\n",
    "\n",
    "[Precision](#section_precision)\n",
    "\n",
    "[Relación entre precision y recall](#section_precision_recall)\n",
    "\n",
    "[Specificity](#section_specificity)\n",
    "\n",
    "[F1 Score](#section_f1)\n",
    "\n",
    "[Resumen](#section_resumen)\n",
    "\n",
    "[Curva ROC - AUC Score](#section_roc)\n",
    "\n",
    "[PR Curve](#section_pr)\n",
    "\n",
    "[Referencias](#section_referencias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a> \n",
    "## Intro\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "En esta guía, veremos las siguientes métricas para evaluar el rendimiento de cualquier modelo de clasificación:\n",
    "\n",
    "\n",
    "* Matriz de confusión\n",
    "* Error tipo I\n",
    "* Error tipo II\n",
    "* Exactitud\n",
    "* Recall o tasa positiva verdadera o sensibilidad\n",
    "* Precisión\n",
    "* Especificidad\n",
    "* F1 Score\n",
    "* Curva ROC- AUC\n",
    "* Curva PR\n",
    "\n",
    "Vamos a usar distintas medidas de evaluación dependiendo si el output del modelo son etiquetas o probabilidades, y si las clases están balanceadas o desbalanceadas. \n",
    "\n",
    " <img src='./img/1_model_performance_metrics.png' align='left' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_imports\"></a> \n",
    "## Imports\n",
    "\n",
    "[volver a TOC](#section_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_dataset\"></a> \n",
    "## Dataset\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "En esta guía usaremos el dataset iris (https://archive.ics.uci.edu/ml/datasets/iris), que viene con la bilbioteca sklearn, para mostrar cómo evaluar las métricas que presentamos.\n",
    "\n",
    "Como la variable target de este dataset tiene tres categorías, vamos a eliminar la categoría 2 (50 registros) así el clasificador no es multiclase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_raw = pd.DataFrame(iris.data)\n",
    "y_raw = iris.target\n",
    "\n",
    "to_drop_mask = y_raw == 2\n",
    "to_keep_mask = np.logical_not(to_drop_mask)\n",
    "sum(to_keep_mask)\n",
    "\n",
    "X = X_raw.loc[to_keep_mask, :]\n",
    "y = y_raw[to_keep_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a agregar ruido a los valores de las variables predictoras para que la clasificación no sea perfecta, porque con este dataset casi cualquier modelo consigue una clasificación óptima.\n",
    "\n",
    "Construimos los conjuntos de train y test, reservando para test el 30% de los registros.\n",
    "\n",
    "Entrenamos en el modelo y los testeamos en los datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=123)\n",
    "\n",
    "# Create a simple classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_class = classifier.predict(X_test)\n",
    "#print(y_class)\n",
    "\n",
    "y_pred_score_classes = classifier.predict_proba(X_test)\n",
    "y_pred_score = y_pred_score_classes[:, 1]\n",
    "#print(y_score)\n",
    "\n",
    "#nota: y_pred_score_classes tiene tantas columnas como clases se puedan predecir, nos quedamos con la que corresponde a la clase 1, que es la columna 1\n",
    "# si nos quedáramos con la 0 tendríamos como scores la probabilidad de pertenecer a la clase 0, o sea querriamos predecir 0 en lugar de 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_confusion_matrix\"></a> \n",
    "## Matriz de confusión\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "<img src='./img/2_confusion_matrix.png' align='left' />\n",
    "<br/>\n",
    "\n",
    "TP (True-positives): el valor predicho por el modelo es \"Yes\" y el verdadero valor es \"Yes\"\n",
    "\n",
    "TN (True-negatives): el valor predicho por el modelo es \"No\" y el verdadero valor es \"No\"\n",
    "\n",
    "FP (False-positives): el valor predicho por el modelo es \"Yes\" y el verdadero valor es \"No\"\n",
    "\n",
    "FN (False-negatives): el valor predicho por el modelo es \"No\" y el verdadero valor es \"Yes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "$C_{00}$: true negatives; $C_{10}$: false negative; $C_{11}$ true positives; $C_{01}$ false positives\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred_class)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_error_type_i\"></a> \n",
    "## Error tipo I\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "El error tipo I también se conoce como falsos positivos y ocurre cuando un modelo de clasificación predice incorrectamente un resultado verdadero para una observación originalmente falsa.\n",
    "\n",
    "<img src='./img/3_type_i_error.png' align='left' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "La cantidad de falsos positivos está representada por el elemento (0,1) de la matriz de confusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = conf_mat[0, 1]\n",
    "false_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_error_type_ii\"></a> \n",
    "## Error tipo II\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "El error de tipo II también se conoce como falsos negativos y ocurre cuando un modelo de clasificación predice incorrectamente un resultado falso para una observación originalmente verdadera.\n",
    "\n",
    "<img src='./img/4_type_ii_error.png' align='left' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "La cantidad de falsos negativos está representada por el elemento (1, 0) de la matriz de confusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = conf_mat[1, 0]\n",
    "false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las tres métricas mencionadas anteriormente son métricas de propósito general, independientemente del tipo de datos de entrenamiento y test que tengamos y el tipo de algoritmo de clasificación que hayamos implementado.\n",
    "\n",
    "Ahora veamos métricas que son adecuadas para un tipo particular de dataset.\n",
    "\n",
    "<a id=\"section_accuracy\"></a> \n",
    "## Accuracy (precisión)\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Comencemos hablando de precisión, esta es una métrica que se utiliza para datasets balanceados.\n",
    "\n",
    "La precisión mide cuán cercanos son los resultados predichos al verdadero valor. \n",
    "\n",
    "$Accuracy = \\frac{(TP+TN)}{(TP+FP+FN+TN)}$\n",
    "\n",
    "<img src='./img/5_balance.png' align='left' />\n",
    "\n",
    "<img src='./img/6_accuracy.png' align='left' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Por qué debemos usar accuracy sólo con datasets balanceados?**\n",
    "\n",
    "Supongamos un modelo que fue entrenado y testeado sobre datasets desbalancedos.\n",
    "\n",
    "La métrica accuracy vale 72%, y podría darnos la impresión de que nuestro modelo está haciendo un buen trabajo en la clasificación. \n",
    "\n",
    "Pero este modelo está haciendo un trabajo terrible al predecir la clase negativa. Solo predijo 20 resultados correctos de 100 observaciones negativas totales. \n",
    "\n",
    "Esta es la razón por la cual la métrica de precisión no debe usarse sobre datasets desbalanceados.\n",
    "\n",
    "<img src='./img/7_accuracy_imbalance_problem.png' align='left' />\n",
    "\n",
    "También podemos mirar cuál es el valor de accuracy del modelo si todas las instancias se clasifican con la etiqueta de la clase mayoritaria:\n",
    "\n",
    "||Yes|No||\n",
    "|---|:---:|:---|:---|\n",
    "|Yes|900|100|<font color='red'>1000</font>|\n",
    "|No|0|0|<font color='red'>0</font>|\n",
    "||<font color='red'>900</font>|<font color='red'>100</font>|<font color='red'>1000</font>|\n",
    "\n",
    "$Accuracy = \\frac{TP+TN}{TP+FP+FN+TN}$\n",
    "\n",
    "$Accuracy = \\frac{900+0}{900+100+0+0}$\n",
    "\n",
    "$Accuracy = 90\\%$\n",
    "\n",
    "Y aunque el modelo es muy malo, el valor de accuracy obtenido es bueno. \n",
    "Por lo tanto no es una buena métrica para medir la performance de modelos sobre datasets desbalanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_recall\"></a> \n",
    "## Recall / Sensitivity / TPR (True Possitive Rate)\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Usaremos esta medida con datasets desbalanceados\n",
    "\n",
    "Esta medida responde la pregunta ¿qué proporción de positivos reales se identificó correctamente?\n",
    "\n",
    "Recall se usa en casos donde la detección positivos es de suma importancia. Por ejemplo en la predicción de cáncer, donde se requiere minimizar los falsos negativos, y esto implica que se debe maximizar recall.\n",
    "\n",
    "<img src='./img/8_recall.png' align='left' />\n",
    "\n",
    "$Recall = TRP = \\frac{TP}{P} = \\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test, y_pred_class)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_precision\"></a>\n",
    "## Precision\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Esta medida responde la pregunta ¿qué proporción de predichos positivos fue realmente correcta?\n",
    "\n",
    "Precision se usa en casos en los que es de suma importancia no tener una gran cantidad de falsos positivos. Por ejemplo en detección de spam, un falso positivo es una observación que no era spam pero que nuestro modelo clasificó como spam. \n",
    "\n",
    "<img src='./img/10_precision.png' align='left' />\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred_class)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_precision_recall\"></a> \n",
    "## Relación entre precision y recall\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "<img src='./img/9_precision_recall.png' align='left' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_specificity\"></a>\n",
    "## Specificity\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Specificity o especificidad (también llamado true negative rate) mide la proporción de verdaderos negativos que son identificados correctamente.\n",
    "\n",
    "\n",
    "<img src='./img/11_specificity.png' align='left' />\n",
    "\n",
    "$Specificity = TNR =  \\frac{TN}{N} = \\frac{TN}{TN + FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = conf_mat[0,0]\n",
    "FP = conf_mat[0,1]\n",
    "specificity = TN / (TN + FP)\n",
    "specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_f1\"></a> \n",
    "## F1 Score\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Esta métrica depedende de precision y recall\n",
    "\n",
    "Accuracy se usa cuando los verdadero positivos y los verdaderos negativos son más importantes, mientras que F1 score se usa cuando los falsos negativos y los falsos positivos lo son.\n",
    "\n",
    "Accuracy se puede usar cuando las clase están balanceadeas, mientras que F1 score es una métrica  mejor cuando hay clases desbalanceadas.\n",
    "\n",
    "En la mayoría de los problemas de clasificación de la vida real, existe una distribución de clases desbalanceada y, por lo tanto, la puntuación F1 es una mejor métrica para evaluar nuestro modelo.\n",
    "\n",
    "<img src='./img/12_f1.png' align='left' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, y_pred_class)\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_resumen\"></a> \n",
    "## Resumen\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "<img src='./img/13_resumen.PNG' align='left' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classif_report = classification_report(y_test, y_pred_class)\n",
    "print(classif_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora vimos métricas de rendimiento de modelos de clasificación que predicen las etiquetas de clase. \n",
    "\n",
    "Veamos ahora las métricas para modelos que predicen basados en la probabilidad de pertenecer a una clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_roc\"></a> \n",
    "## Curva ROC - AUC Score\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Esta es una de las métricas más importantes utilizadas para medir el rendimiento de un modelo de clasificación y es muy popular entre los científicos de datos.\n",
    "\n",
    "Para graficar la curva ROC representamos \n",
    "* en el eje x: (1-Specificity) que equivale a FPR\n",
    "* en el eje y: Sensitivity que equivale a TPR\n",
    "\n",
    "<img src='./img/14_roc_auc.jpeg' align='left' />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos por entender esto con un ejemplo. \n",
    "\n",
    "Tenemos un modelo de clasificación que proporciona valores de probabilidad que oscilan entre 0 y 1 para predecir la probabilidad de que una persona sea covid+. \n",
    "\n",
    "El puntaje de probabilidad cercano a 0 indica una probabilidad muy baja de que la persona considerada sea covid+, mientras que los valores de probabilidad cercanos a 1 indican una probabilidad muy alta de que una persona sea covid+. \n",
    "\n",
    "Ahora, por defecto si consideramos un umbral de 0.5, todas las personas asignadas a probabilidades ≤0.5 serán clasificadas como \"No covid+\" y las personas asignadas a probabilidades> 0.5 serán clasificadas como \"covid+\". \n",
    "\n",
    "Pero, podemos variar este umbral. ¿Qué pasa si lo hago 0.3 o 0.9? Veamos qué pasa.\n",
    "\n",
    "Variando los valores del umbral y calculando los valores FPR y TPR para cada unos de esos umbrales, tenemos las coordenadas (x,y) que definen los puntos de la curva ROC. Observemos que cada punto de la curva corresponde a un valor de umbral distinto.\n",
    "\n",
    "<img src='./img/15_roc_09.png' align='left' />\n",
    "<img src='./img/16_roc_06.png' align='left' />\n",
    "<img src='./img/17_roc_03.png' align='left' />\n",
    "<img src='./img/18_roc_00.png' align='left' />\n",
    "\n",
    "Entonces los puntos a representar son\n",
    "\n",
    "<img src='./img/19_roc_values.png' align='left' />\n",
    "\n",
    "\n",
    "Así podemos graficar la curva ROC para un modelo de clasificación, asignando diferentes umbrales y calculando las coordenas asociadas al valor umbral que es representado con un punto en la curva.\n",
    "\n",
    "El área bajo la curva ROC se conoce como AUC, cuanto mayor es el valor de AUC mejor es el modelo.\n",
    "\n",
    "Cuanto más lejos esté la curva ROC de la línea diagonal (y = x), mejor será el modelo. \n",
    "\n",
    "Así es como ROC-AUC nos permite evaluar el rendimiento de nuestros modelos de clasificación, y nos proporciona un medio para seleccionar un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "    \n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_score)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_pr\"></a> \n",
    "## PR Curve\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "En una curva PR, calcularmeos y representaremos \n",
    "* Precision en el eje Y\n",
    "* Recall en el eje x\n",
    "\n",
    "Dado que las curvas PR no consideran verdaderos negativos, sólo deben usarse cuando no nos preocupa la especificidad del clasificador. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_score)\n",
    "\n",
    "print(precision)\n",
    "\n",
    "print(recall)\n",
    "\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada valor de thresholds, tenemos asociado un valor en la variable precision y uno en la variable recall, que son los valores que usamos como coordenadas en el gráfico que se presenta a continuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_test, y_pred_score)\n",
    "\n",
    "print(average_precision)\n",
    "\n",
    "disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'Average Precision={0:0.2f}'.format(average_precision));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_referencias\"></a> \n",
    "## Referencias\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Top 10 model performance metrics for classification ML models\n",
    "https://towardsdatascience.com/top-10-model-evaluation-metrics-for-classification-ml-models-a0a0f1d51b9\n",
    "\n",
    "Accuracy vs. F1-Score https://medium.com/analytics-vidhya/accuracy-vs-f1-score-6258237beca2\n",
    "\n",
    "Precision and Recall https://en.wikipedia.org/wiki/Precision_and_recall#F-measure\n",
    "\n",
    "Accuracy and precision https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\n",
    "\n",
    "Confusion Matrix https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "Contingency table https://en.wikipedia.org/wiki/Contingency_table\n",
    "\n",
    "F1 score https://en.wikipedia.org/wiki/F1_score\n",
    "\n",
    "How to Use ROC Curves and Precision-Recall Curves for Classification in Python https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
