{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_39_Features_Importances_modelos_clasificacion/Presentacion/img/M5_CLASE_39_portada.jpg\" align=\"center\" width=\"70%\"/></div>\n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 5</div>\n",
    "            <div class=\"div-contenido\">Feature Importance en modelos de clasificación\n",
    "</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- Calcular la importancia de los features en los árboles de decisión y en los modelos de ensamble.\n",
    "\n",
    "- Interpretabilidad de los modelos. Library LIME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "\n",
    "En general nos concentramos en optimizar el rendimiento del modelo. \n",
    "\n",
    "Pero también es importante encontrar *las features del dataset* que contribuyen en el modelo, y cual es su relevancia.\n",
    "\n",
    "Por ejemplo, si tenemos 1000 features para predecir la retención de los clientes.\n",
    "- ¿Qué features son relevantes?\n",
    "- ¿Cómo se pueden identificar?\n",
    "- ¿Cómo medimos la importancia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "\n",
    "Los modelos parámetricos, que parten de una función de clasificación conocida, reducen el problema *a estimar los parámetros* que mejor ajusten al dataset.\n",
    "\n",
    "Por ejemplo, en las *regresiones logísticas* **cada parámetro está asociado a una feature determinada**. \n",
    "\n",
    "Si las features están estandarizadas, podemos interpretar el tamaño de cada parámetro como **indicador de la importancia relativa** del feature asociado.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_39_Features_Importances_modelos_clasificacion/Presentacion/img/M5_CLASE_39_005_logistica.JPG\" alt=\"logistica\" width=30% height=25% />\n",
    "---\n",
    "\n",
    "Pero, los modelos basados en árboles de clasificación *no son paramétricos*; no tenemos coeficientes para ajustar.\n",
    "\n",
    "Veamos como encontrar la importancia de las features para estos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Importancia de las features en árboles CART\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "\n",
    "Primero presentamos el dataset y creamos un modelo CART.\n",
    "\n",
    "Luego analizamos las features del dataset para determinar la importancia de cada una."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Dataset\n",
    "\n",
    "---\n",
    "Vamos a trabajar con datos usados para la predicción de divorcios. \n",
    "\n",
    "Las **observaciones** son *personas* a las cuales se les hace preguntas. Las **features** son las *respuestas a cada una de las 54 preguntas*.\n",
    "\n",
    "La **clase** representa si la persona está en pareja (0) o divorciada (1).\n",
    "\n",
    "Para consultar detalles del dataset y las 54 preguntas ver <a href=\"http://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set#\">aquí</a>.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_39_Features_Importances_modelos_clasificacion/Presentacion/img/M5_CLASE_39_001_luismi.JPG\" alt=\"luismi\" width=60% height=45% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las features Atr1 hasta Atr54 representan *afirmaciones*. Por ejemplo, \"yo conozco a mi cónyuge muy bien\".\n",
    "\n",
    "Las personas responden con  un valor entre 0 y 4, donde **0 -** indica que concuerda totalmente con la afirmación, y **4 -** que no coincide en absoluto. Entre 1 y 3 son valores intermedios.\n",
    "\n",
    "El *atributo class* indica el estado civil de la persona. 0 - casado, 1 - divorciado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/divorce.csv', sep=';')\n",
    "print('Filas:', df.shape[0], 'Columnas:', df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creamos la matriz de features y la variable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La proporción de casos para cada clase es similar: *0 - casado (sin pensar en separarse), 1 - divorciado*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tambien se pueden ver la cantidad de respuestas *por afirmación y por valor*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Arbol de decisión CART\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generamos el modelo de árboles de decisión CART con <a href=\"https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart\">scikit-learn</a>.\n",
    "\n",
    "Dejamos de lado el análisis de la performance. Por eso no dividimos en train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(criterion='gini',max_depth=4,min_samples_leaf=2)\n",
    "dt.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(12,5))  # set plot size (denoted in inches)\n",
    "tree.plot_tree(dt,feature_names=df.columns[:-1],filled=True,rounded=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El modelo nos indica que:\n",
    "\n",
    "- selecciona la afirmación 18, \"My spouse and I have similar ideas about how marriage should be\" (Mi cónyuge y yo tenemos ideas similares sobre cómo debería ser el matrimonio)\n",
    "\n",
    "   - Si no coincide con esta afirmación (*Respuesta entre 2 y 4*) , lo clasifica como *divorciado*.\n",
    "   \n",
    "   - Si coincide (*Respuesta entre 0 y 1*) , selecciona la 26, \" I know my spouse's basic anxieties\" (Conozco las ansiedades básicas de mi cónyuge.)\n",
    "   \n",
    "       - Si no coincide, lo clasifica como *divorciado*.\n",
    "       \n",
    "       - Si coincide, selecciona la afirmación 3, \"When we need it, we can take our discussions with my spouse from the beginning and correct it\" (Cuando lo necesitamos, podemos tomar nuestras discusiones con mi cónyuge desde el principio y corregirlo.)\n",
    "       \n",
    "            - Hasta una respuesta entre 0 y 3, lo clasifica como *casado*.\n",
    "           \n",
    "            - Solo si responde con 4 lo clasifica como *divorciado*.     \n",
    "           \n",
    "*No está mal el modelo!* ;-)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Arbol de decisión CART\n",
    "\n",
    "---\n",
    "Repasemos los valores que vemos en cada nodo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,1))\n",
    "tree.plot_tree(dt,feature_names=df.columns[:-1],filled=True,rounded=True, impurity=True,\n",
    "                fontsize=12, max_depth=0, class_names=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- el criterio de partición. *Atr18 <= 1.5*.\n",
    "- el criterio de impureza. *gini = 0.5*.\n",
    "- total de casos asignados al nodo. *samples = 170*.\n",
    "- casos por clase. *value*.\n",
    "- clase seleccionada. *class*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pureza\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sabemos que para particionar cada nodo, seleccionamos el atributo que logra nodos hijos lo **más homogéneos (puros) posibles**.\n",
    "\n",
    "Definimos un nodo como *puro*, informalmente, si contiene predominantemente observaciones de una misma clase.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_39_Features_Importances_modelos_clasificacion/Presentacion/img/M5_CLASE_39_003_impureza.png\" alt=\"impureza\" width=60% height=45% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Indice de impureza Gini\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Formalmente, usamos el **índice de impureza Gini**.  \n",
    "\n",
    "$$G = \\sum_{k=1}^K \\hat{p}_{mk} . (1 - \\hat{p}_{mk}) = 1 - \\sum_{k=1}^K (\\hat{p}_{mk})^2$$\n",
    "\n",
    "donde $\\hat{p}_{mk}$ representa la proporción de observaciones de entrenamiento en la m-ésima región que pertenecen a la k-ésima clase.\n",
    "\n",
    "Un valor *cercano a cero* indica que contiene mayoritariamente observaciones de una misma clase. Un valor *de 0.5 indica máxima impureza*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Por ejemplo, el nodo:\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_39_Features_Importances_modelos_clasificacion/Presentacion/img/M5_CLASE_39_004_nodo.JPG\" alt=\"nodo\" width=20% height=10% />\n",
    "\n",
    "Tiene a $G = 0.065$, que indica un grado alto de pureza. Y se comprueba viendo que contiene mayoritariamente observaciones de la clase 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recalculamos el $G = 0.5$ del nodo raíz, el cual indica máximo nivel de impureza.\n",
    "\n",
    "$$G = \\sum_{k=1}^K \\hat{p}_{mk} . (1 - \\hat{p}_{mk}) = 1 - \\sum_{k=1}^K (\\hat{p}_{mk})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "total_observaciones_clase = pd.Series(y).value_counts()\n",
    "print(\"Total observaciones por clase\"); print(total_observaciones_clase)\n",
    "total_observaciones = sum(total_observaciones_clase)\n",
    "print(\"Total observaciones\",total_observaciones)\n",
    "proporciones_clases = total_observaciones_clase / total_observaciones\n",
    "print (\"Proporciones Clases\"); print (proporciones_clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G = 1 - sum( proporciones_clases ** 2 ); print (\"G: \", round(G,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importancia de las features\n",
    "\n",
    "---\n",
    "Para calcular la importancia de las features usamos la propiedad <a href=\"\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_\">feature_importances_</a> de `DecisionTreeClassifier`.\n",
    "\n",
    "Solo *tres features tienen alguna importancia*, pues son las únicas que fueron usados para generar las particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "importancia_features = pd.DataFrame(dt.feature_importances_, index = X.columns, columns=['importancia'])\n",
    "importancia_features_sort = importancia_features.sort_values('importancia', ascending=False)\n",
    "importancia_features_sort[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importancia de las features\n",
    "\n",
    "---\n",
    "La importancia **compara la medida de impureza antes y después de la partición**. Se lo conoce también como **Importancia de Gini o ganancia**.\n",
    "\n",
    "*A mayor ganancia, más importancia*, ya que si la medida de impureza del nodo padre **es mayor** a la suma de la impureza de los hijos, *mejoramos la pureza*.\n",
    "\n",
    "Calcula la *reducción total normalizada* del criterio (Gini) por feature. \n",
    "\n",
    "$$\\Delta = I(padre) - \\sum_{j \\in hijos} \\frac{N_j}{N}. I(hijo_j) $$\n",
    "\n",
    "Donde \n",
    "\n",
    "* $I$ es la medida de impureza, \n",
    "* $N_j$ es el número de registros en el nodo hijo $j$ y \n",
    "* $N$ es el número de registros en el nodo padre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recreamos los valores de `dt.feature_importances_`, observando el árbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "tree.plot_tree(dt,feature_names=df.columns[:-1],filled=True,rounded=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Viendo el cálculo\n",
    "\n",
    "$$\\Delta = I(padre) - \\sum_{j \\in hijos} \\frac{N_j}{N}. I(hijo_j) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ganancia_gini_Atr18 = 1.000 * 0.5 - (89/170) * 0.065 - (81/170) * 0.0000\n",
    "ganancia_gini_Atr26 = (89/170) * 0.065 - (87/170) * 0.023 - (2/170) * 0.0000\n",
    "ganancia_gini_Atr3  = (87/170) * 0.023 - (85/170) * 0.0000 - (2/170) * 0.5\n",
    "\n",
    "norm = ganancia_gini_Atr18 + ganancia_gini_Atr26 + ganancia_gini_Atr3\n",
    "\n",
    "print (\"Atr18:\", round(ganancia_gini_Atr18 / norm,3))\n",
    "print (\"Atr26:\", round(ganancia_gini_Atr26 / norm,3))\n",
    "print (\"Atr3:\", round(ganancia_gini_Atr3 / norm,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropia\n",
    "\n",
    "---\n",
    "Scikit-learn nos ofrece otro criterio para particionar, `criterion='entropy'`.\n",
    "\n",
    "En teoría de la información se define **Entropía** como una forma de medir el grado de desorganización en un sistema. \n",
    "\n",
    "Mide qué tan parecidos son los elementos de un sistema:\n",
    "- un *valor mínimo de 0* indica que son totalmente iguales.\n",
    "- un *valor máximo de 1*, el mayor grado de desorden posible.\n",
    "\n",
    "$$D = - \\sum_{k = 1}^K{\\hat{p}_{mk} . log(\\hat{p}_{mk})}$$\n",
    "\n",
    "donde $\\hat{p}_{mk}$ representa la proporción de observaciones de entrenamiento en la m-ésima región que pertenecen a la k-ésima clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observemos que el nodo raíz tiene la *máxima entropía*, ya que la cantidad de observaciones de cada clase son similares.\n",
    "\n",
    "No necesariamente ambos criterios generan igual modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt_entropy = tree.DecisionTreeClassifier(criterion='entropy',max_depth=4,min_samples_leaf=2)\n",
    "dt_entropy.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "tree.plot_tree(dt_entropy,feature_names=df.columns[:-1],filled=True,rounded=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importancia de las features - mejora del modelo\n",
    "\n",
    "---\n",
    "Conocer las features más importantes nos ayuda a generar un árbol más eficiente, en términos de cómputo.\n",
    "\n",
    "Nos permite *reducir el conjunto de features* en uno menor que genera un modelo con bajo o nulo costo en la performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(criterion='gini',max_depth=4,min_samples_leaf=2)\n",
    "dt.fit(X, y)\n",
    "if_select = pd.DataFrame({'atributo':X.columns, 'importancia': dt.feature_importances_})\n",
    "if_select.sort_values('importancia', ascending = False).iloc[0:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "features_select = if_select.atributo.values[if_select.importancia.values>0]\n",
    "features_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dt_select = tree.DecisionTreeClassifier(criterion='gini',max_depth=4,min_samples_leaf=2)\n",
    "dt_select.fit(X.loc[:, features_select], y)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "tree.plot_tree(dt_select,feature_names=features_select,filled=True,rounded=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Importancia de las features en ensambles\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_34_CART/Presentacion/img/M5_CLASE_34_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelos de ensamble\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a generar un modelo de <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">random forest</a> sobre el mismo dataset y calcular la importancia de las features.\n",
    "\n",
    "La importancia de los features se calcula como **la media de la importancia de los features de los árboles base**.\n",
    "\n",
    "Recordemos que Random Forest aplica la técnica de bagging pero en lugar de utilizar todas las variables independientes para cada modelo, aplica muestreos con reposición. \n",
    "\n",
    "En nuestro ejemplo, genera 10 datasets (`n_estimators=10`), donde cada uno es un muestreo de las features tomadas de a M sobre el total P (54) de features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=17, n_estimators=10)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por lo tanto, la cantidad de features con importancia mayor a cero *es mayor* al modelo CART, ya que ahora se generan más árboles que tienen distintas features. \n",
    "\n",
    "Se calcula también con la propiedad `feature_importances_`. Veamos los primeras cuatro features en importancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rf_select = pd.DataFrame({'atributo':X.columns, 'importancia': rf.feature_importances_})\n",
    "rf_select.sort_values('importancia', ascending = False).iloc[0:4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelos de ensamble\n",
    "\n",
    "---\n",
    "Vamos a comprobar que la importancia de las features en los ensambles es *el promedio* de su importancia en los árboles base.\n",
    "\n",
    "Con la propiedad `estimators_` obtenemos los árboles base del ensamble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Total de árboles base: ',len(rf.estimators_))\n",
    "print('Primer árbol base: ',rf.estimators_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cada árbol base tiene su matriz de importancia de features.\n",
    "\n",
    "Como los atributos originales son 54, la matriz tiene 54 elementos, donde cada uno representa la importancia de una feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(rf.estimators_[0].feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Calculemos *la importancia del feature Atr18*, a partir de los árboles base.\n",
    "\n",
    "Sabemos que los atributos del dataset se llaman `Atrx`, con $x=1..54$.\n",
    "\n",
    "Por lo tanto, el *elemento 17* en las matrices de importancia de cada árbol base, es el valor para el *Atr18*.\n",
    "\n",
    "Sumando el valor que tiene en todas las matrices de los árboles base, y dividiendo por la cantidad de árboles base, llegamos a la importancia de la feature para el ensamble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "suma = sum([rf.estimators_[i].feature_importances_[17] for i in range(len(rf.estimators_))])\n",
    "    \n",
    "print('Importancia calculada de Atr18 en el ensamble: ',round(suma/10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Importancia de Atr18 en el ensamble, según feature_importances: ')\n",
    "rf_select[rf_select.atributo=='Atr18']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Interpretabilidad de los modelos\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_34_CART/Presentacion/img/M5_CLASE_34_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "Muchas veces estamos interesados en la *interpretabilidad de los modelos* además de su performance.\n",
    "\n",
    "A veces no podemos entender por qué algunas de nuestras predicciones son correctas mientras que otras veces no, ni podemos rastrear el camino de decisión de nuestro modelo.\n",
    "\n",
    "**<a href=\"https://lime-ml.readthedocs.io/en/latest/lime.html\">LIME</a>** (Local Interpretable Model-agnostic Explanations), es una biblioteca de Python que explica cómo decide un modelo de una manera comprensible, generando explicaciones a nivel local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Seguimos usando el dataset de respuestas, pero limitado a *las primeras siete features* con mayor importancia en el modelo de Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if_select = pd.DataFrame({'atributo':X.columns, 'importancia': rf.feature_importances_})\n",
    "features = if_select.sort_values('importancia', ascending = False).iloc[0:7, :]['atributo'].values.tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_lime = X[features] \n",
    "X_lime.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Separamos los datos en train y test, y creamos un modelo de ensamble Random Forest. *Ya que necesitamos conocer las predicciones*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lime, y, test_size=0.3, random_state = 123)\n",
    "rc_exp = RandomForestClassifier(n_estimators=50, random_state = 123)\n",
    "rc_exp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LimeTabularExplainer\n",
    "\n",
    "---\n",
    "Vamos a crear una instancia de <a href=\"https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=limetabularexplainer#lime.lime_tabular.LimeTabularExplainer\">LimeTabularExplainer</a>, para después generar explicaciones sobre una predicción determinada usando este objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_explainer = np.array(X_train)\n",
    "explainer = LimeTabularExplainer(X_train_explainer, \n",
    "                                 mode = \"classification\",\n",
    "                                 training_labels = y_train,\n",
    "                                 feature_names = X_train.columns, \n",
    "                                 categorical_features  = list(range(7)),\n",
    "                                 discretize_continuous=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Los parámetros del constructor de `LimeTabularExplainer` que vamos a usar son:\n",
    "* training_data – <font color=\"green\"> X_train transformado en un numpy 2d array.</font>\n",
    "* mode – “classification” or “regression”, <font color=\"green\">vamos a usar classification</font>\n",
    "* training_labels – <font color=\"green\">la variable target en train.</font>\n",
    "* feature_names – <font color=\"green\">nombre de las columnas del dataset.</font>\n",
    "* categorical_features – <font color=\"green\">Lista de los índices de las columnas categóricas.</font> Todas.\n",
    "* discretize_continuous – <font color=\"green\">si True, todas las features no categóricas se discretizan.</font>\n",
    "* discretizer – <font color=\"green\">Tipo de discretización. ‘quartile’, ‘decile’ or ‘entropy’.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explainer\n",
    "\n",
    "---\n",
    "Usando el objeto `explainer` y el método `explain_instance` vamos a generar explicaciones sobre una predicción concreta.\n",
    "\n",
    "Por ejemplo, sobre la fila (persona) de índice 47 del dataset de test.\n",
    "\n",
    "`explain_instance` recibe como argumentos:\n",
    "\n",
    "* data_row - <font color=\"blue\">la fila que analizamos, de tipo 1d numpy array.</font>\n",
    "* predict_fn – <font color=\"blue\">clasificador que devuelve los valores de la predicción como probabilidades.</font> Para Clasificación es classifier.predict_proba(). Para Regresiones es regressor.predict()\n",
    "* num_features – <font color=\"blue\">Número de features presentes en la explicación</font>. Traemos todas.\n",
    "\n",
    "Y devuelve una instancia de tipo `explanation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "i = 47\n",
    "data_row = np.array(X_test.iloc[i]) # necesito que sea un np array:\n",
    "explanation = explainer.explain_instance(data_row, rc_exp.predict_proba, num_features=len(data_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explanation\n",
    "\n",
    "---\n",
    "La clase **<a href=\"https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=explanation#module-lime.explanation\">explanation</a>** provee métodos para analizar y visualizar el resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`as_list` devuelve una lista (feature, peso) correspondiente a la predicción de la fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "explanation.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La columna de las features representa los valores que tiene la fila 47."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test.iloc[47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explanation\n",
    "\n",
    "---\n",
    "**<a href=\"https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=explanation#lime.explanation.Explanation.show_in_notebook\">show_in_notebook</a>** muestra un gráfico resumiendo la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "explanation.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El gráfico tiene tres partes:\n",
    "\n",
    "* El panel de la izquierda muestra *la probabilidad predicha* por el modelo para el registro de índice 47.\n",
    "\n",
    "* El panel del medio muestra *las features por orden de importancia*. Las features que tienen color *naranja* son compatibles con la clase 1 y las *azules* con la clase 0. \n",
    "\n",
    "* El panel de la derecha muestra nuevamente las features y sus valores en el registro de índice 47."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`as_pyplot_figure` devuelve un gráfico de barras similar al panel central de `show_in_notebook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "explanation.as_pyplot_figure();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_34_CART/Presentacion/img/M5_CLASE_34_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "---\n",
    "\n",
    "* Vimos que el *índice de impureza Gini*, junto con la *Importancia de Gini* son medidas útiles para realizar el split de un nodo en nodos más puros.\n",
    "\n",
    "* El algoritmo CART aplica el criterio *Gini* o el de *Entropía*.\n",
    "\n",
    "* La importancia de las features es la *reducción total normalizada del criterio (Gini) por feature*. Es decir, cuanto gana en pureza al pasar del nodo padre a los nodos hijos.\n",
    "\n",
    "* La importancia de las features en los ensambles se calcula como *la media de la importancia de las features de los árboles base*. \n",
    "\n",
    "* Conocer las features más importantes nos ayuda a reducir el conjunto de features que componen el modelo y así disminuir su costo computacional.\n",
    "\n",
    "* Lime es una library de Python que explica como se generó la predicción para una observación determinada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "----\n",
    "\n",
    "A partir de un conjunto reducido del dataset usados para la predicción de divorcios:\n",
    "\n",
    "- Generamos un modelo CART con criterio Gini y determinamos la importancia de las features.\n",
    "\n",
    "- Generamos un modelo Random Forest con criterio Gini y 50 árboles base. También determinamos la importancia de las features.\n",
    "\n",
    "- Comparamos las primeras 5 features en importancia. Son iguales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "----\n",
    "Seguimos usando el dataset de respuestas, pero limitado a 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features = ['Atr19', 'Atr20', 'Atr28', 'Atr3', 'Atr17', 'Atr54', 'Atr29', 'Atr39', 'Atr25', 'Atr8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_ej = X[features]\n",
    "y = df['Class'] \n",
    "X_ej.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solución\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "----\n",
    "\n",
    "A partir de un conjunto reducido del dataset usados para la predicción de divorcios:\n",
    "\n",
    "- Generamos un modelo CART con criterio Gini y determinamos la importancia de las features.\n",
    "\n",
    "- Generamos un modelo Random Forest con criterio Gini y 50 árboles base. También determinamos la importancia de las features.\n",
    "\n",
    "- Comparamos las primeras 5 features en importancia. Son iguales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "----\n",
    "Seguimos usando el dataset de respuestas, pero limitado a 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features = ['Atr19', 'Atr20', 'Atr28', 'Atr3', 'Atr17', 'Atr54', 'Atr29', 'Atr39', 'Atr25', 'Atr8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_ej = X[features]\n",
    "y = df['Class'] \n",
    "X_ej.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelo CART\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt_ej = tree.DecisionTreeClassifier(criterion='gini',max_depth=4,min_samples_leaf=2)\n",
    "dt_ej.fit(X_ej, y)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "tree.plot_tree(dt_ej,feature_names=X_ej.columns[:-1],filled=True,rounded=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importancia de las features - CART\n",
    "\n",
    "---\n",
    "Calculamos la importancia de las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_dt_ej = pd.DataFrame({'atributo':X_ej.columns, 'importancia': dt_ej.feature_importances_})\n",
    "if_dt_ej_sort = if_dt_ej.sort_values('importancia', ascending=False)\n",
    "if_dt_ej_sort[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelo Random Forest\n",
    "\n",
    "----\n",
    "Generamos un ensamble con 50 árboles base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_ej = RandomForestClassifier(class_weight='balanced', random_state=17, n_estimators=50)\n",
    "rf_ej.fit(X_ej, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Calculamos también la importancia de las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i4WVH_c3IAS",
    "outputId": "5b842c37-e466-494b-abc9-cdb01da08180",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if_rf_ej = pd.DataFrame({'atributo':X_ej.columns, 'importancia': rf_ej.feature_importances_})\n",
    "if_rf_ej.sort_values('importancia', ascending = False).iloc[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparación\n",
    "\n",
    "----\n",
    "Verificamos que la primera feature es igual en ambos casos.\n",
    "\n",
    "Los restantes son iguales pero en distinto orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 5 primeros features CART.\n",
    "l_dt =  if_dt_ej_sort[0:5]['atributo'].values.tolist()\n",
    "\n",
    "# 5 primeros features RF.\n",
    "l_rf = if_rf_ej.sort_values('importancia', ascending = False).iloc[0:5, :]['atributo'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "l_total = pd.DataFrame(\n",
    "    {'CART': l_dt,\n",
    "     'R.Forest': l_rf\n",
    "    })\n",
    "\n",
    "l_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias y Material Adicional\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Referencias y Material Adicional\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_\" target=\"_blank\">Scikit-learn: feature_importances</a>\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html\" target=\"_blank\">Scikit-learn: plot_tree</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\" target=\"_blank\">The mathematics of decision trees and random-forest and feature importance in scikit learn</a>\n",
    "\n",
    "<a href=\"https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3#:~:text=It%20is%20sometimes%20called%20%E2%80%9Cgini,all%20trees%20of%20the%20ensemble.\" target=\"_blank\">Feature Importance Measures for Tree Models — Part I</a>\n",
    "\n",
    "<a href=\"https://becominghuman.ai/feature-importance-measures-for-tree-models-part-ii-20c9ff4329b\" target=\"_blank\">Feature Importance Measures for Tree Models — Part II</a>\n",
    "\n",
    "<a href=\"https://lime-ml.readthedocs.io/en/latest/lime.html\" target=\"_blank\">Documentación LIME</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
