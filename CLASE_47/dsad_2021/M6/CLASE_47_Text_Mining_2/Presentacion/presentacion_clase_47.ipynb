{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_47_Text_Mining_2/Presentacion/img/M6_CLASE_47_portada.jpg\" align=\"center\" width=\"90%\"/></div>        \n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 6</div>\n",
    "            <div class=\"div-contenido\">Text Minig <br/> Clasificación de texto</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- Análisis de sentimientos como problema de clasificación\n",
    "\n",
    "- Clasificación\n",
    "\n",
    "- N-gramas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Introducción\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_47_Text_Mining_2/Presentacion/img/M6_CLASE_47_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "---\n",
    "\n",
    "<table><tr><td style=\"font-size:14px;width:55%;line-height:2;\">El análisis de sentimientos es una técnica a través de la cual podemos analizar un fragmento de texto para determinar el sentimiento detrás de él. \n",
    "<br/>\n",
    "Combina el aprendizaje automático y el procesamiento del lenguaje natural (NLP) para lograrlo. \n",
    "<br/>\n",
    "Podemos pensar este problema como un problema de clasificación, donde las features son generadas con el técnicas de preprocesamiento de texto que vimos la clase pasada.</td><td><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_47_Text_Mining_2/Presentacion/img/M6_CLASE_47_intro.jpg\" align=\"center\"/></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Clasificación\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_47_Text_Mining_2/Presentacion/img/M6_CLASE_47_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clasificación\n",
    "\n",
    "---\n",
    "\n",
    "En esta clase analizaremos un dataset de reviews de imdb (http://ai.stanford.edu/~amaas/data/sentiment/) usando regresión logística para predecir a partir del texto del comentario si la calificación otorgada por el usuario es positiva o negativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Datos\n",
    "\n",
    "---\n",
    "\n",
    "Los datos están separados en dos archivos, uno de train y uno de test, cada uno con la misma cantidad de registros.\n",
    "\n",
    "Las clases están balanceadas tanto en el dataset de train como en el de test, es decir, hay igual cantidad de reviews positivas y negativas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_file_path = '../Data/imdb_train.zip'\n",
    "data_train = pd.read_csv(train_file_path, sep=\"\\t\")\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_file_path = '../Data/imdb_test.zip'\n",
    "data_test = pd.read_csv(test_file_path, sep=\"\\t\")\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preparación\n",
    "\n",
    "---\n",
    "\n",
    "Limpiamos los datos de train y de test:\n",
    "\n",
    "* usamos un tokennizer que elimine los signos de puntuación y tags html\n",
    "\n",
    "* hacemos stemming para obtener las raíces de las palabras en minúsculas\n",
    "\n",
    "* eliminamos stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def clean_review(review_text, tokenizer, stemmer, stopwords):    \n",
    "    \n",
    "    #tokens (eliminamos todos los signos de puntuación)\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    #print(words)\n",
    "    \n",
    "    # stemming: raiz y minúsculas:\n",
    "    stem_words = [stemmer.stem(x) for x in words]\n",
    "    #print(stem_words)\n",
    "    \n",
    "    # eliminamos stopwords (ya pasaron por stem)\n",
    "    clean_words = [x for x in stem_words if x not in stopwords]\n",
    "    #print(clean_words)\n",
    "    \n",
    "    result = \" \".join(clean_words)\n",
    "    \n",
    "    return(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ejemplo sobre un registro de train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "review_text = data_train.text[1]\n",
    "\n",
    "print(\"antes: \", review_text)\n",
    "\n",
    "#eliminamos todos los signos de puntuación\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "englishStemmer = SnowballStemmer(\"english\")\n",
    "stopwords_en = stopwords.words('english');\n",
    "stopwords_en_stem = [englishStemmer.stem(x) for x in stopwords_en]\n",
    "\n",
    "review_text_clean = clean_review(review_text, tokenizer, englishStemmer, stopwords_en_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"---\")\n",
    "print(\"después: \", review_text_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Procesamos todos los registros de los datasets de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clean_train = [clean_review(x, tokenizer, englishStemmer, stopwords_en_stem) for x in data_train.text]\n",
    "#clean_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clean_test = [clean_review(x, tokenizer, englishStemmer, stopwords_en_stem) for x in data_test.text]\n",
    "#clean_test[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usando `CountVectorizer` vamos a transformar los datos de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(clean_train)\n",
    "X_train_sparse = count_vectorizer.transform(clean_train)\n",
    "X_test_sparse = count_vectorizer.transform(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train_sparse.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "y_train = data_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test_sparse.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelo\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a usar ahora una regresión logística para predecir el valor de `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Veamos qué valor de `C` (regularización) resulta en un modelo de mejor performance\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Valores más pequeños de `C` implican regularización más fuerte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_train, X_train_val, y_train_train, y_train_val = \\\n",
    "    train_test_split(X_train, y_train, train_size = 0.75, shuffle = True, random_state = 147)\n",
    "\n",
    "for c in [0.005, 0.008, 0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, solver=\"newton-cg\", penalty=\"l2\")    \n",
    "    lr.fit(X_train_train, y_train_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_train_val, lr.predict(X_train_val))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Entrenamos el modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "final_model = LogisticRegression(C = 0.05, solver=\"newton-cg\", penalty=\"l2\")\n",
    "final_model.fit(X_train, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "        % accuracy_score(y_test, final_model.predict(X_test)))\n",
    "print (\"Final Confusion Matrix: \\n %s\" \n",
    "        % confusion_matrix(y_test, final_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observemos cuáles son las 3 palabras más discriminantes tanto positiva como negativamente. \n",
    "\n",
    "Para ver esto, veremos cuáles son las palabras (features) asociadas a los coeficientes máximos y mínimos de la regresión logística.\n",
    "\n",
    "Construyamos un dataframe de dos columnas:\n",
    "\n",
    "* word: cada registro tiene un nombre que corresponde a una feature del modelo (las palabras en el corpus)\n",
    "* coef: cada registro tiene un valor que es el coeficiente de la regresión logística para su feature\n",
    "\n",
    "Y ordenemos el dataframe según los valores de coef de forma descendente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feature_to_coef = pd.DataFrame(columns = ['word', 'coef'])\n",
    "feature_to_coef.word = count_vectorizer.get_feature_names()\n",
    "feature_to_coef.coef = final_model.coef_[0]\n",
    "feature_to_coef_sort_desc = feature_to_coef.sort_values(by = 'coef', ascending = False)\n",
    "positive_words = feature_to_coef_sort_desc.word[0:3]\n",
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "feature_to_coef_sort_asc = feature_to_coef.sort_values(by = 'coef', ascending = True)\n",
    "negative_words = feature_to_coef_sort_asc.word[0:3]\n",
    "negative_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Grafiquemos estos resultados para ver cómo se distribuyen los valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "columns =  np.concatenate([positive_words.values, negative_words.values])\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_plot = X_train.loc[:, columns]\n",
    "data_plot.reset_index(inplace = True)\n",
    "data_plot[\"label\"] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_plot_long = pd.melt(data_plot, \n",
    "                         id_vars = [\"index\", \"label\"], \n",
    "                         #value_vars = data_plot.columns[1:len(data_plot.columns)-1], \n",
    "                         var_name = \"word\", value_name = \"value\")\n",
    "data_plot_long[\"word_label\"] = data_plot_long.word + [\"_\" + str(x) for x in data_plot_long.label]  \n",
    "data_plot_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_plot_long.sort_values(by = \"index\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.violinplot(x = data_plot_long.word, y = data_plot_long.value, hue = data_plot_long.label);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "---\n",
    "\n",
    "Entrenemos una regresión logística que use como features el resultado de SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feature_to_coef.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El modelo anterior fue entrenado con más de 50 mil features!\n",
    "\n",
    "Veamos cómo cambia la performance seleccionando 200 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components = 200);\n",
    "\n",
    "X_train_svd = svd.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test_svd = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_svd_train, X_train_svd_val, y_train_train, y_train_val = \\\n",
    "    train_test_split(X_train_svd, y_train, train_size = 0.75, shuffle = True, random_state = 147)\n",
    "\n",
    "for c in [0.005, 0.008, 0.009, 0.01, 0.05, 0.07]:    \n",
    "    lr = LogisticRegression(C=c, solver=\"newton-cg\", penalty=\"l2\")    \n",
    "    lr.fit(X_train_svd_train, y_train_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_train_val, lr.predict(X_train_svd_val))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "final_model_svd = LogisticRegression(C = 0.01, solver=\"newton-cg\", penalty=\"l2\")\n",
    "final_model_svd.fit(X_train_svd, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_model_svd.predict(X_test_svd)))\n",
    "print (\"Final Confusion Matrix: \\n %s\" \n",
    "        % confusion_matrix(y_test, final_model_svd.predict(X_test_svd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## N-gramas\n",
    "\n",
    "---\n",
    "\n",
    "Hasta ahora usamos sólo features compuestas por una sola palabra en nuestro modelo, que llamamos **1-grama** o **unigrama**. \n",
    "\n",
    "Potencialmente, podemos agregar más poder predictivo a nuestro modelo agregando también **secuencias de dos o tres palabras (bigramas o trigramas)**. \n",
    "\n",
    "Por ejemplo, si una reseña tuviera la secuencia de tres palabras \"no me gustó la película\", solo consideraríamos estas palabras individualmente con un modelo de unigramas y probablemente no captaríamos que se trata de un sentimiento negativo porque la palabra \"gustó\" en sí misma va a estar altamente correlacionado con una revisión positiva.\n",
    "\n",
    "Scikit-learn hace que sea muy fácil construir estas features. \n",
    "\n",
    "Simplemente usamos el argumento `ngram_range` con cualquiera de las clases \"Vectorizador\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer_bigram = CountVectorizer(ngram_range = (1, 2))\n",
    "count_vectorizer_bigram.fit(clean_train)\n",
    "X_train_bigram_sparse = count_vectorizer_bigram.transform(clean_train)\n",
    "X_test_bigram_sparse = count_vectorizer_bigram.transform(clean_test)\n",
    "\n",
    "#X_train_bigram = pd.DataFrame(X_train_bigram_sparse.todense(), \n",
    "#             columns = count_vectorizer_bigram.get_feature_names()) \n",
    "             \n",
    "#X_test_bigram = pd.DataFrame(X_test_bigram_sparse.todense(), \n",
    "#             columns = count_vectorizer_bigram.get_feature_names()) \n",
    "\n",
    "#usamos las matrices esparsas porque rompe si trato de convertrlas en densas para esta cantidad de features\n",
    "X_train_bigram_train, X_train_bigram_val, y_train_train, y_train_val = \\\n",
    "    train_test_split(X_train_bigram_sparse, y_train, train_size = 0.75, shuffle = True, random_state = 147)\n",
    "\n",
    "for c in [0.01, 0.05, 0.1, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:    \n",
    "    lr = LogisticRegression(C=c, solver=\"newton-cg\", penalty=\"l2\")    \n",
    "    lr.fit(X_train_bigram_train, y_train_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_train_val, lr.predict(X_train_bigram_val))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "final_model_bigram = LogisticRegression(C = 0.25, solver=\"newton-cg\", penalty=\"l2\")\n",
    "final_model_bigram.fit(X_train_bigram_sparse, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_model_bigram.predict(X_test_bigram_sparse)))    \n",
    "print (\"Final Confusion Matrix: \\n %s\" \n",
    "        % confusion_matrix(y_test, final_model_bigram.predict(X_test_bigram_sparse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_47_Text_Mining_2/Presentacion/img/M6_CLASE_47_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Construimos un modelo de clasificación de textos empleando los algoritmos que ya conocíamos.\n",
    "\n",
    "* Construimos el conjunto de features del modelo transformando los inputs (textos) mediante las técnicas de preprocesamiento que vimos en la clase pasada.\n",
    "\n",
    "* Evaluamos la performance del modelo del mismo modo que lo hacemos con cualquier modelo de clasificación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_47_Text_Mining_2/Presentacion/img/M6_CLASE_47_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio\n",
    "\n",
    "---\n",
    "\n",
    "TF-IDF es mejor que CountVectorizer porque no sólo se centra en la frecuencia de las palabras presentes en el corpus, sino que también tiene en cuenta su importancia. \n",
    "\n",
    "Volvamos a entrenar una regresión logísticas usando como features la representación tf-idf de unigramas, bigramas y trigramas.\n",
    "\n",
    "Veamos cuáles son los n-gramas más discriminantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solución\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range = (1, 3))\n",
    "count_vectorizer.fit(clean_train)\n",
    "X_train_sparse = count_vectorizer.transform(clean_train)\n",
    "X_test_sparse = count_vectorizer.transform(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "\n",
    "X_train_ngram_encoding = transformer.fit_transform(X_train_sparse);\n",
    "\n",
    "X_test_ngram_encoding = transformer.transform(X_test_sparse);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_ngram_train, X_train_ngram_val, y_train_train, y_train_val = \\\n",
    "    train_test_split(X_train_ngram_encoding, y_train, train_size = 0.75, shuffle = True, random_state = 147)\n",
    "\n",
    "for c in [0.05, 0.25, 0.5, 0.7, 0.8, 0.9, 1]:    \n",
    "    lr = LogisticRegression(C=c, solver=\"newton-cg\", penalty=\"l2\")    \n",
    "    lr.fit(X_train_ngram_train, y_train_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_train_val, lr.predict(X_train_ngram_val))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "final_model_tfidf = LogisticRegression(C = 1, solver=\"newton-cg\", penalty=\"l2\")\n",
    "final_model_tfidf.fit(X_train_ngram_encoding, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_model_tfidf.predict(X_test_ngram_encoding)))\n",
    "print (\"Final Confusion Matrix: \\n %s\" \n",
    "        % confusion_matrix(y_test, final_model_tfidf.predict(X_test_ngram_encoding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "feature_to_coef = pd.DataFrame(columns = ['ngram', 'coef'])\n",
    "feature_to_coef.ngram = count_vectorizer.get_feature_names()\n",
    "feature_to_coef.coef = final_model_tfidf.coef_[0]\n",
    "feature_to_coef_sort_desc = feature_to_coef.sort_values(by = 'coef', ascending = False)\n",
    "feature_to_coef_sort_desc.ngram[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "feature_to_coef_sort_asc = feature_to_coef.sort_values(by = 'coef', ascending = True)\n",
    "feature_to_coef_sort_asc.ngram[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vemos que los n-grams más discriminantes son unigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias y material adicional\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_47_Text_Mining_2/Presentacion/img/M6_CLASE_47_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://towardsdatascience.com/how-to-build-a-twitter-sentiment-analysis-system-12b28dcbae56\" target=\"_blank\">How to Build a Twitter Sentiment Analysis System</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184\" target=\"_blank\">Sentiment Analysis with Python (Part 1)</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a\" target=\"_blank\">Sentiment Analysis with Python (Part 2)</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/imdb-reviews-or-8143fe57c825\" target=\"_blank\">Performing Sentiment Analysis on Movie Reviews</a>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews\" target=\"_blank\">Sentiment Analysis of IMDB Movie Reviews</a>  \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
