{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Clasificacion de texto\n",
    "\n",
    "Trabajaremos con un dataset de noticias (en ingles) sobre diferentes temas: world, sports, business, sci/tech.\n",
    "\n",
    "La idea es vectorizar el corpus de noticias e implementar un clasificador para identificar a qué tema pertenecen las noticias.\n",
    "\n",
    "Trabajaremos con una versión reducida de un corpus que pueden descargar completo (~30Mb) del siguiente sitio:\n",
    "\n",
    "https://github.com/mhjabreel/CharCnn_Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1\n",
    "\n",
    "Generamos el corpus:\n",
    "\n",
    "Importen los datos con pandas. Los datos se encuentran en '../Data/ag_news_reduced.csv'. \n",
    "\n",
    "El dataset contiene cuatro columnas: la primera tiene un entero que indica a qué clase pertenece la noticia. La segunda es el nombre de la clase, la tercera es el título y la cuarta es una descripción de la noticia.\n",
    "\n",
    "Como no tenemos las noticias enteras sino solo un resumen, conviene que generemos el corpus concatenando el título y la descripción en un sólo texto. \n",
    "\n",
    "Generen una columna en el dataframe que sea la concatenación del título y la descripción. No olviden agregar un espacio en blanco para no pegar la última palabra del título con la primera de la descrición.\n",
    "\n",
    "¿Cuántos artículos hay de cada clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('../Data/ag_news_reduced.csv');\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['news']=data['title']+ ' '+data['description'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['news'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2\n",
    "\n",
    "¿Cuáles son las palabras más frecuentes dentro de cada clase?\n",
    "\n",
    "Ayuda: Pueden vectorizar el corpus (dividido por temas) usando CountVectorizer() y luego sumar las filas de la matriz para obtener el número total de veces que aparece cada término.\n",
    "\n",
    "Ayuda2: No olviden remover stopwords al vectorizar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "stop_words=stopwords.words('english');\n",
    "\n",
    "vectorizer=CountVectorizer(stop_words=stop_words);\n",
    "\n",
    "clases=['World','Sports','Business','Sci/Tech'];\n",
    "\n",
    "for clase in range(1,5):\n",
    "    X=vectorizer.fit_transform(data[data['class']==clase]['news']);\n",
    "    counts=X.sum(axis=0);\n",
    "    counts=np.array(counts);\n",
    "    \n",
    "    indices=np.argsort(counts);\n",
    "    valores=np.sort(counts);\n",
    "    indices=indices[0][::-1];\n",
    "    valores=valores[0][::-1];\n",
    "    terms=np.array(vectorizer.get_feature_names());\n",
    "\n",
    "    print('\\n Clase ',clases[clase-1])\n",
    "    print(terms[indices[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3\n",
    "\n",
    "Vectorizar el corpus usando TfidfVectorizer y entrenar un clasificador de tipo Multinomial Naive Bayes (MultinomialNB). Para empezar, hagan un simple train_test_split de los datos y vean la performance (accuracy) en el set de validacion.\n",
    "\n",
    "Luego vean si pueden mejorar esa performance optimizando el parámetro alpha del modelo haciendo una Gridsearch cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train,test=train_test_split(data,stratify=data['class'],random_state=3);\n",
    "\n",
    "vectorizer=TfidfVectorizer();\n",
    "X_train=vectorizer.fit_transform(train['news']);\n",
    "y_train=train['class'];\n",
    "\n",
    "X_test=vectorizer.transform(test['news']);\n",
    "y_test=test['class'];\n",
    "\n",
    "nbc=MultinomialNB();\n",
    "\n",
    "nbc.fit(X_train,y_train);\n",
    "y_pred=nbc.predict(X_test);\n",
    "\n",
    "print('Accuracy:',accuracy_score(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizamos en alpha\n",
    "\n",
    "vectorizer=TfidfVectorizer();\n",
    "\n",
    "X=vectorizer.fit_transform(data['news']);\n",
    "y=data['class'];\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=3,shuffle=True);\n",
    "\n",
    "params={'alpha':np.arange(0.01,1,0.05)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,iid=False);\n",
    "GS_CV.fit(X,y);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4\n",
    "\n",
    "Repetir lo anterior removiendo stopwords al vectorizar.\n",
    "\n",
    "¿Cambia la performance?  ¿Cuánto se redujo la dimensionalidad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english');\n",
    "stop_words.append('39');\n",
    "stop_words.append('reuters');\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words);\n",
    "\n",
    "X_train=vectorizer.fit_transform(data['news']);\n",
    "y_train=data['class'];\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True);\n",
    "params={'alpha':np.arange(0.01,1,0.05)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,iid=False);\n",
    "GS_CV.fit(X_train,y_train);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5\n",
    "\n",
    "Utilizar los parámetros min_df y max_df para remover términos que aparecen en muy pocos documentos o que aparecen en demasiados (stopwords específicas del corpus).\n",
    "\n",
    "Reentrenar el modelo y ver si cambia la performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words,min_df=2,max_df=0.8);\n",
    "X_train=vectorizer.fit_transform(data['news']);\n",
    "y_train=data['class'];\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True);\n",
    "params={'alpha':np.arange(0.1,1,0.05)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,iid=False);\n",
    "GS_CV.fit(X_train,y_train);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 6\n",
    "\n",
    "Repetir el análisis incluyendo bigramas en la vectorizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n-grams\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words,min_df=2,ngram_range=(1,2));\n",
    "\n",
    "X_train=vectorizer.fit_transform(data['news']);\n",
    "y_train=data['class'];\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True);\n",
    "params={'alpha':np.arange(0.05,0.15,0.01)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,iid=False);\n",
    "GS_CV.fit(X_train,y_train);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
