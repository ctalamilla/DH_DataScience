{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafío: Clasificación de artículos de diario\n",
    "\n",
    "Trabajaremos con un dataset de noticias de los diarios Clarin y Pagina12. El objetivo de la práctica será implementar un modelo que permita predecir de qué diario proviene una noticia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importamos los datos\n",
    "\n",
    "* Importen los datos con pandas y generen un dataframe agregando una columna 'clase' que indique si son noticias de Clarin o de Pagina12.\n",
    "\n",
    "Las noticias de Clarin se encuentran en '../Data/clarin.csv' y las de Pagina12 en '../Data/pagina12.csv'.\n",
    "\n",
    "* Concatenen ambos data sets en un solo dataframe.\n",
    "\n",
    "* ¿Cuántas noticias tenemos de cada diario?\n",
    "\n",
    "* ¿Qué columnas tiene el dataframe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clarin = pd.read_csv('../Data/clarin.csv')\n",
    "df_clarin['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p12 = pd.read_csv('../Data/pagina12.csv')\n",
    "df_p12['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_clarin,df_p12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Noticias de clarin:',(df['class']==0).sum())\n",
    "print('Noticias de pagina:',(df['class']==1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Limpieza\n",
    "\n",
    "#### 2.1 Faltantes\n",
    "\n",
    "A partir del dataset observamos que los campos que probablemente contengan el vocabulario relevante son \"cuerpo\", \"título\" y \"resumen\".\n",
    "\n",
    "* Saquen del análisis los registros que no tienen cuerpo o título disponible\n",
    "\n",
    "* Completen los resúmenes faltantes con una campo en blanco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['cuerpo','titulo'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['resumen'].isnull(),['resumen']]='';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['resumen'].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Suplementos relevantes\n",
    "\n",
    "Para mejorar la clasificación es conveniente retirar las secciones donde los dos diarios utilizan un vocabulario similar y muy específico del dominio como, por ejemplo, las relacionadas a deportes.\n",
    "\n",
    "* Miren las secciones dentro de la columna 'suplemento': Ojo que hay secciones de deportes con diferente nombre por ejemplo '/deportes/futbol/'\n",
    "\n",
    "* Remuevan las noticias de deportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportes=df['suplemento'].apply(lambda x: 'deporte' in str(x).lower());\n",
    "df.drop(index=df[deportes].index,inplace=True)\n",
    "\n",
    "df.drop(index=df[df['suplemento']=='/br/'].index,inplace=True) # Tiro articulos en portugues\n",
    "\n",
    "df.shape\n",
    "#espectaculos=df['suplemento'].apply(lambda x: 'espectaculos' in str(x).lower());\n",
    "#df.drop(index=df[espectaculos].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Corpus\n",
    "\n",
    "El data set tiene informacion relevante en las columnas 'título', 'resumen' y 'cuerpo', de modo que podemos generar una nueva columna que sea la concatenación de estas tres. \n",
    "\n",
    "* Generen dicha columna, que será nuestro corpus de documentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noticia']=df['titulo']+' '+df['resumen']+' '+df['cuerpo'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noticia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo\n",
    "\n",
    "### 3.1 \n",
    "\n",
    "* Vectoricen el corpus de textos resultante con CountVectorizer, removiendo stopwords. Usen el argumento strip_accents='unicode' para remover tildes del texto.\n",
    "\n",
    "Atención: las stopwords importadas de nltk contienen tildes. Elimínenlas antes de vectorizar el corpus.\n",
    "\n",
    "* ¿Cuál es la dimensión de la matriz de features?\n",
    "\n",
    "* Apliquen un modelo Naive Bayes con un split simple entre train y test. \n",
    "\n",
    "* ¿Cuál es el accuracy obtenido?  \n",
    "\n",
    "* Dibujen la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluimos stopwords\n",
    "stop_words = stopwords.words('spanish');\n",
    "stop_words=[unidecode.unidecode(word.lower()) for word in stop_words ]; # quitamos acentos\n",
    "\n",
    "Train,Test=train_test_split(df[['noticia','class']],stratify=df['class'],random_state=3);\n",
    "\n",
    "Train.reset_index(drop=True,inplace=True);\n",
    "Test.reset_index(drop=True,inplace=True);\n",
    "\n",
    "vectorizer=CountVectorizer(strip_accents='unicode',stop_words=stop_words);\n",
    "vectorizer.fit(Train['noticia']);\n",
    "\n",
    "X_train=vectorizer.transform(Train['noticia']);\n",
    "X_test=vectorizer.transform(Test['noticia']);\n",
    "\n",
    "y_train=Train['class'];\n",
    "y_test=Test['class'];\n",
    "\n",
    "NBC=MultinomialNB();\n",
    "\n",
    "NBC.fit(X_train.todense(),Train['class']);\n",
    "\n",
    "test_pred=NBC.predict(X_test.todense());\n",
    "\n",
    "print('Training set shape:',X_train.shape)\n",
    "\n",
    "print('\\nTest Accuracy:',accuracy_score(Test['class'],test_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:\\n',confusion_matrix(Test['class'],test_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimización del modelo\n",
    "\n",
    "* Hagan una gridsearch cross validation variando el hiperparámetro alpha en el rango (0;0.1)\n",
    "\n",
    "* Vean la accuracy y la matriz de confusión obtenida con el mejor modelo, en el test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.arange(0.1,2,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True);\n",
    "\n",
    "params={'alpha':np.arange(0.1,2,0.1)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=-1);\n",
    "GS_CV.fit(X_train,y_train);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)\n",
    "\n",
    "best_model=GS_CV.best_estimator_;\n",
    "best_model.fit(X_train,y_train); # entrenamos en todo el training set\n",
    "\n",
    "print('\\nTest set:\\n')\n",
    "\n",
    "test_pred=best_model.predict(X_test);\n",
    "\n",
    "print('accuracy:',accuracy_score(Test['class'],test_pred))\n",
    "\n",
    "print('\\nconfusion:\\n',confusion_matrix(Test['class'],test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análisis de los resultados \n",
    "\n",
    "El modelo entrenado tiene el atributo \"feature_log_prob\" que contiene el logaritmo de los coeficientes $\\theta_{yi}$, que representan la probabilidad de que el término i-ésimo pertenezca a la clase $y$.\n",
    "\n",
    "¿Cuáles son las features (palabras) que mejor separan a las dos clases?\n",
    "\n",
    "* Calculen el cociente entre los logaritmos de los coeficientes estimados para la clase \"clarin\" y para \"pagina12\". ¿Cuáles términos mustran mayor diferencia entre ambos valores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_importance=best_model.feature_log_prob_[0]/best_model.feature_log_prob_[1];\n",
    "# Los valores son log-prob (negativos) de modo que relative_importance < 1 implica que\n",
    "# el coeficiente asignado en la clase 0 (clarin) es mayor que en la clase 1 (pagina) y viceveras\n",
    "\n",
    "sns.distplot(relative_importance)\n",
    "plt.xlabel('Importancia Relativa')\n",
    "\n",
    "features=np.array(vectorizer.get_feature_names());\n",
    "\n",
    "indices=np.argsort(relative_importance);\n",
    "\n",
    "print('Términos representativos de Clarín:\\n')\n",
    "print(features[indices[:100]])\n",
    "\n",
    "print('\\nTérminos representativos de Página12:\\n')\n",
    "print(features[indices[-100:]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
