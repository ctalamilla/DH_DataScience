{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: Generando nuestra propia API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción e instrucciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta práctica es simular nuestro propio servidor para poner a disposición nuestra API para un cliente interno. \n",
    "\n",
    "Vamos a utilizar un set de datos adaptados de una competencia de kaggle ya cerrada: https://www.kaggle.com/kevinmh/fifa-18-more-complete-player-dataset\n",
    "\n",
    "En el dataset vamos a encontrar todos los jugadores de fútbol argentino que están en el juego fifa-18, y contaremos con las siguientes variables: \n",
    "- **ID**: un número único que identifica al jugador en toda la base.\n",
    "- **full_name**: nombre completo del jugador.\n",
    "- **age**\n",
    "- **club**: del jugador\n",
    "- **height_cm**\n",
    "- **weight_kg**\n",
    "- **puntaje_global**: puntaje que identifica la habilidad general del jugador.\n",
    "- **potencia**: potencia física del jugador.\n",
    "- **ritmo**: velocidad de aceleración del jugador.\n",
    "- **disparos**: nivel de precisión y potencia de sus remates.\n",
    "- **pases**: nivel de precisión en sus pases.\n",
    "- **amagues**: nivel de habilidad para amagar a un rival.\n",
    "- **defensa**: capacidad defensiva general del jugador.\n",
    "- **físico**: estado físico del jugador (nos indicaría qué tan rápido se cansa)\n",
    "\n",
    "Su **misión** es simular en esta notebook un **SERVIDOR** donde está alojada su API y, desde otra notebook (Clase_PIckle_shelve_flask_CLIENT) hacer los pedidos simulando que son un **CLIENTE**.\n",
    "\n",
    "Su API va a hacer varias cosas: va a entrenar un modelo, va a realizar un gráfico de los coeficientes del modelo y, además, si le pasamos datos de un jugador nos va a devolver la predicción de su **puntaje_global**.\n",
    "\n",
    "La API tiene que tener **tres funciones/endpoints**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PRIMERA FUNCIÓN/ENDOPOINT:** entrenar el modelo\n",
    "> La primera de esas funciones/endpoint entrena el modelo y tiene que recibir:\n",
    "- (i) el dataframe con la base de jugadores argentinos \"fifa_18_jugadores_argentinos_dos.csv\" que se encuentra dentro de la carpeta Data (tienen que pasar la base como un json), \n",
    "- (ii) la lista de predictores, \n",
    "- (iii) la variable target, \n",
    "- (iii) una dirección donde guardar el modelo entrenado, \n",
    "- (iv) un nombre para darle al modelo y a los estimadores que necesiten usar, y \n",
    "- (v) una lista de alphas (van a usar  ElasticNet -eNetCv) y determinar el mejor alpha.\n",
    "\n",
    "> El **output** de esta primera función/endpoint tiene que ser un json que contenga: \n",
    "- (i) el mejor alpha elegido por el modelo\n",
    "- (ii) el resultado de train del modelo\n",
    "- (iii) el resultado de test del modelo\n",
    "- Además, tienen que usar la librería PICKLE para guardar el modelo entrenado y los estimadores que hayan utilizado.\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SEGUNDA FUNCIÓN/ENDOPOINT:** coeficientes del modelo\n",
    "> La segunda función/endpoint, va a grabar en la misma carpeta que el modelo una figura de los coeficientes del modelo entrenado. Los inputs tienen que incluir, por lo menos:\n",
    "- (i) dirección donde está guardado el modelo\n",
    "- (ii) nombre del modelo guardado\n",
    "\n",
    "> El **output** de esta segunda función/endpoint tiene que ser: \n",
    "- (i) un json con los valores de los coeficientes\n",
    "- (ii) un figura .jpg que se guarde en la carpeta donde está guardado el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TERCERA FUNCIÓN/ENDOPOINT:** predicciones\n",
    "> La tercera función/endpoint, va a realizar predicciones cuando nosotros le pasemos los datos de un nuevo jugador. Los inputs tiene que incluir, por lo menos: \n",
    "- (i) dirección donde está guardado el modelo\n",
    "- (ii) nombre del modelo y estimadores\n",
    "- (iii) features del jugador del cual queremos hacer la predicción sobre su puntaje total\n",
    "\n",
    "> El **output** de esta tercera función/endpoint tiene que ser: \n",
    "- (i) un json con la predicción del puntaje total del jugador.\n",
    "\n",
    "<br>  \n",
    "#### ¡Manos a la obra!\n",
    "<br>  \n",
    "\n",
    "<img src=\"img/05_flask.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>Para tener en cuenta:</b> Tal como vimos en las notebooks de la práctica guiada y en los checkpoints, por un lado tienen que definir las funionces/endopoints de su API y, al final de la notebook, ejecutar la línea de código <b>app.run(host='0.0.0.0')</b>. Recuerden que esta línea hace que la notebook simule ser un servidor que está poniendo a disposición a nuestra API para que podamos acceder desde la otra notebook que es el cliente (usando la librería request). Y por último: mientras esté en ejecución esta línea de código, acuérdense de que no pueden ejecutar ninguna otra celda de esta notebook. </label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerden importar todas las librerías que su servidor va a necesitar para poner la API a disposición y para hacer entrenar el modelo y guardarlo a disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from flask import  Flask, request, jsonify, render_template\n",
    "\n",
    "# Recuerden que en este entorno de \"servidor\" tenemos que tener importadas las librerías que necesitamos usar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNetCV as eNetCv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Librerías y configuración para visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context(rc={'xtick.labelsize': 13,'ytick.labelsize': 13, 'axes.labelsize': 16, 'axes.titlesize': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRIMERA FUNCIÓN/ENDOPOINT:** entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos nuestra API\n",
    "app = Flask('Predictor de jugadores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos nuestro primera función asociada a su endpoint. Usamos el método POST ya que vamos a enviar información\n",
    "# al servidor: nuestro modelo entrenado junto con sus estimadores.\n",
    "@app.route('/entrenar_modelo',methods=['POST'])\n",
    "def entrenar_modelo():\n",
    "    \n",
    "    # la función \"request.get_json\" de Flask para capturar la información que le envíemos a la API\n",
    "    data = request.get_json(force=True)\n",
    "    \n",
    "    # Separamos la información de data y usamos json.loads() para transformar el dataframe que está en formato\n",
    "    # json a un diccionario y luego lo convertimos en un DataFrame.\n",
    "    df=pd.DataFrame(json.loads(data['base']))\n",
    "    columns_name=data['lista_predictores']\n",
    "    target_name=data['target']\n",
    "    alphas_modelo=data['alphas']\n",
    "    directorio_modelo=data['directory']\n",
    "    nombre_modelo=data['name']\n",
    "        \n",
    "    # Separamos las \"X\" y la \"y\" para entrenar nuestro modelo\n",
    "    X = df[columns_name]\n",
    "    y = df[target_name]\n",
    "    \n",
    "    # Hacemos el train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state=2020)\n",
    "    \n",
    "    # Hacemos el cross validation para ver el mejor alpha y entrenamos el modelo\n",
    "    k=KFold(n_splits=3, shuffle=True, random_state=45)\n",
    "    modelo = eNetCv(alphas=alphas_modelo, cv = k)\n",
    "    \n",
    "    # Escalamos los datos para que estén todos en la misma unidad (dado que estamos incluyendo la edad,\n",
    "    # peso y altura, por ejemplo)\n",
    "    scalador=StandardScaler()\n",
    "    X_train_sc=scalador.fit_transform(X_train)\n",
    "    X_test_sc=scalador.transform(X_test)\n",
    "    \n",
    "    # Entrenamos el modelo usando los datos escalados\n",
    "    modelo.fit(X_train_sc, y_train)\n",
    "        \n",
    "    # Evaluamos el R2 de train y de test\n",
    "    result_train=r2_score(y_train,modelo.predict(X_train_sc))\n",
    "    result_test=r2_score(y_test,modelo.predict(X_test_sc))\n",
    "        \n",
    "    # Guardamos el modelo entrenado en una carpeta que se llama \"modelos\" que esté a la misma altura que la notebook\n",
    "    dir_=directorio_modelo+'/'+nombre_modelo+'.pkl'\n",
    "    with open(dir_, 'wb') as modelo_pkl:\n",
    "        pickle.dump(modelo, modelo_pkl)\n",
    "        \n",
    "    # Guardamos también el escalador entrenado, ya que lo vamos a tener que aplicar a los casos nuevos que le pasemos\n",
    "    dir_scaler=directorio_modelo+'/'+nombre_modelo+'_scaler.pkl'\n",
    "    with open(dir_scaler, 'wb') as scaler_pkl:\n",
    "        pickle.dump(scalador, scaler_pkl)\n",
    "        \n",
    "    \n",
    "    # La función devuelva el alpha elegido por el modelo, el resultado de train y el de test.\n",
    "    return {'alpha_elegido':float(modelo.alpha_),'resultado_train':float(result_train), 'resultado_test':float(result_test)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SEGUNDA FUNCIÓN/ENDOPOINT:** coeficientes del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos nuestro segunda función asociada a su endpoint. Usamos el método POST ya que vamos a enviar información\n",
    "# al servidor: la figura con los coeficientes de nuestro modelo entrenado\n",
    "@app.route('/plot_coeficientes',methods=['POST'])\n",
    "def plotear_coeficientes():\n",
    "    \n",
    "    # la función \"request.get_json\" de Flask para capturar la información que le envíemos a la API\n",
    "    data = request.get_json(force=True)\n",
    "    \n",
    "    # Separamos la información de data.\n",
    "    direccion=data['direccion']\n",
    "    name_modelo=data['nombre_modelo']\n",
    "    colores=data['paleta_colores']\n",
    "    size=data['tamano']\n",
    "    predictores=data['predictores']\n",
    "    \n",
    "    # Levantamos el modelo que tenemos grabado en disco con el llamado anterior\n",
    "    # (hay que tener en cuenta que tenemos que hacer el llamado anterior primero cuando estemos en la notebook\n",
    "    # que simula ser el cliente).\n",
    "    dir_input=direccion+'/'+name_modelo+'.pkl'\n",
    "    with open(dir_input, 'rb') as modelo_pkl:\n",
    "        modelo_load = pickle.load(modelo_pkl)\n",
    "        \n",
    "    # Levantamos los coeficientes ya calculados del modelo\n",
    "    # Los ponemos en un dataframe junto con el nombre de los predictores.\n",
    "    # Pasamos los coeficientes a valores absolutos y los ordenamos de mayor a \n",
    "    # menor para hacer el gráfico.\n",
    "    coef=abs(pd.DataFrame({'coeficientes':modelo_load.coef_},index=predictores)).\\\n",
    "                           sort_values(by='coeficientes',ascending=False)\n",
    "    \n",
    "    \n",
    "    # Generamos la figura y la guardamos\n",
    "    fig, ((ax1)) = plt.subplots(1,1,gridspec_kw={'hspace': 0.45, 'wspace': 0.15},figsize=size)\n",
    "    fig.suptitle(\"Coeficientes\",y=0.96,x=0.135,fontsize=24,fontweight='bold')\n",
    "\n",
    "    ax1 = sns.barplot(x=\"index\", y=\"coeficientes\", data=coef.reset_index(),ax=ax1,palette=colores)\n",
    "    ax1.xaxis.set_label_text('Coeficientes')\n",
    "    ax1.yaxis.set_label_text('Valores coeficientes')\n",
    "    plt.close(); #usamos esta línea para que la figura no se imprima en pantalla\n",
    "    \n",
    "    # guardamos la figura donde se guardan los modelos\n",
    "    dir_figura=direccion+'/'+name_modelo+'.jpg'\n",
    "    fig.savefig(dir_figura,dpi=150)\n",
    "    \n",
    "    # devolvemos un json con los coeficientes. Dado que las API devuelven la información en formato\n",
    "    # json, usamos el método de los dataframes \".to_json()\" para poder retornar los datos de los\n",
    "    # coeficientes.\n",
    "    return coef.to_json()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TERCERA FUNCIÓN/ENDOPOINT:** predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que en este endopint no vamos a guardar nada en el servidor, sino recibir información, usamos \n",
    "# el método GET. \n",
    "@app.route(\"/prediccion\",methods=['GET'])\n",
    "def predecir_puntaje():\n",
    "    \n",
    "    # Usamos request.args para tomar las query que le pasamos a la URL\n",
    "    direccion=request.args['direccion']\n",
    "    name_modelo=request.args['nombre_modelo']\n",
    "    name_scaler=request.args['nombre_scalador']\n",
    "    caso_to_predict=request.args['features']\n",
    "    \n",
    "    # Levantamos el modelo y el escalador que ya tenemos entrenado\n",
    "    with open(direccion+'/'+name_modelo+'.pkl', 'rb') as modelo_pkl:\n",
    "        modelo_load = pickle.load(modelo_pkl)\n",
    "        \n",
    "    with open(direccion+'/'+name_scaler+'.pkl', 'rb') as scaler_pkl:\n",
    "        scaler_load = pickle.load(scaler_pkl)\n",
    "        \n",
    "    # re-escalamos los datos con el escalador entrenado (tengan en cuenta que el escalador va a estar\n",
    "    # esperando una estructura como un DataFrame)\n",
    "    scaled_case=scaler_load.transform(pd.DataFrame(json.loads(caso_to_predict),index=[0]))\n",
    "    \n",
    "    # realizamos la prediccion\n",
    "    prediccion=modelo_load.predict(scaled_case)\n",
    "    \n",
    "    return {'prediccion':float(prediccion)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Disponibilizamos nuestra API**\n",
    "<br>  \n",
    "\n",
    "<img src=\"img/forrest.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora ejecutamos esta línea de código que pone a disposición los tres endpoints que armamos arriba. \n",
    "# Es hora de ir a la otra notebook en la que simulamos ser un cliente y hacer los llamadados para cada\n",
    "# uno de estos tres endopoints...\n",
    "app.run(host='0.0.0.0', port= 5009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
