{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En esta clase vamos a comparar las performance de los distintos modelos basados en ensambles de árboles:\n",
    "\n",
    "* Árbol de ensamble -> Bagging\n",
    "* Árbol de ensamble -> Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "En esta clase usaremos un dataset con info de películas (\"Movie_classification.csv\").  \n",
    "Este dataset esta conformado por los siguientes features:  \n",
    "\n",
    " *   **Marketing expense:**    (float64)    Gasto total en Marketing      \n",
    " *   **Production expense:**   (float64)    Gasto total de Producción\n",
    " *   **Multiplex coverage:**   (float64)    Cobertura promedio de Multiplex\n",
    " *   **Budget:**               (float64)    Presupuesto\n",
    " *   **Movie_length:**         (float64)    Duración de la película\n",
    " *   **Lead_ Actor_Rating:**   (float64)    Puntaje sobre el actor principal\n",
    " *   **Lead_Actress_rating:**  (float64)    Puntaje sobre la actriz principal\n",
    " *   **Director_rating:**      (float64)    Puntaje sobre el Director\n",
    " *   **Producer_rating:**      (float64)    Puntaje sobre el Productor\n",
    " *   **Critic_rating:**        (float64)    Puntaje que le puso la crítica\n",
    " *   **Trailer_views:**        (int64)      Cantidad de vistas del Trailer\n",
    " *   **3D_available:**         (object)     Si esta disponible en 3D (Yes/No)\n",
    " *   **Time_taken:**           (float64)    Duración de la película\n",
    " *   **Twitter_hastags:**      (float64)    Cantidad de menciones en twitter\n",
    " *   **Genre:**                (object)     Genero de la película\n",
    " *   **Avg_age_actors:**       (int64)      Edad promedio de los actores\n",
    " *   **Num_multiplex:**        (int64)      Cantidad de Multiplex\n",
    " *   **Collection:**           (int64)      Recaudación\n",
    " *   **Start_Tech_Oscar:**     (int64)      Si recibió un oscar o no.\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 - Importar datos\n",
    "\n",
    "1) Leamos los datos de \"Movie_classification.csv\" y lo guardamos en un dataframe de pandas.  \n",
    "2) Veamos cuántos registros hay en cada DataFrame y de qué tipos son los datos de cada columna.   \n",
    "3) Veamos los primeros registros de cada DataFrame para verificar que los datos fueron importados correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Movie_classification.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 - Imputación de valores faltantes\n",
    "\n",
    "Veamos si existen valores faltantes y en tal caso imputemos los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vemos la cantidad de valores que tiene cada columna\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observamos que time_taken es la unica columna que no tiene 506 observaciónes, \n",
    "# por ende vamos a imputar los valores faltantes utilizando la media.\n",
    "df['Time_taken'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_taken'].fillna(value = df['Time_taken'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3 - Generación de Variables Dummies.\n",
    "\n",
    "Veamos si existen variables categóricas y en tal caso generar variables dummies para dichas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.loc[df.dtypes==\"object\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['3D_available','Genre']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,columns = [\"3D_available\",\"Genre\"],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4 - Features, Target\n",
    "\n",
    "Construyamos una matriz de features (X) y el vector target (Y) para predecir `Start_Tech_Oscar` en el dataset de datos completos\n",
    "\n",
    "¿Qué valores toma la variable `Start_Tech_Oscar` en el dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,df.columns!=\"Start_Tech_Oscar\"]\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Start_Tech_Oscar\"]\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5 - Train Test Split\n",
    "\n",
    "Constuyamos los conjuntos de train y test, asignando el 70% de los registros a train y el 30% a test\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6 - Construimos un modelo utilizando Bagging\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html  \n",
    "\n",
    "1) creamos un clasificador de árbol simple.  \n",
    "2) con este clasificador simple, generar el meta-modelo basado en la técnica de Bagging (utilizar 1000 estimadores).  \n",
    "3) entrenamos el modelo de Bagging.  \n",
    "4) calculamos la matriz de confusión  \n",
    "5) calculamos el accuracy tanto para el dataset de prueba.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clftree = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(base_estimator=clftree, n_estimators=1000,\n",
    "                            bootstrap=True, n_jobs=-1,\n",
    "                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, bag_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, bag_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 7 - Construimos un modelo utilizando Random Forest\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Algunos de los parámetros más importantes son los siguientes:\n",
    "\n",
    "* `n_estimators`: el número de iteraciones (o sea, de `base_estimators`) para entrenar\n",
    "* `criterion`: define el criterio de impureza para evaluar la calidad de las particiones (por defecto, es `gini`) \n",
    "* `max_features`: la cantidad de features que extraerá para entrenar cada `base_estimator`. Por default es igual a `sqrt(X.shape[1])`\n",
    "* `bootstrap` y `bootstrap_features`: controla si tanto los n_samples como las features son extraidos con reposición.\n",
    "* `max_depth`: la pronfundidad máxima del árbol\n",
    "* `min_samples_leaf`: el número mínimo de n_samples para constituir una hoja del árbol (nodo terminal)\n",
    "* `min_samples_split`: el número mínimo de n_samples para realizar un split.\n",
    "\n",
    "y varios otros que pueden llegar a ser importantes al momento de realizar el tunning. En general, los más importantes suelen ser: `n_estimators`, `max_features`, `max_depth` y `min_samples_leaf`.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "1) Generaramos el meta-modelo basado en la técnica de Random Forest (utilizar 1000 estimadores)  \n",
    "2) entrenamos el modelo   \n",
    "3) calculamos la matriz de confusión    \n",
    "4) calculamos el accuracy tanto para el dataset de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1 ,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, rf_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, rf_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 8 - Construimos un modelo utilizando ExtraTreesClassifier()\n",
    "\n",
    "Este modelo es una variación del Random Forest que busca ser más rápido. Esto lo hace evitando buscar el punto de split optimo en cada nodo de los árboles, sino que por el contrario, selecciona un split de forma aleatoria. Esto hace que el modelo se vuelva más veloz, pudiendo alcanzar o incluso superar la precisión alcanzada por Random Forest.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html  \n",
    "https://www.youtube.com/watch?v=Q1qpG7gwix4\n",
    "\n",
    "### Ejercicio\n",
    "1) Generaramos el meta-modelo basado en la técnica de ExtraTrees (utilizar 10000 estimadores)  \n",
    "2) entrenamos el modelo   \n",
    "3) calculamos la matriz de confusión    \n",
    "4) calculamos el accuracy tanto para el dataset de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=10000, class_weight='balanced', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, et.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, et.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
