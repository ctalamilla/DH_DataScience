{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota: esta notebook ya fue presentada en la clase de Modelos de Ensamble**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de Árboles de Decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_toc_2\"></a> \n",
    "## Tabla de Contenidos\n",
    "\n",
    "[Introducción](#section_introduccion)\n",
    "\n",
    "[Marco Teórico](#section_mteorico_2)\n",
    "- [Bagging](#section_bagging)\n",
    "- [Random Forest](#section_RF)\n",
    "- [Boosting](#section_boosting)\n",
    "\n",
    "[Referencias](#section_referencias_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_introduccion\"></a> \n",
    "## Introducción\n",
    "Como mencionamos en la clase de CART, existen modelos de árboles de decisión simples, y árboles de decisión de Ensamble. Vimos también que los primeros, si bien son fáciles de interpretar, no pertenecen al grupo de modelos que ofrecen mayor precisión y que ésto se debía en gran medida a la varianza propia de dichos modelos. Como alternativa superadora surgieron los árboles de decisión por ensamble. Los modelos de Ensamble logran reducir la varianza inherente de los árboles de decisión y han logrados muy buenos resultados en cuanto a la precisión alcanzada, sin embargo ésto es a costa de perder la facilidad de interpretación que poseían los árboles de decisión simples.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_mteorico_2\"></a> \n",
    "## Marco teórico\n",
    "\n",
    "Los métodos de ensamble de modelos o métodos combinados intentan ayudar a mejorar el rendimiento de los modelos de Machine Learning al mejorar su precisión. Este es un proceso mediante el cual se construyen estratégicamente varios modelos de Machine Learning para resolver un problema particular.  \n",
    "\n",
    "### Aprendizaje por Ensamble:\n",
    "\n",
    "Esta técnica se basa en los siguientes principios:\n",
    "- Entrenar differentes modelos sobre el mismo dataset.\n",
    "- Permitir que cada modelo haga sus propias predicciones.\n",
    "- Generar un meta-modelo que realice una agregación de las predicciones individuales de los distintos modelos.\n",
    "- Generar una predicción final usando el meta-modelo que sea más robusta que las predicciones de los modelos individualmente.\n",
    "\n",
    "Debemos destacar que esta técnica logra mayor eficiencia cuando los modelos individuales son construidos de manera tal que logran volverse  \"buenos\" para predecir distintos escenarios. Otra forma de ver este mismo concepto es que si uno de los modelos individuales hace predicciones muy desacertadas en un determinado escenario, el resto de los modelos individuales estaría en condiciones de compensarlo. \n",
    "Es por ésto que las predicciones del meta-modelo se vuelven más robusta.\n",
    "\n",
    "<img style=\"float: center;\" src=\"img/ensamble_01.png\">\n",
    "\n",
    "Vale destacar que al utilizar la técnica de ensamble se podrían utilizar modelos distintos. A continuación nos vamos a enfocar en algunos meta-modelos muy reconocidos basados en modelos de árboles de decisión: \n",
    "- **Bagging:** \n",
    "- **Random Forest**  \n",
    "- **Boosting**  \n",
    "    - **Gradient Boost**\n",
    "    - **ADA Boost**  \n",
    "    - **XG Boost**  \n",
    "    \n",
    "A continuación veremos en mayor detalle estas técnicas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_bagging\"></a> \n",
    "### Bagging:\n",
    "Como mencionamos anteriormente, el problema inherente de los árboles de decisión es la alta varianza que poseen. Para comprender esto pensemos en el siguiente ejercicio: supongamos que ya hemos contruido un árbol de decisión sobre un dataset determinado. Ahora si dividimos dicho dataset en 2 partes y luego generamos un árbol de decisión para cada una de las partes, lo más probable es que obtengamos 2 nuevos árboles muy disímiles entre sí e incluso muy diferentes al árbol que generamos para el dataset original. \n",
    "\n",
    "También mencionamos que los métodos de ensamble ayudan a reducir el error por varianza aumentando asi el accuracy final del modelo. Veremos a continuación cómo funciona el método de Bagging:  \n",
    " \n",
    "El nombre **Bagging** proviene de: **Bootsrap Aggregation**, veamos entonces primero qué significa Boostrap. \n",
    "\n",
    "> #### El método Bootstrap:\n",
    "El método Bootstrap consiste en generar a partir de un dataset original, N nuevos datasets con la misma cantidad de variables independientes, y tomando muestras **con repetición**. Podemos observar que al aplicar la técnica de bootstrap, podemos tener observaciones repetidas dentro de los nuevos datasets.\n",
    ">\n",
    "> <img style=\"float: center;\" src=\"img/bootstrap_01.png\">\n",
    "\n",
    "Por lo tanto, el método de ensamble genera N modelos distintos al entrenar N árboles de decisión con N datasets \"distintos\". Estos datasets son creados a partir de la técnica de bootstrapping.  \n",
    "\n",
    "<img style=\"float: center;\" src=\"img/bagging_02.png\">\n",
    "\n",
    "Dependiendo de la naturaleza de la predicción, ya sea una clasificación o una regresión, vamos a tener que la predicción final del meta-modelos va a ser:  \n",
    "- **Clasificación:** la predicción agregada se consigue mediante votación por mayoría. La clase que indique la mayoría de los modelos será la utilizada.\n",
    "- **Regresión:** la predicción agregada utiliza el promedio de las regresiones de los modelos.\n",
    "\n",
    "Al realizar Bagging no se realiza poda de los N árboles de decisión que luego se utilizan en el ensamble, ésto es para que los mismos tengan el menor error de bias posible aún cuando tengan un gran error de varianza. Luego, mediante la agregación, el méta-modelo reducirá el error de varianza de manera tal que:  \n",
    "\n",
    "si la varianza de los modelos de árbol de decisión es $S^2$,  \n",
    "entonces se espera que el modelo de bagging alcance un error de varianza de $\\frac{S^2}{N}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_RF\"></a> \n",
    "### Random Forest:\n",
    "Como mencionamos antes, los modelos de ensamble funcionan mejor cuando cada uno de los modelos que luego son ensamblados estan lo más descorrelacionado posible. Es decir, que cada uno sea bueno para algún escenario posible. En este sentido, el método de bagging mediante la técnica de bootstrapping genera N modelos. No obstante, estos modelos aún tienen una gran correlación ya que cada modelo de árbol de decisión fue entrenado sobre la totalidad de las variables independientes.  \n",
    "Es por esto que para reducir la correlación entre los modelos y así sacar mejor provecho del potencial de la técnica de ensamble, utilizamos el método de Random Forest. Este último método aplica la misma técnica que bagging pero en lugar de utilizar todas las variables independientes para cada modelo, aplica muestreos sin repeteción. En particular, va a generar los N datasets que luego entrenarán a los N árboles y para ello realizará un  muestreo de las variables independientes tomadas de a M dado un total de P variables independientes (o predictores). \n",
    "\n",
    "<img style=\"float: center;\" src=\"img/random_forest_01.png\">\n",
    "\n",
    "Vemos entonces que Bagging en realidad es un caso particular de Random Forest donde el tamaño de las muestra de los predictores M coincide con el tamano de la población de los mismos P.  \n",
    "\n",
    "Empíricamente se observó que los siguientes valores de M suelen lograr descorrelacionar los modelos:\n",
    "- Para **Regresiones**: M = P/3\n",
    "- Para **Clasificaciones**: M = sqrt(P) \n",
    "No obstante si los predictores estan altamente correlacionados entonces se pueden utilizar valores más pequeños de M.  \n",
    "\n",
    "Finalmente la predicción del meta-modelo funciona exactamente igual que en el caso de Bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_boosting\"></a> \n",
    "### Boosting:\n",
    "Boosting es la tercera técnica de agregación que veremos. La principal diferencia con las técnicas que vimos anteriormente, es que en ellas se entrenaban los modelos independientemente para luego generar un meta-modelo. En el caso de Boosting, se entrenan los modelos de manera secuencial donde cada modelo aprende de los errores del modelo predecesor. \n",
    "\n",
    "Las técnicas de boosting más conocidas son:\n",
    "- Gradient Boosting\n",
    "- ADA Boost\n",
    "- XG Boost\n",
    "\n",
    "Veamos cada una de ellas:\n",
    "\n",
    "**1) Gradient Boost:**\n",
    "\n",
    "Gradient boosting es un método de aprendizaje lento donde los sucesivos modelos de árboles de decisión son entrenados para predecir los residuales del árbol antecesor. El hiperparámetro de Learning Rate ($\\eta$) es un escalar entre 0 < $\\eta$ < 1 que multiplica los residuales para asegurar convergencia. A medida que se reduce el valor de $\\eta$ es recomendable aumentar el número de estimadores N.\n",
    "\n",
    "<img style=\"float: center;\" src=\"img/Gradientboost_01.png\">\n",
    "\n",
    "La predicción del meta-modelo estará luego conformada de la siguiente manera:\n",
    "\n",
    "$y_{pred} = y_1 + \\eta r_1 + ... +  \\eta r_N$\n",
    "\n",
    "En sklearn:  \n",
    "- Regresión: `GradientBoostingRegressor`\n",
    "- Clasificador: `GradientBoostingClassifier`\n",
    "\n",
    "**2) ADA Boost:**\n",
    "\n",
    "El nombre ADA proviene de Adaptative Boosting, que hace referencia a su capacidad de adaptar la importancia de los predictores asignándole mayor peso a aquellos sobre los que se comete más error. Es importante destacar que mientras en Random Forest mencionamos que a los árboles no se los poda, en el caso de Adaboost sucede todo lo contrario: se suelen usar árboles de 1 nodo raíz y 2 nodos hojas. A este tipo de árboles se los conoce como **stump**. Por otro lado, mientras que en Random Forest cada árbol tenía igual voto sobre la predicción final, en el caso de ADA Boost tenemos que los votos de los **stumps** pueden tener más pesos unos que otros.  \n",
    "\n",
    "Tal como mencionamos los métodos de boosting trabajan por definición secuencialmente, con lo cual cada **stump** va a \"aprender\" de la secuencia anterior. Al primer stump se lo va a entrenar con un dataset al cual se le va a asignar los mismos pesos a cada observación (fila). De esta manera si nuestro dataset tiene un total de K observaciones, entonces cada observación tendrá un peso de 1/K siendo todas igual de \"importantes\". \n",
    "\n",
    "Luego se elige el feature que genera la menor entropía o gini y se crea el primer stump con dicho feature. Calculamos ahora la importancia de dicho árbol (o cuanto voto va a tener el mismo sobre la predicción del meta-modelo) dependiendo de la cantidad de error que cometió. Este error lo calculamos sumando los pesos de todas las observaciones que fueron mal clasificadas (este valor va a estar dentro del rango entre 0 y 1). \n",
    "- Si es 1 significa que no logró clasificar nada correctamente y por ende tendra muy poco voto en la predicción final.\n",
    "- Si es 0 significa que clasificó todo perfectamente y por ende tendrá mayor voto en la predicción final.\n",
    "\n",
    "A su vez, para asegurarnos que el siguiente stump pueda aprender del precursor, realizamos un ajuste de los pesos de cada observación del dataset. Ahora ya no serán todas las observaciones de igual peso o importancia, sino que se le dará mayor peso a aquellos observaciones que fueron mal clasificadas, y consecuentemente se le restará peso a aquellas que fueron correctamente clasificadas, de modo que siempre la suma total de pesos sume 1.\n",
    "\n",
    "Ahora el segundo Stump va a utilizar weighted Gini index para seleccionar la mejor partición, y a continuación se repiten todos los pasos:\n",
    "- se calcula el error total de este stump para asignarle el peso a su voto.\n",
    "- se vuelven a recalcular los pesos de cada observacion para entrenar al siguiente stump.\n",
    "\n",
    "\n",
    "<img style=\"float: center;\" src=\"img/Adaboost_01.png\">\n",
    "\n",
    "La predicción del meta-modelo estará luego conformada de la siguiente manera:\n",
    "- Clasificador: \n",
    "    - Voto con pesos\n",
    "    - En sklearn: `AdaBoostClassifier`\n",
    "- Regresión: \n",
    "    - Promedio ponderado\n",
    "    - En sklearn: `AdaBoostRegressor`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3) XG Boost:\n",
    "\n",
    "Finalmente el XG Boosting es un caso especifico de gradient boosting donde se aplica Regularizacion a la funcion de costo lo cual ayuda a evitar overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section_referencias_2\"></a> \n",
    "## Referencias \n",
    "\n",
    "[volver a TOC](#section_toc_2)\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "\n",
    "https://www.youtube.com/watch?v=LsK-xG1cLYA\n",
    "\n",
    "https://www.youtube.com/watch?v=3CC4N4z3GJc\n",
    "\n",
    "https://www.youtube.com/watch?v=m-S9Hojj1as\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
