{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "---\n",
    "\n",
    "En esta notebook vamos a trabajar con el siguiente dataset:\n",
    "\n",
    "https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "\n",
    "El objetivo de este analisis será predecir si lloverá o no al día siguiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1. EDA y Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Leemos los datos del archivo weatherAUS.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos cuántos valores no-nulos hay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Eliminar las columnas que no nos interesan, entre ellas las que tienen pocos datos (menos de cien mil). Además, eliminar 'Location' y 'Date', ya que no nos interesa el lugar ni fecha (al menos en este análisis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Eliminar todas las filas que tengan valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Para simplificar el preprocesamiento, también eliminar todas las columnas que tengan valores categóricos. ¿Por qué no nos molesta eliminar `RainToday`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Realizar un countplot para ver cuántos casos hay de lluvia y no-lluvia.\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.countplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y hacer el `pairplot` para ver cómo se relacionan las variables. Recuerden que este gráfico puede llevar bastante tiempo. También recuerden que pueden agrandar el gráfico haciendo doble click en él.\n",
    "\n",
    "Ayuda: usar como datos la muestra dada por `data.sample(frac = 0.1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Existe alguna feature, o par de features, que por sí solos sean buenos para predecir si lloverá o no al día siguiente?\n",
    "\n",
    "Hay algunas que parecen *correlacionadas*. Tratamos de cuantificarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# descomentar #\n",
    "###############\n",
    "\n",
    "\n",
    "# corr = data.drop(columns = ['RainTomorrow']).corr(method='pearson') # .corr is used for find corelation\n",
    "# plt.figure(figsize=(14,14))\n",
    "# sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n",
    "#            xticklabels= data.drop(columns = ['RainTomorrow']).columns, \n",
    "#            yticklabels= data.drop(columns = ['RainTomorrow']).columns,\n",
    "#            cmap= 'coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a la correlación, podemos descartar (o no) algunas variables. **Para pensar**, ¿por qué haríamos (o no) esto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Convertir `RainTomorrow` a una variable númerica 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de entrenamiento y casos *benchmark*\n",
    "\n",
    "Generamos casos base contra los cuales comparar nuestros resultados.\n",
    "\n",
    "1. Elegir variables de entrenamiento (empezar con dos) y construir la matriz de features X y el vector target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generar un modelo que diga siempre que NO va a llover y medir su exactitud (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y generar otro modelo que diga siempre que va a llover y medir su exactitud (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2. Bagging\n",
    "\n",
    "Separamos entre train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerden que el objetivo de bagging es entrenar distintos modelos, donde cada uno vea distintas porciones del set de entrenamiento. Entonces, vamos a entrenar distintos árboles de decisión y mostrarles distintas porciones del set de datos. Lo vamos a hacer en un `for`.\n",
    "\n",
    "1. Crear una lista vacía donde guardaremos los modelos entrenados y elegir cuántos modelos entrenar (Empezar por algún valor entre 5 y 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Entrenar cada modelo y guardar cada modelo entrenado en una lista. Para hacer el split, usar la función `train_test_split`. ¿Sobre qué conjunto van a hacer el split?¿Hay que fijar el `random_state`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluar el accuracy de cada modelo usando el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecen estar un poco overfitteados, que era lo que esperábamos.\n",
    "\n",
    "4. Evaluar el accuracy de todo el ensamble usando el conjunto de test. Vamos a hacerlo usando un promedio de las probabilidades que devuelven cada árbol. Si la probabilidad promedio es mayor a 0.5, clasificamos como positivo. Para ello:\n",
    "    1. Inicializar un arreglo de probabilidades del tamaño de la cantidad de instancias del conjunto de test en ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Recorrer la lista de modelos y predecir las probabilidades. Mirar como es el `shape` de ese arreglo predicho. Elegir las probabilidades que correspondan a la clase positiva. Luego, sumarlas al vector que definieron antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Dividir `probs_test_pred` por la cantidad de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Crear las clases predichas (0s y 1s) a partir de comparar la probabilidad predicha con la probabilidad umbral (0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y evaluar la exactitud de todo el ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explorar el `BagginnClassfier` de scikit-learn y algunas de sus características. Usarlo para predecir sobre el train y test, y medir su desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Si usaron dos features, pueden graficar las fronteras de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# descomentar #\n",
    "###############\n",
    "\n",
    "\n",
    "# N = 20 #para no graficar todos los puntos y saturar el grafico\n",
    "\n",
    "# plt.figure(figsize = (8,6))\n",
    "\n",
    "#Grafico Clasificador Sesgado\n",
    "# ax = sns.scatterplot(x = X_test[::N].MaxTemp, y = X_test[::N].Humidity3pm, hue=y_test[::N], palette='Set2')\n",
    "# xlim = ax.get_xlim()\n",
    "# ylim = ax.get_ylim()\n",
    "# xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "#                       np.linspace(*ylim, num=200))\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "# contours = ax.contourf(xx, yy, Z, alpha=0.3, cmap = 'Set2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3. Random Forest\n",
    "\n",
    "Random Forest, además de aplicar Bagging, también selecciona features al azar, de esa manera descorrelaciona aún más los distintos modelos de árbol creados.\n",
    "\n",
    "1. Importar de scikit-learn el modelo `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Investigar sus parámetros. En particular, `n_estimators`, `max_features` y `oob_score`. Luego, crear y entrenar un modelo en el conjunto de train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluar su desempeño en el conjunto de train y de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Cuál es su `oob_score_`?¿Y que son `feature_importances_`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ¿Qué hay en la propiedad `estimators_`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Elegir uno de los `estimators` y evaluar su desempeño sobre train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Está overfiteado?¿Por qué la accuracy sobre el conjunto de train no es 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Hacer y graficar la curva de validación/complejidad para un modelo Random Forest en función del número de estimadores. No usamos CV porque puede llevar bastante tiempo. Si quieren, lo pueden probar después. Además, obtener su oob_score para graficar en la curva de complejidad (No se preocupen por los mensajes de warning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# descomentar #\n",
    "###############\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# train_accuracy = []\n",
    "# test_accuracy = []\n",
    "# oob_scores = []\n",
    "\n",
    "# N_estimadores = [1,2,3,4,5,10,25,50,100,250,500,1000]\n",
    "# for estimadores in N_estimadores:\n",
    "#     print(estimadores)\n",
    "#     clf = RandomForestClassifier(n_estimators=estimadores, n_jobs=-1, oob_score= True, random_state = 42)\n",
    "#     clf.fit(X_train,y_train)\n",
    "    \n",
    "#     y_train_pred = clf.predict(X_train)\n",
    "#     y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "#     train_accuracy.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "#     test_accuracy.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "#     oob_scores.append(clf.oob_score_)\n",
    "    \n",
    "# train_accuracy = np.array(train_accuracy)\n",
    "# test_accuracy = np.array(test_accuracy)\n",
    "# oob_scores = np.array(oob_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# descomentar #\n",
    "###############\n",
    "\n",
    "# plt.figure(figsize = (8,6))\n",
    "# plt.plot(N_estimadores, train_accuracy, label = 'Train')\n",
    "# plt.plot(N_estimadores, test_accuracy, label = 'Test')\n",
    "# plt.plot(N_estimadores, oob_scores, label = 'OOB')\n",
    "# plt.xlabel('Numero de estimadores')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Hacer y graficar la curva de aprendizaje (¿Qué es eso?) para un modelo con 250 estimadores. Puede llevar bastante tiempo, no se preocupen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completar:\n",
    "# clf = ...\n",
    "\n",
    "###############\n",
    "# descomentar #\n",
    "###############\n",
    "\n",
    "#train_sizes, train_scores, valid_scores = learning_curve(clf, X_train, y_train, \n",
    "#                                                         train_sizes = np.linspace(0.0001,1,10),\n",
    "#                                                         scoring = 'accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############\n",
    "# descomentar #\n",
    "###############\n",
    "\n",
    "#plt.figure(figsize = (8,6))\n",
    "#plt.plot(train_sizes, train_scores.mean(axis = 1), color = 'r')\n",
    "#plt.plot(train_sizes, valid_scores.mean(axis = 1), color = 'g')\n",
    "\n",
    "#plt.fill_between(train_sizes, train_scores.mean(axis = 1)- train_scores.std(axis = 1),\n",
    "#                     train_scores.mean(axis = 1)+ train_scores.std(axis = 1), alpha=0.25,\n",
    "#                     color=\"r\")\n",
    "#plt.fill_between(train_sizes, valid_scores.mean(axis = 1) - valid_scores.std(axis = 1),\n",
    "#                     valid_scores.mean(axis = 1) + valid_scores.std(axis = 1), alpha=0.25, color=\"g\")\n",
    "\n",
    "#plt.ylim(0.5,1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Si usaron dos features, pueden graficar las fronteras de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# descomentar #\n",
    "###############\n",
    "\n",
    "# N = 20 #para no graficar todos los puntos y saturar el grafico\n",
    "# clf = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "\n",
    "# plt.figure(figsize = (8,6))\n",
    "\n",
    "#Grafico Clasificador Sesgado\n",
    "# ax = sns.scatterplot(x = X_test[::N].MaxTemp, y = X_test[::N].Humidity3pm, hue=y_test[::N], palette='Set2')\n",
    "# xlim = ax.get_xlim()\n",
    "# ylim = ax.get_ylim()\n",
    "# xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "#                      np.linspace(*ylim, num=200))\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "# contours = ax.contourf(xx, yy, Z, alpha=0.3, cmap = 'Set2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: elegir más features y volver a entrenar.\n",
    "\n",
    "**Para pensar**: ¿qué otras métricas utilizarían para evaluar estos modelos, dadas las características particulares del problema? Comparar con los casos *benchmark* que hicieron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
