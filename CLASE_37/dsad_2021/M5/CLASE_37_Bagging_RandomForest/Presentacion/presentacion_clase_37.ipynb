{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_portada.png\" align=\"center\" width=\"90%\"/></div>\n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 5</div>\n",
    "            <div class=\"div-contenido\">Bagging. Random Forest y Extra Trees.</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- Bagging\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- Extra Trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Bagging (Bootstrap Aggregation)\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src =\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_intro.png\" align = \"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "---\n",
    "\n",
    "**Bagging** o **B**ootstrap **Agg**regat**ing**\n",
    "\n",
    "Bootstrap: Muestreo **con reposición** de las instancias\n",
    "<table>\n",
    "    <tr><td>\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_bagging_1.png\" align=\"center\" /></td>\n",
    "        \n",
    "<td style=\"font-size:16px;\">        \n",
    "<ul>    \n",
    "<li><p>Dada una muestra de datos, se extraen varias muestras, bootstrapped</p>\n",
    "    <p>Esta selección se realiza de manera aleatoria y con reposición.</p></li>\n",
    "<li><p>Una vez que forman las muestras bootstrapped, se entrenan los modelos de manera separada.</p> <p>En general, estos modelos serán modelos con mucha varianza.</p></li>\n",
    "<li><p>La predicción final resulta de combinar las predicciones de todos los modelos base.</p></li>\n",
    "    </ul></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "---\n",
    "\n",
    "El **Bagging** o **B**ootstrap **agg**regat**ing** es un método que consiste en manipular el set de entrenamiento haciendo **remuestreo** para generar k clasificadores base.\n",
    "\n",
    "Bootstrap es el procedimiento de generar, a partir del set de entrenamiento, **k muestras diferentes**.\n",
    "\n",
    "Las muestras se crean de forma independiente haciendo un **muestreo con reposición** sobre los datos de entrenamiento, usando una **distribución de muestreo uniforme**.\n",
    "\n",
    "Luego con estas k muestras, son entrenados k clasificadores.\n",
    "\n",
    "Por último, estos **k modelos distintos son agregados y el resultado será un ensamble que tomará una decisión por voto mayoritario**. En otras palabras, cada modelo base en el ensamble vota con igual peso.\n",
    "\n",
    "El bagging entrena a cada modelo en el ensamble usando un subset aleatorio del set de entrenamiento, con el fin de promover la varianza de los modelos base. Como ejemplo, el algoritmo random forest combina árboles de decisión aleatorios con bagging para lograr una precisión de clasificación muy alta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Muestreo \n",
    "\n",
    "---\n",
    "\n",
    "<table>\n",
    "    <tr><td style=\"width:45%;\">\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_sampling.png\" align=\"center\" />        \n",
    "</td>\n",
    "        \n",
    "<td style=\"font-size:16px;\">        \n",
    "       \n",
    "<p>    \n",
    "Dado un set de entrenamiento estándar D de tamaño n, el bagging genera m nuevos sets de entrenamiento Di , cada uno de tamaño n, muestreando uniformemente en D con reemplazo. \n",
    "</p>\n",
    "<p>    \n",
    "Mediante el muestreo con reemplazo, algunas observaciones pueden repetirse en cada Di. \n",
    "</p>\n",
    "<p>        \n",
    "Los m modelos se entrenan usando las m muestras anteriores y se combinan promediando la salida (para regresión) o votando (para clasificación).\n",
    "</p> \n",
    "    \n",
    "</td>    \n",
    "    \n",
    "</table>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Características\n",
    "\n",
    "---\n",
    "\n",
    "* El Bagging **reduce la varianza** del error de generalización al combinar múltiples clasificadores de base (siempre que estos satisfagan los requisitos anteriores).\n",
    "\n",
    "* Si el clasificador base es estable, entonces el error del ensamble se debe principalmente al sesgo, y el bagging puede no ser efectivo.\n",
    "\n",
    "* Dado que cada muestra de datos de entrenamiento es igualmente probable, el bagging no es muy susceptible a overfitting con datos ruidosos.\n",
    "\n",
    "* Dado que proporcionan una manera de reducir el overfitting, los métodos de bagging **funcionan mejor con modelos fuertes y complejos** (por ejemplo, árboles de decisión completamente desarrollados), en contraste a los métodos de boosting que usualmente funcionan mejor con modelos débiles (por ejemplo, árboles de decisión superficiales).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Bagging** o **B**ootstrap **Agg**regat**ing**\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_bagging.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging Predictors (Abstract del <a href=\"https://www.stat.berkeley.edu/~breiman/bagging.pdf\" target=\"_blank\">paper</a> Leo Breiman 1994) \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "“Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. \n",
    "\n",
    "The **aggregation averages** over the versions when predicting a numerical outcome and does a **plurality vote** when predicting a class. \n",
    "\n",
    "The multiple versions are formed by making **bootstrap replicates of the learning set** and using these as new learning sets. \n",
    "\n",
    "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. \n",
    "\n",
    "The **vital element is the instability of the prediction method**. \n",
    "\n",
    "**If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy**.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging de árboles de decisión\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_bagging_tree.png\" align=\"center\" width= \"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementación\n",
    "\n",
    "---\n",
    "\n",
    "### Meta-estimador Bagging en Scikit Learn\n",
    "\n",
    "En scikit-learn, los métodos de bagging se ofrecen como un **meta-estimador** unificado de **BaggingClassifier** y **BaggingRegressor**, tomando como entrada un estimador de base definido por el usuario junto con parámetros que especifican la estrategia para construir subsets aleatorios.\n",
    "\n",
    "En particular, **max_samples** y **max_features** controlan el tamaño de los subsets (en términos de muestras y features), mientras que **bootstrap** y **bootstrap_features** controlan si las muestras y features se toman con o sin reemplazo. A su vez, **n_estimators** controla la cantidad de clasificadores base a entrenar (y, por ende, la cantidad de remuestras a extraer).\n",
    "\n",
    "Cuando se utiliza un subset de las instancias de entrenamiento disponibles, el error de generalización se puede estimar con aquellas instancias \"fuera de bolsa\" **(out-of-bag) estableciendo oob_score=True**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a usar el dataset Hitters (que ya usamos en CARTy Ensambles) para entrenar un modelo de regresión usando BaggingRegressor para predecir el valor del `log(Salary)`. El modelo base que usaremos en una regresión lineal múltiple con dos variables predictoras.\n",
    "\n",
    "Vamos a calcular la predicción para cada instancia como el promedio de las predicciones de cada uno de estos tres modelos.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Leemos los datos y, para simplificar, nos quedamos sólo con los registros completos y las features numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"../Data/Hitters.csv\")\n",
    "print(data_raw.shape)\n",
    "data_complete = data_raw.dropna()\n",
    "print(data_complete.shape)\n",
    "\n",
    "data_columns = ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', \n",
    "                'Walks', 'Years', 'CAtBat', 'CHits', 'CHmRun', \n",
    "                'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', \n",
    "                'Errors', 'Salary']\n",
    "\n",
    "data = data_complete.loc[:, data_columns]\n",
    "print(data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creamos los conjuntos de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = data.drop(\"Salary\", axis = 1)\n",
    "print(X.shape)\n",
    "\n",
    "y = np.log(data.Salary)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 127)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estandarizamos las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scl = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scl = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observación: Hasta acá repetimos exactamente los mismo pasos que hicimos en la clase de modelos de ensamble.\n",
    "\n",
    "Ahora creamos el modelos de bagging.\n",
    "\n",
    "Parámetros:\n",
    "\n",
    "* n_estimators: cantidad de estimadores base en el ensamble.\n",
    "* max_samples: cantidad de muestras del conjunto de entrenamiento que se usan para entrenar cada modelo base\n",
    "* bootstrap: indica si las muestran se hacen con reposición.\n",
    "* max_features: cantidad de features por predictor base\n",
    "* bootstrap_features: indica si las features se seleccionan con reposición\n",
    "* n_jobs: cantidad de jobs que se ejecuta en paralelo, -1 indica que usa todos los procesadores disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base_regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Miremos la performance del modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fit_base = base_regressor.fit(X_train_scl, y_train)\n",
    "predict_base = fit_base.predict(X_test_scl)\n",
    "performance_base = mean_squared_error(y_test, predict_base)\n",
    "performance_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora entrenamos bagging de modelos base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bag_linreg = BaggingRegressor(base_estimator = base_regressor, \n",
    "                            n_estimators = 1000,\n",
    "                            max_samples = 0.3,\n",
    "                            bootstrap = True, \n",
    "                            max_features = 2,\n",
    "                            bootstrap_features = False,\n",
    "                            n_jobs = -1,\n",
    "                            random_state = 127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bag_linreg.fit(X_train_scl, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test mediante el error cuadrático medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction = bag_linreg.predict(X_test_scl)\n",
    "performance = mean_squared_error(y_test, prediction)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La performance que obtuvimos con bagging es mejor que la obtenida con el modelo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Random Forest\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_separador.png\" align=\"center\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging de árboles de decisión\n",
    "\n",
    "---\n",
    "\n",
    "Como vimos, los árboles de decisión son modelos muy poderosos de machine learning. Son muy fáciles de usar porque requieren establecer el valor de muy pocos parámetros y se desempeñan bastante bien.\n",
    "\n",
    "Pero los **árboles de decisión** tienen algunas limitaciones, en particular, los árboles que crecen muy profundamente tienden a aprender patrones altamente irregulares, es decir que **sobreajustan**.\n",
    "\n",
    "El **bagging** ayuda a mitigar este problema al entrenar diferentes árboles con diferentes subconjuntos de los datos de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_random_forest.png\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random forest\n",
    "\n",
    "---\n",
    "\n",
    "**Random forest** se diferencia de bagging de árboles de decisión en una sola característica: usa un algoritmo de aprendizaje de árbol modificado que selecciona, en cada división candidata, un **subconjunto aleatorio de variables**. \n",
    "\n",
    "Este proceso se denomina a veces **bagging de variables (feature bagging)**.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_diferencia_bagging_random_forest.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Random forest es una forma distinta de promediar múltiples árboles de decisión profundos, entrenados en diferentes partes del mismo set de entrenamiento, con el objetivo de reducir la varianza. \n",
    "\n",
    "Esto se logra a expensas de un pequeño aumento en el sesgo y cierta pérdida de interpretabilidad, pero en general **aumenta considerablemente el rendimiento del modelo final**.\n",
    "\n",
    "La razón para hacer esto es la correlación de los árboles en una muestra de bootstrap normal: si una o algunas variables son **predictores muy fuertes** para la variable target, estas variables **serán seleccionadas en muchos de los árboles base** del bagging, haciendo que queden correlacionados. Seleccionando un subconjunto aleatorio de las variables en cada división, contrarrestamos esta correlación entre los árboles base, fortaleciendo el modelo final.\n",
    "\n",
    "Para un problema de clasificación con p variables, se suelen utilizar **$\\sqrt{p}$ de las variables en cada división**.\n",
    "\n",
    "Para problemas de regresión, recomiendan utilizar $p/3$.\n",
    "\n",
    "Pero también podría considerarse como un **hiperparámetro a optimizar**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "¿Qué característica necesaria para la buena performance de bagging se pierde cuando los árboles están correlacionados? ¿Diversidad o precisión?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Diversidad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging - Random Forest\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_bagging_vs_random_forest.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo Random Forest\n",
    "\n",
    "---\n",
    "\n",
    "* Se seleccionan k features de las m totales (siendo k menor a m) y se crea un árbol de decisión con esas k features.\n",
    "\n",
    "* Se crean n árboles variando siempre la cantidad de k features\n",
    "\n",
    "* Se guarda el resultado de cada árbol obteniendo n salidas.\n",
    "\n",
    "* Se calculan los votos obtenidos para cada \"clase\" seleccionada y se considera a la más votada como la clasificación final de nuestro \"bosque\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "B = Cantidad de Árboles del Ensamble\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_random_forest_algorithm.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Out of bag (OOB)\n",
    "\n",
    "---\n",
    "\n",
    "Out of bag (OOB) es una **forma de validar el modelo random forest**.\n",
    "\n",
    "Cada una de las instancias (filas) de entrenamiento que quedan OOB se evaluán en **cada uno de los árboles de decisión que no vieron esta instancia en entrenamiento**, y se se calcula la predicción como la etiqueta más frecuente predicha por estos árboles\n",
    "\n",
    "La puntuación (score) OOB se calcula como el número de filas predichas correctamente de la muestra OOB (fuera de la bolsa).\n",
    "\n",
    "### ¿Cuál es la diferencia entre la puntuación OOB y la el score de validación?\n",
    "\n",
    "El score OOB se calcula usando sólo un **subconjunto de árboles de decisión que no vieron esta instancia en entrenamiento**, mientras que el score de validación se calcula usando **todos los árboles de decisión del ensamble**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Cuándo es útil usar OOB score?\n",
    "\n",
    "En general, la validación en un conjunto completo de árboles de decisión es mejor que en un subconjunto. \n",
    "\n",
    "Sin embargo, cuando el conjunto de datos no es lo suficientemente grande, reservar una parte para validación no es opción. \n",
    "\n",
    "Entonces, en los casos en los que no tenemos muchas instancias y queremos usarlo todo como conjunto de entrenamiento, el score OOB proporciona una buena compensación. \n",
    "\n",
    "No obstante, **el score de validación y el score OOB no son iguales, se calculan de manera diferente y, por lo tanto, no deben compararse**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "\n",
    "---\n",
    "\n",
    "Continuemos el ejemplo de bagging usando ahora Random Forest, y comparemos la performance en test con el modelo anterior.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(n_estimators=1000, \n",
    "                                      criterion='mse', \n",
    "                                      max_depth = 4, \n",
    "                                      bootstrap=True, \n",
    "                                      n_jobs = -1, \n",
    "                                      random_state = 127,\n",
    "                                      max_samples= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "random_forest.fit(X_train_scl, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test mediante el error cuadrático medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction = random_forest.predict(X_test_scl)\n",
    "performance = mean_squared_error(y_test, prediction)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La performance obtenida, medida como error cuadrático medio, es mejor que la obtenida en el modelo de bagging de regresiones lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Extra Trees\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_separador.png\" align=\"center\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extremely randomized trees (Abstract del <a href=\"http://www.montefiore.ulg.ac.be/~ernst/uploads/news/id63/extremely-randomized-trees.pdf\" target=\"_blank\">paper</a> Geurts et al. 2005) \n",
    "\n",
    "---\n",
    "\n",
    "This paper proposes a new tree-based **ensemble method for supervised classification and regression problems**. \n",
    "\n",
    "It essentially consists of **randomizing strongly both attribute and cut-point choice while splitting a tree node**. \n",
    "\n",
    "In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. \n",
    "\n",
    "The **strength of the randomization can be tuned** to problem specifics by the appropriate choice of a parameter. \n",
    "\n",
    "We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. \n",
    "\n",
    "**Besides accuracy, the main strength of the resulting algorithm is computational efficiency**. \n",
    "\n",
    "A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extremely randomized trees (Extra Trees)\n",
    "\n",
    "---\n",
    "\n",
    "* La adición de un paso más de aleatorización produce árboles muy aleatorizados o **ExtraTrees**. \n",
    "   \n",
    "* Éstos se entrenan usando bagging y el método de selección aleatoria de variables, como en un Random Forest ordinario, pero con una capa random adicional.\n",
    "\n",
    "* En lugar de calcular la combinación variable/división óptima local (ej ganancia de información), para cada variable en consideración se generan una división aleatoria (dentro del rango de la variable). Y luego se selecciona la variable/división que maximice la ganancia.\n",
    "\n",
    "* La diferencia principal es que **la división para cada variable no será la óptima, sino una seleccionada random**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo Extra Trees\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_extra_tree_algorithm_clean.png\" align= \"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_extra_tree_algorithm.png\" align= \"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "\n",
    "---\n",
    "\n",
    "Continuemos el ejemplo de bagging y Random Forest usando ahora ExtraTree, y comparemos la performance en test con el modelo anterior.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "extra_tree = ExtraTreesRegressor(n_estimators = 1000,\n",
    "                                 criterion = 'mse', \n",
    "                                 max_depth = 5, \n",
    "                                 bootstrap = True, \n",
    "                                 n_jobs = -1, \n",
    "                                 random_state = 171,\n",
    "                                 max_samples = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "extra_tree.fit(X_train_scl, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test mediante el error cuadrático medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction = extra_tree.predict(X_test_scl)\n",
    "performance = mean_squared_error(y_test, prediction)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La performance obtenida, medida como error cuadrático medio, es mejor que para el modelo de bagging de regresiones lineales y muy similar a la obtenida en el modelo random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La ventaja de extra trees frente a random forest es su menor costo computacional.\n",
    "\n",
    "Midamos esto usando timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "repeat_count = 10\n",
    "time_rf = timeit.timeit(stmt='random_forest.fit(X_train_scl, y_train)', globals=globals(), number=repeat_count)\n",
    "print(time_rf / repeat_count)\n",
    "time_et = timeit.timeit(stmt='extra_tree.fit(X_train_scl, y_train)', globals=globals(), number=repeat_count)\n",
    "print(time_et / repeat_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* Los ensambles son modelos de predicción con menor varianza que sus modelos base\n",
    "\n",
    "* Son robustos frente a outliers y ruido\n",
    "\n",
    "* Son computacionalemnte demandantes pero fácilmente paralelizables\n",
    "\n",
    "* La performance de extra trees es similar a la de random forest, con un costo computacional menor\n",
    "\n",
    "* La performance de random forest generalmente es superior a la de bagging de árboles de decisión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_separador.png\" align=\"center\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a usar el dataset Hitters para entrenar tres modelos de ensamble para clasificar el valor de `Salary` en alto o bajo.\n",
    "\n",
    "Los modelos serán \n",
    "\n",
    "* Bagging usando como modelo base Naive Bayes (Gaussian) https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "\n",
    "* Random Forest https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "* Extra Tree https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "\n",
    "Vamos a evaluar la performance de los tres ensambles usando \n",
    "\n",
    "* AUC\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "* Matriz de confusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Puntos 1 a 4 son idénticos a los de la clase de Modelos de Ensamble:\n",
    "\n",
    "1. Leer los datos y, para simplificar, conservar sólo los registros completos y las features numéricas.\n",
    "\n",
    "2. Crear una variable categórica, a partir de `Salary`, de valores alto / bajo representados como 1 / 0, usando como umbral un valor de Salary igual a 600\n",
    "\n",
    "3. Crear los conjuntos de train y test\n",
    "\n",
    "4. Estandarizar las features\n",
    "\n",
    "Nuevo:\n",
    "\n",
    "5. Entrenar cada uno de los modelos de ensamble y evaluar AUC, accuracy y matriz de confusión\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solución\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1 a 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"../Data/Hitters.csv\")\n",
    "print(data_raw.shape)\n",
    "data_complete = data_raw.dropna()\n",
    "print(data_complete.shape)\n",
    "\n",
    "data_columns = ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', \n",
    "                'Walks', 'Years', 'CAtBat', 'CHits', 'CHmRun', \n",
    "                'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', \n",
    "                'Errors', 'Salary']\n",
    "\n",
    "data = data_complete.loc[:, data_columns]\n",
    "print(data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "salary_cat = binarize(pd.DataFrame(data.Salary), threshold = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = data.drop(\"Salary\", axis = 1)\n",
    "print(X.shape)\n",
    "\n",
    "y = salary_cat\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scl = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scl = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bag_nb = BaggingClassifier(base_estimator = base_model, \n",
    "                            n_estimators = 100,\n",
    "                            max_samples = 0.8,\n",
    "                            bootstrap = True, \n",
    "                            bootstrap_features = False,\n",
    "                            n_jobs = -1,\n",
    "                            random_state = 127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train_nb = y_train.reshape(y_train.shape[0], )\n",
    "bag_nb.fit(X_train_scl, y_train_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction_bag = bag_nb.predict(X_test_scl)\n",
    "\n",
    "accuracy_bagging = accuracy_score(y_test, prediction_bag)\n",
    "print(accuracy_bagging)\n",
    "\n",
    "conf_mat_bagging = confusion_matrix(y_test, prediction_bag)\n",
    "print(conf_mat_bagging)\n",
    "\n",
    "prediction_bag_proba = bag_nb.predict_proba(X_test_scl)\n",
    "prediction_bag_class_1 = prediction_bag_proba[:, 1]\n",
    "auc_bagging = roc_auc_score(y_test, prediction_bag_class_1)\n",
    "print(auc_bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=50, \n",
    "                                      max_depth = 4, \n",
    "                                      bootstrap=True, \n",
    "                                      n_jobs = -1, \n",
    "                                      random_state = 127,\n",
    "                                      max_samples= 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train_rf = y_train.reshape(y_train.shape[0], )\n",
    "random_forest.fit(X_train_scl, y_train_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction_random_forest = random_forest.predict(X_test_scl)\n",
    "\n",
    "accuracy_random_forest = accuracy_score(y_test, prediction_random_forest)\n",
    "print(accuracy_random_forest)\n",
    "\n",
    "conf_mat_random_forest = confusion_matrix(y_test, prediction_random_forest)\n",
    "print(conf_mat_random_forest)\n",
    "\n",
    "prediction_random_forest_proba = random_forest.predict_proba(X_test_scl)\n",
    "prediction_random_forest_class_1 = prediction_random_forest_proba[:, 1]\n",
    "auc_random_forest = roc_auc_score(y_test, prediction_random_forest_class_1)\n",
    "print(auc_random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "extra_tree = ExtraTreesClassifier(n_estimators = 50,\n",
    "                                 max_depth = 5, \n",
    "                                 bootstrap = True, \n",
    "                                 n_jobs = -1, \n",
    "                                 random_state = 171,\n",
    "                                 max_samples = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train_et = y_train.reshape(y_train.shape[0], )\n",
    "extra_tree.fit(X_train_scl, y_train_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction_extra_trees = extra_tree.predict(X_test_scl)\n",
    "\n",
    "accuracy_extra_trees = accuracy_score(y_test, prediction_extra_trees)\n",
    "print(accuracy_extra_trees)\n",
    "\n",
    "conf_mat_extra_trees = confusion_matrix(y_test, prediction_extra_trees)\n",
    "print(conf_mat_extra_trees)\n",
    "\n",
    "prediction_extra_trees_proba = extra_tree.predict_proba(X_test_scl)\n",
    "prediction_extra_trees_class_1 = prediction_extra_trees_proba[:, 1]\n",
    "auc_extra_trees = roc_auc_score(y_test, prediction_extra_trees_class_1)\n",
    "print(auc_extra_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_37_Bagging/M5_CLASE_37_separador.png\" align=\"center\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.statlearning.com/\">An Introduction to Statistical Learning. Cap. 8 Tree-Based Methods </a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710\" target=\"_blank\">What is Out of Bag (OOB) score in Random Forest?</a>\n",
    "\n",
    "<a href=\"https://www.youtube.com/playlist?list=PLblh5JKOoLUIE96dI3U7oxHaCAbZgfhHk\" target=\"_blank\">Random Forests - StatQuest</a>\n",
    "\n",
    "<a href=\"http://rstudio-pubs-static.s3.amazonaws.com/399564_634f8305946d4b52a25007c0b012ae6f.html\" target=\"_blank\">Tree-Based Methods</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
