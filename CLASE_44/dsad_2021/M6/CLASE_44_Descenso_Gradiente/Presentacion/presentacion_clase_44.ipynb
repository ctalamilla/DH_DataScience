{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_portada.jpg\" align=\"center\" width=\"90%\"/></div>        \n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 6</div>\n",
    "            <div class=\"div-contenido\">Descenso Gradiente</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- Gradiente\n",
    "\n",
    "- Descenso gradiente\n",
    "\n",
    "- Funciones de costo y cálculo de gradiente\n",
    "\n",
    "- Algoritmo descenso gradiente\n",
    "\n",
    "- Velocidad de aprendizaje\n",
    "\n",
    "- Stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Introducción\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué es gradiente?\n",
    "\n",
    "---\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>    \n",
    "\n",
    "<td style=\"font-size:16px;line-height:2;\">    \n",
    "El gradiente es la derivada o tasa de cambio de una función. \n",
    "\n",
    "Es un <b>vector (una dirección para moverse)</b> que:\n",
    "\n",
    "<ul>    \n",
    "<li> Apunta en la dirección de mayor aumento de una función </li>\n",
    "\n",
    "<li> Es cero en un máximo o un mínimo local (porque no hay una única dirección de aumento) </li>\n",
    "\n",
    "</ul>    \n",
    "    \n",
    "El término \"gradiente\" se utiliza normalmente para funciones con varias variables y una única salida (un campo escalar). \n",
    "\n",
    "Podemos hablar del gradiente de una recta (su pendiente), pero usar \"gradiente\" para funciones de una sola variable es innecesariamente confuso.\n",
    "</td>\n",
    "    \n",
    "<td style=\"width:45%;\">    \n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_gradiente.gif\" align=\"center\" />\n",
    "\n",
    "</td>    \n",
    "    \n",
    "</tr>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué es descenso gradiente?\n",
    "\n",
    "---\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>    \n",
    "\n",
    "<td style=\"font-size:16px;line-height:2;\">    \n",
    "\n",
    "<b>Es un algoritmo de optimización genérico para encontrar el mínimo de una función.</b> <br/>\n",
    "\n",
    "Es un algoritmo de optimización iterativo, comenzamos con un punto aleatorio en la función y nos movemos en la dirección negativa del gradiente de la función para alcanzar los mínimos locales / globales. <br/>\n",
    "\n",
    "<b>En machine learning buscamos mínimos de la función de costo.</b> <br/>\n",
    "\n",
    "    \n",
    "La función de costo (J) o función de pérdida mide la diferencia entre el valor real y la predicción del modelo. La función de costo es una función convexa.<br/>\n",
    "\n",
    "</td>\n",
    "    \n",
    "<td style=\"width:45%;\">    \n",
    "    \n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_que_es.gif\" align=\"center\" />\n",
    "\n",
    "</td>    \n",
    "    \n",
    "</tr>    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Descenso gradiente\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Cuándo usamos descenso gradiente?\n",
    "\n",
    "---\n",
    "\n",
    "Generalmente utilizamos descenso de gradiente cuando los parámetros del modelo no se pueden calcular analíticamente (por ejemplo, utilizando álgebra lineal) y deben buscarse mediante un algoritmo de optimización.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_ejemplo_1.gif\" align=\"center\" />\n",
    "\n",
    "Cuando nuestro modelo tiene muchas variables, muchas observaciones o cuando su función de costo no puede resolverse de forma analítica (como es el caso de la regresión logística y las redes neuronales), podemos entrenar nuestro modelo usando el algoritmo **descenso gradiente**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Función de Costo\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función de costo de la regresión lineal es una **función convexa**, lo que nos asegura que el descenso del gradiente va a converger al **mínimo global**. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_descenso_gradiente_1.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_global_local_min.jpg\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En las funciones de costo que con más de un mínimo, como la de la función logística, cuando ejecutamos el/los algoritmos de descenso del gradiente buscamos además iniciarlos desde diferentes lugares/posiciones para que no quede “atrapado” en un mínimo local.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_ejemplo_2.gif\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplos de funciones de costo que conocemos\n",
    "\n",
    "---\n",
    "\n",
    "### Regresión lineal\n",
    "\n",
    "La función de costo (J) de la regresión lineal es el error cuadrático medio (RMSE) entre el valor y predicho (pred) y el valor y verdadero (y).\n",
    "\n",
    "Supongamos que el dataset tiene n filas y m columnas.\n",
    "\n",
    "Para la fila i del dataset, el valor predicho $h_i$:\n",
    "\n",
    "<p style=\"font-size:24px;\">\n",
    "$$h_i =  \\beta_0 + \\beta_1. X_{i1} + \\beta_2. X_{i2} + \\beta_3. X_{i3} + ... + \\beta_m. X_{im}$$\n",
    "</p>    \n",
    "\n",
    "Costo:\n",
    "\n",
    "<p style=\"font-size:24px;\">\n",
    "$$J = \\frac{1}{n} \\sum_{i=1}^n{(h_i - y_i)^2}$$\n",
    "</p>    \n",
    "\n",
    "\n",
    "Gradiente: \n",
    "\n",
    "<p style=\"font-size:24px;\">    \n",
    "$$\\frac{\\partial J(\\beta_0, ..., \\beta_m)}{\\partial \\beta_j} = \\frac{2}{n} \\sum_{i=1}^n (h_i - y_i). X_{ij} $$\n",
    "</p>    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regresión logística\n",
    "\n",
    "<p style=\"font-size:24px;\">\n",
    "$$\\beta_0 + \\beta_1 . x_1 + ... + \\beta_m. x_m = \\beta^t.X$$    \n",
    "</p>        \n",
    "\n",
    "<p style=\"font-size:24px;\">\n",
    "$$h_{\\beta}(X) = g(\\beta^t.X)$$\n",
    "</p>            \n",
    "\n",
    "<p style=\"font-size:24px;\">\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "</p>            \n",
    "\n",
    "Costo: \n",
    "\n",
    "<p style=\"font-size:24px;\">\n",
    "$$J = - \\space \\frac{1}{n} \\sum_{i=1}^n(y_i . log(h_{\\beta}(X_i)) + (1-y_i) . log(1-h_{\\beta}(X_i)))$$    \n",
    "</p>        \n",
    "\n",
    "Gradiente:\n",
    "\n",
    "<p style=\"font-size:24px;\">\n",
    "$$\\frac{\\partial J(\\beta_0, ..., \\beta_m)} {\\partial \\beta_j} = \\sum_{i=1}^n(h_{\\beta}(X_i) - y_i).X_{ij}$$    \n",
    "</p>        \n",
    "\n",
    "Pueden ver el detalle de la derivación <a href=\"https://towardsdatascience.com/gradient-descent-demystified-bc30b26e432a\" target=\"_blank\">aquí</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Algoritmo\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo\n",
    "\n",
    "---\n",
    "\n",
    "El algoritmo comienza estableciendo valores iniciales para el/los coeficiente/s de la función.\n",
    "\n",
    "Estos valores pueden ser 0 o valores aleatorios pequeños.\n",
    "\n",
    "```\n",
    "coefficient = 0.0\n",
    "```\n",
    "\n",
    "El costo para estos valores en los coeficientes se calcula evaluando la función de costo en los valores iniciales.\n",
    "\n",
    "```\n",
    "cost = f(coefficient)\n",
    "```\n",
    "\n",
    "ó\n",
    "\n",
    "```\n",
    "cost = evaluate(f(coefficient))\n",
    "```\n",
    "\n",
    "Se calcula la derivada de la función de costo. \n",
    "\n",
    "La derivada es un concepto de cálculo que refiere a la pendiente de la función en un punto dado. \n",
    "\n",
    "Necesitamos conocer la pendiente para determinar la dirección (signo) en que debemos variar los valores de los coeficientes para que el costo en la siguiente iteración sea menor.\n",
    "\n",
    "```\n",
    "delta = derivative(cost)\n",
    "```\n",
    "\n",
    "Ahora que sabemos por la derivada qué dirección es \"cuesta abajo\", podemos actualizar los valores de los coeficientes.\n",
    "\n",
    "Debemos especificar el valor de un parámetro de tasa de aprendizaje (learning rate) $\\alpha$ que controla cuánto pueden cambiar los coeficientes en cada iteración.\n",
    "\n",
    "```\n",
    "coefficient = coefficient – (alpha * delta)\n",
    "```\n",
    "\n",
    "Este proceso se repite hasta que el costo sea 0 o suficientemente cercano a 0 para ser aceptable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para una regresión lineal simple: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_funcion_costo_1.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_funcion_costo_2.png\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Learning rate\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning rate\n",
    "---\n",
    "\n",
    "La tasa o velocidad de aprendizaje (learning rate) $\\alpha$ es un hiperparámetro muy importante del descenso gradiente.\n",
    "\n",
    "Para reducir el **tiempo de convergencia** del descenso del gradiente, es conveniente que las **variables estén normalizadas**. \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_learning_rate.png\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una buena forma de asegurarse de que el descenso gradiente se ejecute correctamente es graficando los valores de la función de costo mientras se ejecuta la optimización. \n",
    "\n",
    "Graficando el número de iteraciones en el eje x y el valor de la función de costo en el eje y, podemos ver el valor de la función de costo después de cada iteración de descenso de gradiente, y detectar fácilmente qué tan apropiada es la tasa de aprendizaje. \n",
    "\n",
    "La imagen de la izquierda muestra un gráfico de este tipo, mientras que la imagen de la derecha ilustra la diferencia entre las tasas de aprendizaje buenas y malas.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_learning_rate_2.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_convergencia.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Stopping criteria\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stopping criteria\n",
    "\n",
    "---\n",
    "\n",
    "Algunos criterios de terminación del algoritmo de descenso gradiente son:\n",
    "\n",
    "* Cantidad máxima de iteraciones\n",
    "\n",
    "* Tolerancia absoluta: se detiene cuando el valor de la función de costo se \"acerca lo suficiente\" a cero.\n",
    "\n",
    "* Tolerancia relativa: se detiene cuando la mejora en esa iteración cae por debajo de un umbral.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusiones \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Descenso gradiente es un algoritmo de optimización simple, que podemos usar en muchos algoritmos de machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hands-on\n",
    "\n",
    "---\n",
    "Vimos que el gradiente para una regresión lineal se calcula como \n",
    "\n",
    "<p style=\"font-size:16px;\">    \n",
    "$$\\frac{\\partial J(\\beta_0, ..., \\beta_m)}{\\partial \\beta_j} = \\frac{2}{n} \\sum_{i=1}^n (h_i - y_i). X_{ij} $$\n",
    "</p>    \n",
    "\n",
    "El algortimo actualiza los coeficientes como\n",
    "<p style=\"font-size:16px;\">    \n",
    "$$\\beta_j = \\beta_j - \\alpha . \\frac{\\partial J(\\beta_0, ..., \\beta_m)}{\\partial \\beta_j} $$\n",
    "</p>    \n",
    "\n",
    "Queda: \n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "$\\beta_0 = \\beta_0 - \\alpha \\frac{2}{N} \\sum (h_i - y_i)$\n",
    "</p>    \n",
    "<p style=\"font-size:16px;\">\n",
    "$\\beta_i = \\beta_i - \\alpha \\frac{2}{N} \\sum (h_i - y_i). X_i$\n",
    "</p>   \n",
    "\n",
    "Presentamos a continuación una clase que resuelve el algoritmo de descenso gradiente para una regresión lineal simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MyGradientDescent():\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate #usamos la velocidad de aprendizaje establecida en el constructor de la clase\n",
    "        self.beta1 = 0 #inicializamos beta1 en 0\n",
    "        self.beta0 = 0 #inicializamos beta0 en 0\n",
    "          \n",
    "    def fit(self, X, y, epochs = 100):\n",
    "        N = len(X)  #cantidad de filas del dataset\n",
    "        history = [] #vamos a guardar acá el valor de la función de pérdida en cada iteración\n",
    "        \n",
    "        for e in range(epochs):  #calculamos el valor de la pérdida para cada iteración\n",
    "            for i in range(N):                \n",
    "                Xi = X[i, :]\n",
    "                yi = y.iloc[i] \n",
    "                \n",
    "                hi = self.beta1 * Xi + self.beta0 #valor predicho para la fila i\n",
    "                f = hi - yi #residuo o error de predicción para la fila i\n",
    "                \n",
    "                self.beta1 -= self.learning_rate * 2 / N * f * Xi #por definicion de beta0 \n",
    "                self.beta0 -= self.learning_rate * 2 / N * f # por definicion de beta0 \n",
    "            \n",
    "            loss = mean_squared_error(y, (self.beta1 * X + self.beta0))\n",
    "                                      \n",
    "            if e % 100 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss})\")\n",
    "            \n",
    "            history.append(loss) #guardamos los valores de la función de pérdida en cada iteración\n",
    "                                      \n",
    "        return history\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return self.beta1 * X + self.beta0\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usando el dataset de propiedades en Boston (https://www.kaggle.com/c/boston-housing) vamos a tratar de predecir el valor de `medv` usando una regresión lineal simple.\n",
    "\n",
    "1. Vamos a usar como variable predictora aquella que tenga correlación más alta con la variable target. ¿Cuál es esa variable?\n",
    " \n",
    "2. Graficar en un scatterplot `medv`  en función de la variable determinada en el paso anterior.\n",
    "\n",
    "3. Construir los conjuntos de train y test, normalizar la variable predictora y usar la clase `MyGradientDescent` para ajustar la regresión lineal simple.\n",
    "\n",
    "4. Graficar el valor de la función de costo para cada iteración (época)\n",
    "\n",
    "5. Graficar en un scatterplot los datos de test y los valores predichos por el modelo para esos datos.\n",
    " \n",
    "6. Calcular el error cuadrático medio sobre los datos de test.\n",
    "\n",
    "7. Usar `LinearRegression` de `sklearn.linear_model` y comparar con resultados obtenidos con descenso gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solución \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/boston_data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "abs(data.corr()[\"medv\"]).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vamos a tratar de predecir el valor de `medv` a partir de la variable `lstat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x = data.lstat, y = data.medv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y = data['medv']\n",
    "X = pd.DataFrame(data['lstat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scl = scaler.fit_transform(X_train)\n",
    "X_test_scl = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = MyGradientDescent(learning_rate = 0.01)\n",
    "\n",
    "history = model.fit(X_train_scl, y_train, 1000)\n",
    "\n",
    "predictions = model.predict(X_test_scl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Grafiquemos el valor de la función de costo para cada iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x = range(len(history)), y = history);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Grafiquemos el modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x = X_test_scl[:, 0], y = y_test )\n",
    "sns.lineplot(x = X_test_scl[:, 0], y = predictions[:, 0], color=\"orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Calculemos el error cuadrático medio de la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos los valores de los parámetros de la regresión lineal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(model.beta1)\n",
    "print(model.beta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora comparemos este modelo con el obtenido con scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scl, y_train)\n",
    "predictions_sklearn = model.predict(X_test_scl)\n",
    "mean_squared_error(y_test, predictions_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Grafiquemos el modelo de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x = X_test_scl[:, 0], y = y_test )\n",
    "sns.lineplot(x = X_test_scl[:, 0], y = predictions_sklearn, color=\"orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Obtuvimos resultados equivalentes con las dos formas de resolución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias y material adicional\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/\" target=\"_blank\">Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aurélien Géron</a>\n",
    "\n",
    "<a href=\"https://medium.com/diogo-menezes-borges/what-is-gradient-descent-235a6c8d26b0\" target=\"_blank\">What in god’s name is Gradient Descent?</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/calculus-in-data-science-and-its-uses-3f3e1b5e5b35\" target=\"_blank\">Calculus in Data Science and it uses</a>\n",
    "\n",
    "<a href=\"https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html\" target=\"_blank\">5 Concepts You Should Know About Gradient Descent and Cost Function</a>\n",
    "\n",
    "<a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" target=\"_blank\">Gradient Descent For Machine Learning</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1\" target=\"_blank\">Implement Gradient Descent in Python</a>\n",
    "\n",
    "<a href=\"https://eleijonmarck.dev/posts/understanding-gradient-descent\" target=\"_blank\">An Introduction to Gradient Descent w. Linear Regression</a>\n",
    "\n",
    "<a href=\"https://web.stanford.edu/~jurafsky/slp3/5.pdf\" target=\"_blank\">Logistic Regression</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/gradient-descent-training-with-logistic-regression-c5516f5344f7\" target=\"_blank\">Gradient Descent Training With Logistic Regression</a>\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=sDv4f4s2SB8\" target=\"_blank\">Statsquest - Gradient Descent, Step-by-Step</a>\n",
    "\n",
    "<a href=\"https://nitin9809.medium.com/linear-regression-with-gradient-descent-maths-implementation-and-example-using-scikit-learn-1ed1ed3440cc\" target=\"_blank\">Linear Regression with Gradient Descent Maths, Implementation and Example Using Scikit-Learn</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
