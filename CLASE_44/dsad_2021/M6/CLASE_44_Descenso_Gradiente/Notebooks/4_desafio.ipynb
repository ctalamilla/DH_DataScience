{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-rachel",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-glasgow",
   "metadata": {},
   "source": [
    "# Descenso gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-sunrise",
   "metadata": {},
   "source": [
    "Vamos a usar el dataset de propiedades en Boston (https://www.kaggle.com/c/boston-housing) y tratar de predecir el valor de `medv` usando una regresión lineal múltiple.\n",
    "\n",
    "Para eso, vamos a modificar la clase `MyGradientDescent` presentada en el encuentro sincrónico y usarla para entrenar un modelo de regresión lineal múltiple y uno simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-channels",
   "metadata": {},
   "source": [
    "Ayuda: \n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "$h =  \\beta_0 + \\beta_1. X_1 + \\beta_2. X_2 + \\beta_3. X_3 + ... + \\beta_m. X_m$\n",
    "</p>\n",
    "\n",
    "i es el índice de la fila en el dataset\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "$h_i =  \\beta_0 + \\beta_1. X_{i1} + \\beta_2. X_{i2} + \\beta_3. X_{i3} + ... + \\beta_m. X_{im}$\n",
    "</p>    \n",
    "<p style=\"font-size:16px;\">\n",
    "$\\beta_0 = \\beta_0 - \\alpha \\frac{1}{m} \\sum (h_i - y_i)$\n",
    "</p>    \n",
    "<p style=\"font-size:16px;\">\n",
    "$\\beta_i = \\beta_i - \\alpha \\frac{1}{m} \\sum (h_i - y_i). X_i$\n",
    "</p>    \n",
    "\n",
    "Costo (error cuadrático medio): \n",
    "<p style=\"font-size:16px;\">    \n",
    "$J(\\beta_0, ..., \\beta_m) = \\frac{1}{N} \\sum_{i=1}^N (h_i - y_i)^2  $\n",
    "</p>\n",
    "Gradiente: \n",
    "\n",
    "<p style=\"font-size:16px;\">    \n",
    "$\\frac{\\partial J(\\beta_0, ..., \\beta_m)}{\\partial \\beta_j} = \\frac{2}{N} \\sum_{i=1}^N (h_i - y_i). X_{ij} $\n",
    "</p>    \n",
    "\n",
    "N es el número de observaciones o filas del dataset\n",
    "\n",
    "Entonces \n",
    "\n",
    "$\\beta_0 = \\beta_0 - \\alpha .\\frac{2}{N} \\sum_{i=1}^N (h_i - y_i). X_{i0}$\n",
    "\n",
    "como $X_{i0} = 1$ queda:\n",
    "\n",
    "$\\beta_0 = \\beta_0 - \\alpha .\\frac{2}{N} \\sum_{i=1}^N (h_i - y_i)$\n",
    "\n",
    "$\\beta_1 = \\beta_1 - \\alpha .\\frac{2}{N} \\sum_{i=1}^N (h_i - y_i). X_{i1}$\n",
    "\n",
    "$\\beta_2 = \\beta_2 - \\alpha .\\frac{2}{N} \\sum_{i=1}^N (h_i - y_i). X_{i2}$\n",
    "\n",
    "... \n",
    "\n",
    "$\\beta_j = \\beta_j - \\alpha .\\frac{2}{N} \\sum_{i=1}^N (h_i - y_i). X_{ij}$\n",
    "\n",
    "\n",
    "$\\alpha$ = Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-young",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-rachel",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Leer los datos del archivo `Data/boston_data.csv` en un dataframe y construir un heatmap de correlaciones entre sus columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-lancaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-complexity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pacific-award",
   "metadata": {},
   "source": [
    "## Ejercicio 2 \n",
    "\n",
    "La variable target del modelo es `medv`.\n",
    "\n",
    "Seleccionar como variables predictoras las tres variables que tengan mayor correlación (en valor absoluto) con la variable target.\n",
    "\n",
    "Construir los conjuntos de train y test y normalizar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-fancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-reliance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-fifty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-hungarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "arctic-paraguay",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Modificar la clase `MyGradientDescent` presentada en el encuentro sincrónico para resolver ahora una regresión **múltiple** usando descenso gradiente.\n",
    "\n",
    "```\n",
    "\n",
    "class MyGradientDescent():\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = 0\n",
    "        self.beta0 = 0\n",
    "          \n",
    "    def fit(self, X, y, epochs = 100):\n",
    "        N = len(X)\n",
    "        history = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            for i in range(N):\n",
    "                Xi = X[i, :]\n",
    "                yi = y.iloc[i]                 \n",
    "                \n",
    "                hi = self.beta1 * Xi + self.beta0\n",
    "                f = hi - yi\n",
    "                \n",
    "                self.beta1 -= self.learning_rate * 2 / N * f * Xi\n",
    "                self.beta0 -= self.learning_rate * 2 / N * f \n",
    "\n",
    "            loss = 0\n",
    "            loss = mean_squared_error(y, (self.beta1 * X + self.beta0))\n",
    "                                      \n",
    "            if e % 100 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss})\")\n",
    "            \n",
    "            history.append(loss)\n",
    "                                      \n",
    "        return history\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return self.beta1 * X + self.beta0\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Tener en cuenta las fórmulas presentadas en la ayuda al inicio de la notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-treasure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "multiple-cleaning",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Entrenar la regresión lineal múltiple con tres variables predictoras (seleccionadas en el ejercicio 2) usando la clase que definieron en el ejercicio 3.\n",
    "\n",
    "Evaluar la performance en test mediante el error cuadrático medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-preference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-battery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aware-advocate",
   "metadata": {},
   "source": [
    "## Ejercicio 5\n",
    "\n",
    "Graficar el valor de pérdida en función de las épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-addiction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "editorial-sculpture",
   "metadata": {},
   "source": [
    "## Ejercicio 6\n",
    "\n",
    "Usar la misma clase del ejercicio 3 para ajustar una regresión lineal simple cuya variable predictora sea `lstat` y comprobar el resultado obtenido es el mismo que con `MyGradientDescent`\n",
    "\n",
    "Graficar en un scatterplot los datos de test y los predichos por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-waterproof",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "mental-meeting",
   "metadata": {},
   "source": [
    "Ahora veamos que devuelve `MyGradientDescent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientDescent():\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = 0\n",
    "        self.beta0 = 0\n",
    "          \n",
    "    def fit(self, X, y, epochs = 100):\n",
    "        N = len(X)\n",
    "        history = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            for i in range(N):\n",
    "                Xi = X[i, :]\n",
    "                yi = y.iloc[i] \n",
    "                \n",
    "                hi = self.beta1 * Xi + self.beta0\n",
    "                f = hi - yi\n",
    "                \n",
    "                self.beta1 -= self.learning_rate * 2 / N * f * Xi\n",
    "                self.beta0 -= self.learning_rate * 2 / N * f \n",
    "\n",
    "            loss = 0\n",
    "            loss = mean_squared_error(y, (self.beta1 * X + self.beta0))\n",
    "                                      \n",
    "            if e % 100 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss})\")\n",
    "            \n",
    "            history.append(loss)\n",
    "                                      \n",
    "        return history\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return self.beta1 * X + self.beta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-merchandise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-armenia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-class",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "minimal-wright",
   "metadata": {},
   "source": [
    "## Ejercicio 7 - Opcional\n",
    "\n",
    "Intenten entrenar un modelo con cinco variables predictoras. \n",
    "\n",
    "Posiblemente tengan que probar distintos valores de learning rate para conseguir resultados aceptables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-steal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "widespread-assumption",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Implementando de forma más general la clase `MyGradientDescent` logramos usar el mismo código para resolver regresiones lineales simples y múltiples con descenso gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-cologne",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "---\n",
    "\n",
    "https://towardsdatascience.com/multivariate-linear-regression-in-python-step-by-step-128c2b127171\n",
    "\n",
    "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
