{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_portada.gif\" align=\"center\" width=\"70%\"/></div>\n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 6</div>\n",
    "            <div class=\"div-contenido\">Reducción de la Dimensionalidad\n",
    "\n",
    "</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- PCA - Principal Component Analysis.\n",
    "   - Representación Geométrica.\n",
    "   - Implementación. Técnica del bastón quebrado.\n",
    "   - Pesos (loadings). Proyección.\n",
    "   - Visualización: Biplot.\n",
    "   - Representación Algebraica.\n",
    "\n",
    "---\n",
    "\n",
    "- Manefold Learning.\n",
    "   - Conceptos.\n",
    "   - t-SDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Introducción\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "Estudiamos en una clase anterior *la maldicion de la dimensionalidad*: si agregamos features *mejoramos la performance del modelo hasta cierto punto*, a partir del cual comienza a empeorar.\n",
    "\n",
    "Vimos también que los métodos de **features selection** identifican y seleccionan las variables más relevantes para el entrenamiento de los modelos. \n",
    "\n",
    "O dicho de otra forma *remueve las variables que son poco informativas o redundantes*.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_001_feature_selection.JPG\" alt=\"feature_selection\" width=50% height=40% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "Existe otra forma de reducir features que llamamos **feature extraction**.\n",
    "\n",
    "La idea es **construir nuevas variables como combinación de las existentes, de forma especifica (mediante una tarnsformación lineal), y con un orden de importancia asociado**.\n",
    "\n",
    "Luego seleccionamos aquellas variables más importantes, que son las que usaremos como features para construir nuestros modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por ejemplo, el gráfico muestra tres features (Gene1, Gene2, Gene3) con las cuales generamos tres nuevas variables (PC1, PC2, PC3), y nos quedamos con las dos primeras (que son las más importantes).\n",
    "\n",
    "Vemos que las observaciones pasan de tener **tres dimensiones a dos** pero la pérdida de información en esta reducción es menor que si seleccionáramos cualquier par de las variables originales.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_002_PCA.png\" alt=\"feature_selection\" width=80% height=60% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\">  PCA - Principal Component Analysis\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA\n",
    "\n",
    "---\n",
    "Vamos a estudiar **Principal Component Analysis** (Análisis de Componentes Principales).\n",
    "\n",
    "PCA es un algoritmo de aprendizaje no supervisado, aplicado a la reducción de dimensiones.\n",
    "\n",
    "Veremos como implementamos con PCA un proceso de *feature extraction*.\n",
    "\n",
    "Se puede usar también como *modo de visualización, filtro de ruido, variables latentes*.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_003_cuadro_scikit_learn.png\" alt=\"cuadro_scikit_learn\" width=80% height=60% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representación geométrica\n",
    "\n",
    "---\n",
    "PCA permite *proyectar datos a un subespacio de menor dimensionalidad*, **preservando la mayor cantidad posible de información**.\n",
    "\n",
    "Imaginemos que tenemos 2 variables, $x1$ y $x2$, ambas *correlacionadas*. Y queremos proyectarlas sobre una sola coordenada.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_004_geometrico1.JPG\" alt=\"geometrico1\" width=85% height=65% align=\"center\"/>\n",
    "\n",
    "PCA busca una proyección que **minimice el error cuadrático medio de reconstrucción**. O bien, que **maximice la varianza de nuestros datos**.\n",
    "\n",
    "$$ J = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\hat{x_i})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Calculado el vector $V1$, definimos un segundo vector $V2$, **ortogonal a $V1$** (es la generalización de perpendicularidad).\n",
    "\n",
    "Creamos *un nuevo sistema de coordenadas* que reemplaza al espacio de las features originales $x_1$ y $x_2$, al que llamamos **Componentes Principales**.\n",
    "\n",
    "$V1$ y $V2$ son el *primer y segundo componentes principales*.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_005_geometrico2.png\" alt=\"geometrico2\" width=50% height=40% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representación geométrica\n",
    "\n",
    "---\n",
    "PCA define cada componente principal (CP) como el vector que **tiene más varianza de los datos**, *el que más información captura*.\n",
    "\n",
    "El *primer componente* se ubica en la dirección donde *más varían los datos*; *el segundo es\n",
    "ortogonal al primero* recogiendo la mayor parte de la variabilidad restante y así sucesivamente.\n",
    "\n",
    "**Las componentes principales son linealmente independientes entre sí**.\n",
    "\n",
    "Cómo además PCA exige que todos los CP estén normalizados, se genera una base ortonormal (ortogonales y de igual medida) con *una cantidad de dimensiones igual al espacio de las features originales*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Así, dejando solo los primeras CP, podemos **reducir la dimensionalidad del dataset original**, conservando la mayor cantidad posible de información.\n",
    "\n",
    "En este caso, descartamos a $V2$, y nos quedamos con la dimensión $V1$.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_006_geometrico3.png\" alt=\"geometrico3\" width=50% height=40% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\">  PCA - Ejemplo\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "Vamos a aplicar PCA sobre un dataset. Además de mostrar el procedimiento, podemos observar que nos ofrece:\n",
    "\n",
    "- **Descubrir variables latentes**, es decir, aquellas que no se observan directamente sino que son inferidas a partir de otras. En nuestro caso, a partir de los CP.\n",
    "\n",
    "- **Facilitar la visualización de dataset multivariados**, reduciendo las features a dos dimensiones.\n",
    "\n",
    "- **Conocer los loadings**, que representan el peso de las variables originales sobre cada CP, y muestran su influencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "---\n",
    "El dataset es un conjunto de datos sobre los contribuyentes de 20 provincias argentinas. Se exceptúan las 4 más significativas económicamente. \n",
    "\n",
    "Los datos indican, por ejemplo, recaudación total, cantidad de contribuyentes, total de contribuyentes por actividad económica, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2PHSRoPK8OK",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/provincias_actividad.csv', sep=';', index_col='Provincia')\n",
    "print('Filas:', df.shape[0], 'Columnas:', df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La matriz de correlación entre las variables indica que no son completamente independientes entre ellas; podemos aplicar PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aplicando PCA\n",
    "\n",
    "---\n",
    "Necesitamos estandarizar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "df_std = pd.DataFrame(std.fit_transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instanciamos un objeto de la clase PCA. Sin argumentos, PCA conserva todos los CP.\n",
    "\n",
    "Calculamos los componentes principales ajustando el dataset al modelo con el método `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_df = PCA() # n_components indica cuantos CP calcular\n",
    "pca_df.fit(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos *la varianza explicada* de cada componente principal. Observemos que el primero tiene la mayor varianza (el que más información tiene) y luego decrece el valor para los restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=6, suppress=True)\n",
    "pca_df.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El *ratio o proporción de la varianza explicada* de cada componente principal nos indica que los tres primeros explican el 53 + 24 + 11 = 88% de la varianza de los datos.\n",
    "\n",
    "Nos podemos quedar solo con ellos, ya que perderemos poca información.\n",
    "\n",
    "La suma de los ratios da 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca_df.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Graficamos el ratio de varianza explicada en función de la cantidad de componentes.\n",
    "\n",
    "Este gráfico también nos permite usar **la técnica del baston quebrado**: indica con que número de componentes quedarnos. Es donde *se quiebra la curva*, o bien cuando el descenso se estabiliza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.plot(range(11), pca_df.explained_variance_ratio_, '-o', label='Componente individual')\n",
    "plt.plot(range(11), np.cumsum(pca_df.explained_variance_ratio_), '-s', label='Acumulado')\n",
    "\n",
    "plt.ylabel('Porcentaje de Varianza Explicada'); plt.xlabel('Componentes Principales')\n",
    "plt.ylim(0,1.05); plt.xticks(range(11))\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Verifiquemos la ortogonalidad de las componentes principales mediante el producto escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dots = np.ones(shape=(pca_df.components_.shape[0], pca_df.components_.shape[0]))\n",
    "\n",
    "for i in range(pca_df.components_.shape[0]):\n",
    "    for j in range(i+1, pca_df.components_.shape[0]):        \n",
    "        dots[i, j] = np.dot(pca_df.components_[i], pca_df.components_[j])\n",
    "        dots[j, i] = dots[i, j]\n",
    "        \n",
    "dots        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Calculando la *correlación entre los componentes principales*, comprobamos que están muy poco correlacionadas: los valores son en general cercanos a cero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "name_columns = ['CP1', 'CP2', 'CP3', 'CP4','CP5', 'CP6', 'CP7', 'CP8','CP9', 'CP10', 'CP11']\n",
    "pcs_df = pd.DataFrame(pca_df.components_, columns=name_columns)\n",
    "pcs_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(Pueden leer sobre la relación entre ortogonal, correlación e independencia <a href=\"https://stats.stackexchange.com/questions/171324/what-is-the-relationship-between-orthogonal-correlation-and-independence\"> aquí</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\">  PCA - características\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pesos (loadings)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Los componentes principales son **combinaciones lineales de las variables originales**. \n",
    "\n",
    "Las variables originales están relacionadas entre sí, y esa información común se representa en los CPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Si el peso de la variable en la componente principal es positiva, indica que ambos tienen una *correlación positiva*. La variable **influye** sobre el CP.\n",
    " \n",
    "- Por el contrario, si el loading es negativo, indica que la variable *correlaciona en forma negativa* con el CP. La variable también **influye** sobre el CP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos los pesos de cada variable original del dataset para los dos primeros componentes principales. \n",
    "\n",
    "- En CP1, Transporte_carga y Venta_Mayor son los que más influyen en forma negativa.\n",
    "\n",
    "- En CP2, RecaCM el que más contribuye en forma positiva y Oleaginosas más en forma negativa.\n",
    "\n",
    "Más adelante vamos a usar esta información para estimar *variables latentes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca_loadings = pd.DataFrame(pca_df.components_.T, index=df.columns).iloc[:,:2]\n",
    "pca_loadings.rename(columns = {0 : 'CP1', 1 : 'CP2'}, inplace = True)\n",
    "pca_loadings.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Proyección \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Podemos ver como se proyectan los datos originales sobre el *nuevo sistema de coordenadas* del espacio generado por las componentes principales.\n",
    "\n",
    "Cada provincia sigue teniendo 11 valores, pero ahora representan su ubicación sobre los 11 CPs, en lugar de las 11 variables originales.\n",
    "\n",
    "Para lograr *la reducción de la dimensionalidad*, podemos considerar solo a CP1 y CP2. \n",
    "\n",
    "Así cada provincia pasa **de tener once dimensiones a solo dos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "name_columns = ['CP1', 'CP2', 'CP3', 'CP4','CP5', 'CP6', 'CP7', 'CP8','CP9', 'CP10', 'CP11']\n",
    "df_pca = pd.DataFrame(pca_df.fit_transform(df_std), columns=name_columns, index=df.index)\n",
    "df_pca.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualización\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Biplot** es una representación gráfica de datos multivariados. \n",
    "\n",
    "De la misma manera que un diagrama de dispersión muestra la distribución conjunta de dos variables, un *Biplot representa tres o más variables*.\n",
    "\n",
    "Para ello, grafica la distribución de los datos multivariados en un espacio reducido, **normalmente de dimensión dos**, y le superpone la representación de las variables originales.\n",
    "\n",
    "---\n",
    "\n",
    "En nuestro ejemplo, Biplot representa: \n",
    "\n",
    "-  Cada provincia sobre las *dos coordenadas CP1 y CP2*.\n",
    "\n",
    "- *Las variables originales como vectores*.\n",
    "\n",
    "   - Las variables que forman ángulos muy pequeños indican que están correlacionadas. Por ejemplo: Cereales, Oleaginosas y Transporte a Granel.\n",
    "\n",
    "   - Por el contrario, si las variables son ortogonales (perpendiculares) indica que no están correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generamos el gráfico Biplot mediante una función que se puede ver en la notebook 2_PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# definimos una función para hacer un Biplot\n",
    "def biplot(df_pca, pca_loadings):\n",
    "    fig , ax1 = plt.subplots(figsize=(12,9)); ax1.set_xlim(-4.5,4.5); ax1.set_ylim(-4.5,4.5)\n",
    "    # Ploteamos a las provincias en el espacio de los Componentes Principales 1 y 2\n",
    "    for i in df_pca.index:\n",
    "        ax1.annotate(i, (-df_pca.CP1.loc[i], df_pca.CP2.loc[i]), ha='center')\n",
    "\n",
    "    # Ploteamos las líneas de referencia\n",
    "    ax1.hlines(0,-4.5,4.5, linestyles='dotted', colors='grey'); \n",
    "    ax1.vlines(0,-4.5,4.5, linestyles='dotted', colors='grey');\n",
    "    ax1.set_xlabel('Primer CP1');ax1.set_ylabel('Segundo CP2')   \n",
    "\n",
    "    # Creamos ejes secundarios\n",
    "    ax2 = ax1.twinx().twiny(); ax2.set_ylim(-1,1); ax2.set_xlim(-1,1); \n",
    "    ax2.tick_params(axis='y', colors='orange');\n",
    "    ax2.set_xlabel('Vectores de loadings de los Componentes Principales', color='orange')\n",
    "\n",
    "    # Ploteamos a las variables originales en relación a los Componentes Principales 1 y 2\n",
    "    for i in pca_loadings[['CP1', 'CP2']].index:\n",
    "        ax2.annotate(i, (pca_loadings.CP1.loc[i], pca_loadings.CP2.loc[i]), color='orange')\n",
    "\n",
    "    # Plot vectors\n",
    "    ax2.arrow(0,0,pca_loadings.CP1[0], pca_loadings.CP2[0]);\n",
    "    ax2.arrow(0,0,pca_loadings.CP1[1], pca_loadings.CP2[1]);\n",
    "    ax2.arrow(0,0,pca_loadings.CP1[2], pca_loadings.CP2[2]);\n",
    "    ax2.arrow(0,0,pca_loadings.CP1[3], pca_loadings.CP2[3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "biplot(df_pca, pca_loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variables latentes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Se pueden deducir **variables latentes** en el gráfico:\n",
    "\n",
    "- CP1 es la variable con mayor varianza y tiene todos sus valores positivos (al dar vuelta el signo). \n",
    "\n",
    "   Se interpreta como un **componente de tamaño**. Es decir que ordena a las provincias por tamaño a nivel económico.\n",
    "\n",
    "- CP2 tiene valores positivos y negativos, se puede interpretar como un **componente de forma**. \n",
    "\n",
    "   Valores negativos más pequeños (mayores en valor absoluto), corresponden a provincias con mayor importancia a nivel productos de campo (Cereales, Oleaginosas, Transporte a Granel).\n",
    "   \n",
    "   Valores positivos mayores, parecen asociados a provincias con mayor importancia en producción de energía (Neuquén, Rio Negro, Mendoza, Chubut).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> PCA - Representación Algebraica\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_34_CART/Presentacion/img/M5_CLASE_34_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representación Algebraica\n",
    "\n",
    "---\n",
    "Veamos brevemente PCA desde el algebra lineal. Para más detalles, ver la notebook *1_algebra_lineal*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- PCA transforma el *dataset original X* en un *nuevo dataset U* que representan las observaciones del dataset original sobre el nuevo sistema de coordenadas de los componentes principales.\n",
    "\n",
    "Podemos representarlo así:\n",
    "\n",
    "$U = t(W) * X$ donde $t(W)$ es la transpuesta de W. \n",
    "\n",
    "Pero como se calcula W?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W es el conjunto de autovectores que se calcula a partir de los autovalores de la matriz de covarianza de X.\n",
    "\n",
    "Cada autovector es una columna de W; el total k de columnas seleccionado determina la cantidad k de componentes principales que usamos para reducir la dimensionalidad.\n",
    "\n",
    "Los autovectores se ordenan de acuerdo a orden de mayor a menor de sus autovalores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos entenderlo al ver como aplicamos (`fit_transform`) los features originales X (`df_std`) al modelo `pca_df`, generando el nuevo dataset U (`df_pca`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "name_columns = ['CP1', 'CP2', 'CP3', 'CP4','CP5', 'CP6', 'CP7', 'CP8','CP9', 'CP10', 'CP11']\n",
    "df_pca = pd.DataFrame(pca_df.fit_transform(df_std), columns=name_columns, index=df.index)\n",
    "df_pca.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_34_CART/Presentacion/img/M5_CLASE_34_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "---\n",
    "PCA es un algoritmo no supervisado, aplicado a la reducción de dimensiones.\n",
    "\n",
    "- Implementa un proceso de *feature extraction*, donde **creamos** nuevas variables, los componentes principales, y nos quedamos con un subconjunto de ellos intentando minimizar la pérdida de información.\n",
    "\n",
    "- Tiene otras características:\n",
    "\n",
    "   - **Descubrir variables latentes**, aquellas que no se observan directamente desde los datos, sino que son inferidas a partir de los CPs.\n",
    "\n",
    "   - **Facilitar la visualización de dataset multivariados**, reduciendo las features originales a dos dimensiones.\n",
    "\n",
    "   - **Conocer los loadings**, que representan el peso de las variables originales sobre cada CP, y muestran su influencia.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Manifold Learning\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_34_CART/Presentacion/img/M5_CLASE_34_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "---\n",
    "Vimos que PCA es una técnica no supervisada para reducir la dimensionalidad de los datos **basada en transformaciones lineales**.\n",
    "\n",
    " **Manifold learning** es una técnica que la reduce con **transformaciones no lineales**.\n",
    "\n",
    "Los algoritmos se basan en la idea de que la dimensionalidad en los datasets es artificialmente alta.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_007_manifold.png\" alt=\"manifold\" width=90% height=80% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manifolds\n",
    "\n",
    "---\n",
    "Manifold learning intenta representar los **manifolds**, superficies suaves y curvas dentro del espacio multidimensional.\n",
    "\n",
    "En general las imagenes están formadas por *sutiles transformaciones que PCA no encuentra*.\n",
    "\n",
    "Esos \"parches lineales locales\" (*local linear patches*) son **tangentes** (rectas que toca en un punto a la curva) y pueden ser calculados.\n",
    "\n",
    "Usualmente *los parches son abundantes* como para representar con exactitud al manifold.\n",
    "\n",
    "Algunos algoritmos no trabajan así realmente, pero la idea fundamental es la misma.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_008_manifold_ejemplo1.png\" alt=\"manifold_ejemplo1\" width=65% height=55% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distribución de datos y conectividad\n",
    "\n",
    "---\n",
    "La suposición clave es que en una estructura de alta dimensión, la información más relevante *se concentra en un pequeño número de manifolds de baja dimensión*. \n",
    "\n",
    "Esto se conoce como la hipótesis de manifold que incluye dos puntos principales: **distribución de datos y conectividad**.\n",
    "\n",
    "- Supone que la *distribución de probabilidad* en conjuntos de datos como imágenes o texto está *altamente concentrada*.\n",
    "\n",
    "- La *conectividad* implica que los puntos de datos relevantes *están conectados* a otros puntos de datos relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Este concepto se puede observar al reducir la dimensionalidad para \"estirar\" un dataset con forma de roll.\n",
    "\n",
    "- **PCA** preserva la forma de los datos, donde los puntos están conectados entre sí.\n",
    "\n",
    "- **Manifold learning** solo evalúa la distancia entre agrupamientos de puntos. Podemos decir que los modelos de manifolds tratan la superficie curvada como una composición de pequeños \"barrios\".\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_45_Reduccion_Dimensionalidad/Presentacion/img/M6_CLASE_45_009_manifold_ejemplo2.png\" alt=\"manifold_ejemplo2\" width=65% height=55% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Manifold learning usa las propiedades geométricas de los datos para implementar:\n",
    "\n",
    "**1.** **Clustering**: encontremos grupos de puntos similares. \n",
    "\n",
    "**2.** **Reducción de la dimensionalidad**: proyectemos puntos en un espacio de dimensionalidad menor preservando la estructura. \n",
    "\n",
    "**3.** Semi-supervisado, supervisado: Dados puntos etiquetados / no etiquetados, creamos una **función de etiquetado**. \n",
    "\n",
    "\n",
    "La noción de **cercanía** se puede calcular desde distintos puntos de vista:\n",
    "\n",
    "1. Probabilístico: la densidad acorta las distancias.\n",
    "\n",
    "2. Cluster: los puntos en las regiones conectadas comparten las mismas propiedades.\n",
    "\n",
    "3. Manifold: la distancia debe medirse \"a lo largo\" de los manifold.\n",
    "\n",
    "4. Versión mixta: dos puntos \"cercanos\" son aquellos conectados por un camino corto que atraviesa regiones de alta densidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> t-SNE\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_34_CART/Presentacion/img/M5_CLASE_34_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## t-SNE\n",
    "\n",
    "---\n",
    "t-SNE (t-distributed Stochastic Neighbor Embedding) genera una representación en 2D que preserva la distancia entre los puntos: *los puntos que están cerca en el dataset original quedan cerca en el espacio resultante* y los que están lejos, quedan lejos. \n",
    "\n",
    "El algoritmo convierte las relaciones o similitudes en el espacio original en *distribuciones t de Student*.\n",
    "\n",
    "Así, t-SNE se centra en *la estructura local de los datos* e intenta *extraer grupos locales agrupados* en lugar de intentar \"desenrollarlos\" o \"desplegarlos\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "\n",
    "---\n",
    "Vamos a considerar un conjunto de imagenes de dígitos escritos a mano de `datasets.load_digits`.\n",
    "\n",
    "Solo tomamos números entre 1 y 6, `n_class=6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets)\n",
    "\n",
    "digits = datasets.load_digits(n_class=6)\n",
    "X = digits.data; y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_img_per_row = 20\n",
    "img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n",
    "\n",
    "plt.imshow(img, cmap=plt.cm.binary)\n",
    "plt.xticks([]);  plt.yticks([]); plt.title('A selection from the 64-dimensional digits dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "\n",
    "---\n",
    "Mediante t-SNE, reducimos la dimensionalidad representando los dígitos en dos dimensiones. \n",
    "\n",
    "Y luego los agrupamos según su valor real indicado en `digits.target`.\n",
    "\n",
    "Usamos los hiperparámetros:\n",
    "\n",
    "-  `n_components=2`. Reduce a dos dimensiones. \n",
    "-  `init='pca'`. Es recomendable usar previamente otro método de reducción de dimensionalidad, para llevarlo a un número razonable de dimensiones (por ejemplo 50), si el dataset original tiene alta dimensionalidad.\n",
    "\n",
    "Para graficar la clasificación, usamos la función `plot_embedding`, que se encuentra en  <a href=\"https://www.w3cschool.cn/doc_scikit_learn/scikit_learn-auto_examples-manifold-plot_lle_digits.html?lang=en#sphx-glr-auto-examples-manifold-plot-lle-digits-py\" target=\"_blank\">w3cschool - Example: Manifold learning on handwritten digits</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(digits.target[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(digits.data.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# t-SNE embedding of the digits dataset\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "t0 = time()\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plot_embedding(X_tsne, \"t-SNE embedding of the digits (time %.2fs)\" % (time() - t0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_34_CART/Presentacion/img/M5_CLASE_34_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "---\n",
    "Vimos dos tipos de métodos para trabajar reducción de la dimensionalidad.\n",
    "\n",
    "- PCA se basa en **transformaciones lineales**.\n",
    "\n",
    "- Manifold learning reduce con **métodos no lineales**.\n",
    "\n",
    "---\n",
    "\n",
    "- PCA intenta crear varios hiperplanos lineales para representar las dimensiones.\n",
    "\n",
    "- Manifold learning intenta representar manifolds, las superficies suaves y curvas dentro del espacio multidimensional.\n",
    "\n",
    "---\n",
    "\n",
    "- PCA funciona bien con datos separados en el espacio (sparse), con relaciones tangentes, paralelas, envolventes u ortogonales.\n",
    "\n",
    "- Manifolds learning trabaja mejor con datos agrupados, densos, sesgados, con valores extremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "----\n",
    "\n",
    "A partir del dataset sobre jugadores de futbol, aplicamos PCA para:\n",
    "\n",
    "- Comprobar cuantas componentes principales necesitamos para reducir la dimensionalidad con baja pérdida de información.\n",
    "- Validarlo con la técnica del bastón quebrado.\n",
    "- Calcular los loadings e identificar las variables originales que más influyen sobre las CPs.\n",
    "- Generar un Biplot y buscar variables latentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2PHSRoPK8OK",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_ej = pd.read_csv('../Data/Fifa_players.csv',sep=';', index_col='Name')\n",
    "print('Total de filas: ',df_ej.shape[0],'Total de columnas: ',df_ej.shape[1])\n",
    "df_ej.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estandarizamos las features. Y aplicamos PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "std = StandardScaler()\n",
    "df_std_ej = pd.DataFrame(std.fit_transform(df_ej))\n",
    "# pca_df_ej = \n",
    "# pca_df_ej.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El ratio de la varianza explicada nos indica que los ... primeros explican el ... = ..% de la varianza de los datos. Nos quedamos con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# pca_df_ej......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Graficamos el ratio de varianza explicada en función de la cantidad de componentes. Y comprobamos que la técnica del baston quebrado nos indica también que seleccionemos ... CPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# descomentar y completar:\n",
    "\n",
    "#plt.figure(figsize=(4,3))\n",
    "\n",
    "#plt.plot(range(..), pca_df_ej.explained_variance_ratio_, '-o', label='Componente individual')\n",
    "#plt.plot(range(..), np.cumsum(pca_df_ej.explained_variance_ratio_), '-s', label='Acumulado')\n",
    "\n",
    "#plt.ylabel('Porcentaje de Varianza Explicada'); plt.xlabel('Componentes Principales')\n",
    "#plt.ylim(0,1.05); plt.xticks(range(..))\n",
    "#plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vemos los loadings, el peso de las variables originales sobre los dos CPs seleccionados.\n",
    "\n",
    "- En CP1, ..... son las variables que más influyen negativamente.\n",
    "\n",
    "  Podemos decir que es una componente de .......\n",
    "\n",
    "- En CP2, .... influye más en forma positiva, y .... más en forma negativa.\n",
    "\n",
    "  Al tener valores positivos y negativos, es una componente de ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# descomentar y completar:\n",
    "\n",
    "#pca_loadings_ej = pd.DataFrame(.......components_.T, index=.......columns).iloc[:,:2]\n",
    "#pca_loadings_ej.rename(columns = {0 : 'CP1', 1 : 'CP2'}, inplace = True)\n",
    "#pca_loadings_ej.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para graficar el Biplot, primero tenemos que proyectar los datos originales sobre el nuevo sistema de coordenadas del espacio generado por las componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# descomentar y completar:\n",
    "\n",
    "#name_columns = ['CP1', 'CP2', 'CP3', 'CP4','CP5', 'CP6', 'CP7','CP8','CP9', 'CP10', 'CP11' ]\n",
    "#df_pca_ej = pd.DataFrame(......fit_transform(......), columns=name_columns, index=df_ej.index)\n",
    "#df_pca_ej.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generamos el Biplot usando la función biplot usada en la presentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# descomentar después de completar las celdas anteriores:\n",
    "\n",
    "# biplot(df_pca_ej, pca_loadings_ej)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Se pueden deducir **variables latentes** en el gráfico:\n",
    "\n",
    "- CP1 es la variable con mayor varianza y tiene todos sus valores con igual signo. \n",
    "\n",
    "   Se interpreta como un **componente de .....**. Podemos decir que ..... son los jugadores más importantes.\n",
    "\n",
    "- CP2 tiene valores positivos y negativos, se puede interpretar como un **componente de .....**. \n",
    "\n",
    "   Valores positivos mayores, están asociados a mayor ....... Con su exponente máximo .......\n",
    "   \n",
    "   Valores negativos menores, están asociados a mayores valores en ......... Se destacan los jugadores ........"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solución\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2PHSRoPK8OK",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_ej = pd.read_csv('../Data/Fifa_players.csv',sep=';', index_col='Name')\n",
    "print('Total de filas: ',df_ej.shape[0],'Total de columnas: ',df_ej.shape[1])\n",
    "df_ej.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estandarizamos las features. Y aplicamos PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "std = StandardScaler()\n",
    "df_std_ej = pd.DataFrame(std.fit_transform(df_ej))\n",
    "pca_df_ej = PCA()\n",
    "pca_df_ej.fit(df_std_ej)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El ratio de la varianza explicada nos indica que los dos primeros explican el 81 + 8 = 89% de la varianza de los datos. Nos quedamos con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca_df_ej.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Graficamos el ratio de varianza explicada en función de la cantidad de componentes. Y comprobamos que la técnica del baston quebrado nos indica también que seleccionemos dos CPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "\n",
    "plt.plot(range(11), pca_df_ej.explained_variance_ratio_, '-o', label='Componente individual')\n",
    "plt.plot(range(11), np.cumsum(pca_df_ej.explained_variance_ratio_), '-s', label='Acumulado')\n",
    "\n",
    "plt.ylabel('Porcentaje de Varianza Explicada'); plt.xlabel('Componentes Principales')\n",
    "plt.ylim(0,1.05); plt.xticks(range(11))\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Vemos los loadings, el peso de las variables originales sobre los dos CPs seleccionados.\n",
    "\n",
    "- En CP1, BallControl, Curve, Dribbling y Free_kick son las variables que más influyen negativamente.\n",
    "\n",
    "  Podemos decir que es una componente de tamaño, si tomamos todos los valores como positivos, cambiando el signo.\n",
    "\n",
    "- En CP2, Aggresion influye más en forma positiva, y Balance mas en forma negativa.\n",
    "\n",
    "  Al tener valores positivos y negativos, es una componente de forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca_loadings_ej = pd.DataFrame(pca_df_ej.components_.T, index=df_ej.columns).iloc[:,:2]\n",
    "pca_loadings_ej.rename(columns = {0 : 'CP1', 1 : 'CP2'}, inplace = True)\n",
    "pca_loadings_ej.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para graficar el Biplot, primero tenemos que proyectar los datos originales sobre el nuevo sistema de coordenadas del espacio generado por las componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "name_columns = ['CP1', 'CP2', 'CP3', 'CP4','CP5', 'CP6', 'CP7','CP8','CP9', 'CP10', 'CP11' ]\n",
    "df_pca_ej = pd.DataFrame(pca_df_ej.fit_transform(df_std_ej), columns=name_columns, index=df_ej.index)\n",
    "df_pca_ej.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generamos el Biplot usando la función biplot usada en la presentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "biplot(df_pca_ej, pca_loadings_ej)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Se pueden deducir **variables latentes** en el gráfico:\n",
    "\n",
    "- CP1 es la variable con mayor varianza y tiene todos sus valores con igual signo. \n",
    "\n",
    "   Se interpreta como una **componente de tamaño**. Podemos decir que Messi, Neymar y Ronaldo son los más importantes.\n",
    "\n",
    "- CP2 tiene valores positivos y negativos, se puede interpretar como una **componente de forma**. \n",
    "\n",
    "   Valores positivos mayores, están asociados a mayor agresión. Con su exponente máximo Ibrahimovic.\n",
    "   \n",
    "   Valores negativos menores, están asociados a mayores valores en Aceleración, Balance y Agilidad. Se destacan Messi, Hazard y Dybala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias y Material Adicional\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_21_Regresion_Lineal_Simple/Presentacion/img/M3_CLASE_21_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Referencias y Material Adicional\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/decomposition.html#pca\" target=\"_blank\">User guide PCA - Scikit-learn</a>\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" target=\"_blank\">Documentation PCA in Scikit-learn</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\" target=\"_blank\">A One-Stop Shop for Principal Component Analysis</a>\n",
    "\n",
    "<a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\" target=\"_blank\">Python Data Science Handbook, Jake VanderPlas</a>\n",
    "\n",
    "<a href=\"http://biplot.usal.es/multbiplot/documentation/notas-sobre-biplot-clasico-.pdf\" target=\"_blank\">Los Métodos Biplot (Teoría)</a>\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/manifold.html\" target=\"_blank\">Manifold - Scikit-learn</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/manifold-learning-t-sne-lle-isomap-made-easy-42cfd61f5183\" target=\"_blank\">Manifold Learning [t-SNE, LLE, Isomap, +] Made Easy</a>\n",
    "\n",
    "<a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html\" target=\"_blank\">Python DataScience Handbook 05.10 Manifold Learning</a>\n",
    "\n",
    "<a href=\"https://www.w3cschool.cn/doc_scikit_learn/scikit_learn-auto_examples-manifold-plot_lle_digits.html?lang=en#sphx-glr-auto-examples-manifold-plot-lle-digits-py\" target=\"_blank\">w3cschool - Example: Manifold learning on handwritten digits</a>\n",
    "\n",
    "<a href=\"https://distill.pub/2016/misread-tsne/\" target=\"_blank\">How to Use t-SNE Effectively</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
