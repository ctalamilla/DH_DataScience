{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-arbitration",
   "metadata": {},
   "source": [
    "[<img src=\"https://www.digitalhouse.com/ar/logo-DH.png\" width=\"400\" height=\"200\" align='right'>](http://digitalhouse.com.ar/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-religious",
   "metadata": {},
   "source": [
    "# Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-removal",
   "metadata": {},
   "source": [
    "### Nota:\n",
    "\n",
    "En este ejercicio vamos a escalar las features del dataset usando `MinMaxScaler` con el objetivo de que tengan un ejercicio resuelto de ejemplo con una alternativa a `StandardScaler`, no porque consideremos que en este problema `MinMaxScaler` resulte en una mejor performance que `StandardScaler`.\n",
    "\n",
    "---\n",
    "\n",
    "Aunque la normalización a través de min-max es una técnica de uso común que es útil cuando necesitamos valores en un intervalo acotado, la estandarización puede ser más práctica para muchos algoritmos de aprendizaje automático. \n",
    "\n",
    "La razón es que muchos modelos lineales inicializan las ponderaciones en O o valores aleatorios pequeños cercanos a 0.\n",
    "\n",
    "Usando la estandarización centramos las columnas de features en la media 0 con el desvío estándar 1, así las columnas de features adoptan la forma de una distribución normal, lo que facilita el aprendizaje de los pesos. \n",
    "\n",
    "Además, la estandarización mantiene información útil sobre los valores atípicos y hace que el algoritmo sea menos sensible a ellos en contraste con el escalado min-max, que escala los datos a un rango limitado de valores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-population",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import eval_measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-concentrate",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Este dataset contiene los precios y otros atributos de casi 54.000 diamantes.\n",
    "\n",
    "Sus features son:\n",
    "\n",
    "* **price**: price in US dollars (\\$326--\\$18,823).  **Esta es la variable target**.\n",
    "\n",
    "* carat: weight of the diamond (0.2--5.01)\n",
    "\n",
    "* cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "\n",
    "* color: diamond colour, from J (worst) to D (best)\n",
    "\n",
    "* clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "\n",
    "* x: length in mm (0--10.74)\n",
    "\n",
    "* y: width in mm (0--58.9)\n",
    "\n",
    "* z: depth in mm (0--31.8)\n",
    "\n",
    "* depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "\n",
    "* table: width of top of diamond relative to widest point (43--95)\n",
    "\n",
    "Fuente: https://www.kaggle.com/shivam2503/diamonds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-browser",
   "metadata": {},
   "source": [
    "## Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/diamonds.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-distribution",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Normalicemos las features y creemos las variables dummies necesarias para poder entrenar un modelo de regresión para predecir el valor de `price` para cada registro\n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['cut', 'color', 'clarity']\n",
    "\n",
    "enc = OneHotEncoder(drop='first')\n",
    "X = data[categoricals]\n",
    "enc.fit(X)\n",
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = enc.transform(X).toarray()\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_df = pd.DataFrame(dummies)\n",
    "dummies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [categoricals[i] + '_' + enc.categories_[i] for i in range(len(categoricals)) ]\n",
    "\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_drop_first = [sublist[i] for sublist in col_names for i in range(len(sublist)) if i != 0]\n",
    "col_names_drop_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_df.columns = col_names_drop_first\n",
    "dummies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-asthma",
   "metadata": {},
   "source": [
    "Otra opción es usar `get_dummies`\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-welcome",
   "metadata": {},
   "source": [
    "Ahora estandarizamos las features numéricas:\n",
    "\n",
    "`carat` `depth` `table` `x` `y` `z`\n",
    "\n",
    "usando  `MinMaxScaler`\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "\n",
    "X = data[numericals]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "std_numerical_data = scaler.transform(X)\n",
    "std_df = pd.DataFrame(std_numerical_data)\n",
    "std_df.columns = [i + '_std' for i in numericals]\n",
    "std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-cookie",
   "metadata": {},
   "source": [
    "Entonces nuestro dataset de features serán las variables dummies y las variables numéricas estandarizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([dummies_df, std_df], axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-comfort",
   "metadata": {},
   "source": [
    "Y la variable target es `price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-syria",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Separemos el conjunto en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 117)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-format",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Ajustemos una regresión lineal múltiple con los datos del conjunto de entrenamiento usando statsmodels y evaluemos la significancia de cada uno de los coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos que agregar explícitamente a una constante:\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "model = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reg_model_params = model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-participation",
   "metadata": {},
   "source": [
    "Vemos que los p-value de los coeficientes de y_std y z_std son altos, por lo tanto no podemos rechazar la hipótesis nula que dice que los coeficientes de esas dos variables son 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_prediction_train = model.predict(X_train_sm)\n",
    "print(eval_measures.rmse(y_train, sm_prediction_train))\n",
    "\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "sm_prediction_test = model.predict(X_test_sm)\n",
    "print(eval_measures.rmse(y_test, sm_prediction_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-conducting",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Ajustamos el modelo aplicando regularización de Lasso y validación cruzada para estimar el mejor valor de $\\alpha$ para este problema\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n",
    "\n",
    "¿Cuál es el mejor valor de $\\alpha$ para este problema?\n",
    "\n",
    "¿Cuál es el score obtenido ($R^2$) para este modelo en entrenamiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el rango de de búsqueda del hiperparametro explicitamente\n",
    "lm_lasso = linear_model.LassoCV(alphas=[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01,\\\n",
    "                                        0.05, 0.1, 1, 5, 10],\\\n",
    "                                        normalize = False, cv = 5) \n",
    "\n",
    "model_cv = lm_lasso.fit(X_train, y_train)\n",
    "\n",
    "model_cv.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-criminal",
   "metadata": {},
   "source": [
    "## Ejercicio 5 \n",
    "\n",
    "Ajustemos los datos de entrenamiento con una regresión con regularización de Lasso para el valor de $\\alpha$ calculado en el punto anterior usando statsmodels.\n",
    "\n",
    "Usemos scatterplots para mostrar \n",
    "\n",
    "* los valores de los coeficientes de la regresión lineal múltiple obtenidos en el Ejercicio 3, y los valores de los coeficientes de la regresión lineal con regularización de Lasso para el modelo entrenado.\n",
    "\n",
    "* los valores de los residuos en entrenamiento resultado del Ejercicio 3, y los residuos en entrenamiento para el modelo con regularización.\n",
    "\n",
    "https://www.statsmodels.org/0.6.1/generated/statsmodels.regression.linear_model.OLS.fit_regularized.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = model_cv.alpha_\n",
    "\n",
    "#L1_wt : 0, the fit is ridge regression. 1, the fit is the lasso \n",
    "\n",
    "no_reg_model = sm.OLS(y_train, X_train_sm)\n",
    "\n",
    "reg_model = no_reg_model.fit_regularized(alpha = best_alpha, L1_wt = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    " reg_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=reg_model.params, y=no_reg_model_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_residuals = y_train - reg_model.fittedvalues\n",
    "\n",
    "linear_residuals = y_train - model.fittedvalues\n",
    "\n",
    "sns.scatterplot(x = reg_residuals, y = linear_residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-ready",
   "metadata": {},
   "source": [
    "## Ejercicio 6\n",
    "\n",
    "Usandos statsmodels y scikit-learn calculemos la performance en test del modelo construído y comparemos los resultados de las dos bibliotecas usando como métricas el error absoluto medio (MAE) y la raiz del error cuadrático medio (RMSE) \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sm_prediction = reg_model.predict(X_test_sm)\n",
    "sm_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_lasso = linear_model.Lasso(alpha = best_alpha, fit_intercept=True, normalize=False)\n",
    "\n",
    "skl_lasso = skl_lasso.fit(X= X_train, y = y_train)\n",
    "\n",
    "skl_prediction = skl_lasso.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_residuals = y_test - skl_prediction\n",
    "\n",
    "sm_residuals = y_test - sm_prediction\n",
    "\n",
    "sns.scatterplot(x = skl_residuals, y = sm_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lasso_coef = np.insert(skl_lasso.coef_, 0, skl_lasso.intercept_)\n",
    "\n",
    "sns.scatterplot(x = lasso_coef, y = reg_model.params);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-board",
   "metadata": {},
   "source": [
    "Métricas en `statsmodels`\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.tools.eval_measures.rmse.html\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.tools.eval_measures.meanabs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_measures.rmse(y_test, sm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_measures.meanabs(y_test, sm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de scikit-learn\n",
    "metrics.r2_score(y_test, sm_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-newcastle",
   "metadata": {},
   "source": [
    "Métricas en `scikit-learn`\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(metrics.mean_squared_error(y_test, skl_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, skl_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de scikit-learn\n",
    "metrics.r2_score(y_test, skl_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-estonia",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "https://www.kaggle.com/yogendran/intro-to-linear-ridge-and-lasso-regressions\n",
    "    \n",
    "https://towardsdatascience.com/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
