{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/paulati/Nuevo vol/paula/dh/2021/dsad_2021_paula/common\n",
      "default checking\n",
      "Running command `conda list`... ok\n",
      "jupyterlab=2.2.6 already installed\n",
      "pandas=1.1.5 already installed\n",
      "bokeh=2.2.3 already installed\n",
      "seaborn=0.11.0 already installed\n",
      "matplotlib=3.3.2 already installed\n",
      "ipywidgets=7.5.1 already installed\n",
      "pytest=6.2.1 already installed\n",
      "chardet=4.0.0 already installed\n",
      "psutil=5.7.2 already installed\n",
      "scipy=1.5.2 already installed\n",
      "statsmodels=0.12.1 already installed\n",
      "scikit-learn=0.23.2 already installed\n",
      "xlrd=2.0.1 already installed\n",
      "Running command `conda install --yes nltk=3.5.0`... ok\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "unidecode=1.1.1 already installed\n",
      "pydotplus=2.0.2 already installed\n",
      "pandas-datareader=0.9.0 already installed\n",
      "flask=1.1.2 already installed\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_portada.png\" align=\"center\" width=\"90%\"/></div>\n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 3</div>\n",
    "            <div class=\"div-contenido\">Regularización</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- Entender la regularización como técnica para evitar el sobreajuste\n",
    "\n",
    "- Aplicar regularización usando scikit-learn con  validación cruzada para el ajuste de hiperparámetros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Sesgo - Varianza\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sesgo-varianza\n",
    "\n",
    "---\n",
    "\n",
    "Dado el modelo:\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$Y = f(x) + \\epsilon$$\n",
    "</p>\n",
    "\n",
    "Donde epsilon es un término aleatorio con distribución:\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$\\epsilon \\sim N(0, \\sigma_{\\epsilon})$$\n",
    "</p>\n",
    "    \n",
    "Podemos obtener una estimación de f para hacer predicciones sobre Y:\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$\\hat{Y} = \\hat{f}(X)$$\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La esperanza del error de predicción al cuadrado será la siguiente:\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$Err(x) = E[(Y - \\hat{f}(x))^2]$$\n",
    "</p>\n",
    "\n",
    "Podemos descomponer la esperanza del error al cuadrado de la siguiente manera: \n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$Err(x) = (E[ \\hat{f}(x)]-f(x))^2 + E[(\\hat{f}(x) - E[\\hat{f}(x)])^2] + \\sigma_{\\epsilon}^2$$\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$Err(x) = Bias^2 + Variance + Irreducible \\space error$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si el modelo es **demasiado simple** (tiene pocos grados de libertad), entonces no importa cuán grande sea la muestra tenemos **sesgo o error sistemático**:\n",
    "\n",
    "$$E[(\\hat{f}(x)] \\ne E[(f(x)]$$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_sesgo.png\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si el modelo es **demasiado complejo** (ie. tiene demasiados grados de libertad), entonces el estimador puede ajustar regularidades espurias de la muestra generando **sobreajuste**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_overfitting.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto, el modelo no debe ser ni muy simple ni muy complejo\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_sesgo_varianza.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_dilema_sesgo_varianza.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Regularización\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularización: una primera intuición\n",
    "\n",
    "---\n",
    "\n",
    "Función de pérdida de una regresión lineal:\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$CF = \\sum_{i=1}^N(\\hat{y}_i - y_i)^2$$\n",
    "</p>\n",
    "\n",
    "Las técnicas de regularización agregan una **\"penalidad\"** a esta función de costo:\n",
    "\n",
    "\n",
    "    \n",
    "<p style=\"font-size:20px;\">\n",
    "$$CF = \\sum_{i=1}^N(\\hat{y}_i - y_i)^2 + \\alpha . \\vec{\\theta}$$\n",
    "</p>\n",
    "\n",
    "$\\vec{\\theta}$ (theta) es el vector que corresponde a los parámetros del modelo (en una regresión lineal, los betas) y alpha es un parámetro que “regula” la fuerza de la penalización: cuanto más grande es, mayor es la penalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vamos a ver a continuación dos técnicas de regularización\n",
    "\n",
    "**Regresión Ridge**\n",
    "\n",
    "**Regresión Lasso**\n",
    "\n",
    "Estas técnicas proponen cambiar ligeramente el problema de optimización de mínimos cuadrados, para intentar **\"achicar\" (shrink) el valor absoluto de los estimadores de los betas $\\beta_i$**. \n",
    "\n",
    "\n",
    "**¿Por qué esto mejoraría la estimación?** \n",
    "\n",
    "Vamos a ver de qué forma este método **introduce un sesgo pero reduce la varianza**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normas L1 y L2\n",
    "\n",
    "---\n",
    "\n",
    "### Norma L0\n",
    "\n",
    "La cantidad de elementos distintos de cero en el vector.\n",
    "\n",
    "\n",
    "#### Norma L1\n",
    "\n",
    "La suma de los valores absolutos de los elementos del vector\n",
    "\n",
    "$\\lVert \\beta \\rVert_1 = \\sum_i \\vert x_i \\vert$\n",
    "\n",
    "\n",
    "#### Norma L2\n",
    "\n",
    "La raíz cuadrada de la suma de cuadrados de los elementos del vector\n",
    "\n",
    "$\\lVert \\beta \\rVert_2 = \\sqrt{\\sum_i  x_i^2 }$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### Normalización L1\n",
    "\n",
    "La idea de esta normalización es que la suma del valor absoluto sea unitaria:\n",
    "\n",
    "$\\sum_{j=1}^p \\lVert x_{ij} \\rVert = 1 \\space \\space \\forall i = 1 .. n$\n",
    "\n",
    "#### Normalización L2\n",
    "\n",
    "La idea de esta normalización es que la suma del valor absoluto al cuadrado sea unitaria\n",
    "\n",
    "$\\sum_{j=1}^p \\lVert x_{ij} \\rVert^2 = 1 \\space \\space \\forall i = 1 .. n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regresión Ridge\n",
    "\n",
    "---\n",
    "\n",
    "Recordemos la función que se minimiza en la estimación de mínimos cuadrados:\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$RSS = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2  $$\n",
    "</p>\n",
    "\n",
    "La función que se minimiza en Regresión Ridge es:\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$\\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2  + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2$$\n",
    "</p>\n",
    "\n",
    "La diferencia es que agregamos un término nuevo. En este término, un **hiperparámetro $\\lambda$** (lambda) penaliza el valor de los coeficientes al cuadrado. \n",
    "Entonces, tenemos que minimizar el cuadrado de los errores, intentando que ningún $\\beta_j^2$ sea demasiado grande.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"font-size:20px;\">\n",
    "$$\\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2  + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2$$\n",
    "</p>\n",
    "\n",
    "\n",
    "- Al igual que MCO, buscamos achicar la función de pérdida.\n",
    "\n",
    "- Existe un **término de penalización**, que es menor cuando los betas se acercan a cero, por lo tanto tiene el efecto de achicar los mismos hacia cero (tanto si son negativos como positivos)\n",
    "\n",
    "- El **hiperparámetro $\\lambda$** maneja la ponderación de cada término.\n",
    "\n",
    "    ¿Cuál es el mejor valor para lambda? ¿Cómo elegíamos el valor óptimo de un hiperparámetro?\n",
    "\n",
    "    Como siempre, lo hacemos mediante **CROSS VALIDATION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo Ridge. Dataset Credit.\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_credit_ridge.png\" align=\"center\" />\n",
    "\n",
    "\n",
    "La línea negra sólida representa la estimación de la regresión de ridge para el coeficiente de `income` en función de $\\lambda$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En el extremo izquierdo del gráfico, $\\lambda$ es esencialmente cero, por lo que las estimaciones de coeficientes de ridge correspondientes son las mismas que las estimaciones de mínimos cuadrados habituales. \n",
    "\n",
    "\n",
    "A medida que aumenta $\\lambda$, las estimaciones del coeficiente de ridge se reducen a cero. Cuando $\\lambda$ es extremadamente grande, entonces todas las estimaciones del coeficiente de ridge son básicamente cero; esto corresponde al modelo nulo que no contiene predictores.\n",
    "\n",
    "En el panel de la derecha, podemos pensar el eje x como la cantidad en que se ha reducido el tamaño de las estimaciones del coeficiente de regresión de ridge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regresión Ridge: Sensibilidad a la escala\n",
    "\n",
    "---\n",
    "\n",
    "- En Regresión Ridge tanto la estimación de los coeficientes como la predicción son **sensibles a la escala**.\n",
    "\n",
    "- Si una variable se encuentra en una escala que le da un valor absoluto mayor, esto **va a afectar el cálculo de la suma de cuadrados del vector de coeficientes**.\n",
    "\n",
    "- Por esta razón **es importante estandarizar (dividir por el desvío estándar)** todos los regresores antes de ejecutar una regresión Ridge. Así ya no están en unidades físicas sino en unidades de su propio desvío estándar.\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$\\tilde{x_{ij}} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - \\bar{x_j})^2}}$$\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tradeoff Bias-Variance\n",
    "\n",
    "---\n",
    "\n",
    "Aquí se pueden ver n=50 simulaciones, p=45 predictores, todos con coeficientes no nulos.\n",
    "\n",
    "El gráfico expresa el sesgo cuadrado (negro), varianza (verde) y el MSE del test (violeta), para una regresión ridge en los datos simulados, como una función de $\\lambda$  y de \n",
    "<p style=\"font-size:20px;\">\n",
    "$\\frac{ \\lVert{\\hat{\\beta}_{\\lambda}^R}\\rVert_2}{\\lVert{\\hat{\\beta}}\\rVert_2}$\n",
    "</p>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_tradeoff_bias_variance.png\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regresión Lasso\n",
    "\n",
    "---\n",
    "\n",
    "* La regresión Ridge tiene una clara desventaja: incluye todos los predictores p en el modelo final, a diferencia de aquellos modelos que eligen un subconjunto de predictores.\n",
    "\n",
    "* La regresión Lasso es un alternativa a Ridge, que corrige esta desventaja. Los  coeficientes $\\hat{\\beta_{\\lambda}^L}$ minimizan el número de predictores\n",
    "\n",
    "\n",
    "* Lasso utiliza la norma l1 en la penalización a diferencia de ridge que usa la norma l2. \n",
    "\n",
    "    La norma de l1 de un un vector de coeficientes $\\beta$ está dada por $\\lVert\\beta\\rVert_1 = \\sum \\vert \\beta_j \\vert$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Como en la regresión Ridge, **Lasso \"achica\" los coeficiente** estimados.\n",
    "\n",
    "    En el caso de Lasso, **la regularización l1 fuerza los coeficientes a valer exactamente cero**, en el caso de que $\\lambda$ sea lo suficientemente grande.\n",
    "\n",
    "    Por lo tanto, la regularización de **Lasso hace selección de variables**\n",
    "\n",
    "* Decimos que Lasso genera **modelos dispersos**, es decir modelos con una selección de variables\n",
    "\n",
    "* Al igual que en Ridge, la elección de un buen valor $\\lambda$ es crítico en Lasso; nuevamente **cross-validation** es el método para su elección\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo Lasso. Dataset Credit.\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_credit_lasso.png\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparación Lasso Ridge\n",
    "\n",
    "---\n",
    "\n",
    "¿Por qué Lasso, a diferencia de Ridge, resulta en coeficientes estimados exactamente igual a cero?\n",
    "\n",
    "La estimación de coeficientes de las regresión Lasso y Ridge resuelve estos problemas, respectivamente\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$ minimize \\space \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2  \\space subject \\space to \\space \\sum_{j=1}^p \\vert \\beta_j \\vert \\le s$$\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$ minimize \\space \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2  \\space subject \\space to \\space \\sum_{j=1}^p \\beta_j ^2 \\le s$$\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lasso  gráficamente\n",
    "\n",
    "---\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_lasso.png\" align=\"center\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge  gráficamente\n",
    "\n",
    "---\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_ridge.png\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_lasso_ridge.png\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.com/Digital-House-DATA/ds_blend_2021_img/blob/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_ridgelassoItayEvron.gif?raw=true\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_lasso_ridge_2.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparación Lasso Ridge\n",
    "\n",
    "---\n",
    "\n",
    "El sesgo (bias) se calcula como la distancia desde la predicción promedio y el valor verdadero, es decir el valor verdadero menos la media de predicciones \n",
    "\n",
    "$Bias^2  = (E[ \\hat{f}(x)]-f(x))^2$\n",
    "\n",
    "La varianza es el desvío promedio de la predicción promedio, es decir el valor medio de (predicción menos media de predicciones) \n",
    "\n",
    "$Variance = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$\n",
    "\n",
    "#### Sesgo y varianza en función de $\\lambda$:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_bias_variance_regularization.png\" align=\"center\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estos datos se generaron haciendo que **todos los coeficientes** fueran **diferentes a cero**. \n",
    "\n",
    "En este caso, **los dos modelos tienden a performar prácticamente igual**. Ridge tiene una menor varianza y por eso parece mejorar respecto a Lasso\n",
    "\n",
    "\n",
    "Izquierda: Sesgo cuadrado (negro), la varianza (verde) y el MSE (violeta) para Lasso en datos simulados.\n",
    "\n",
    "Derecha: Comparación del Sesgo cuadrado, la varianza y el MSE, entre **Lasso (llena)** y **Ridge (punteada)**.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_lasso_ridge_todos_los_coeficientes.png\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estos datos se generaron haciendo que **sólo dos coeficientes** fueran **diferentes a cero**. \n",
    "\n",
    "De esta forma, vemos cómo **Lasso mejora claramente la performance con respecto a Ridge**, tanto en lo referido a variancia como a MSE.\n",
    "\n",
    "Izquierda: Sesgo cuadrado (negro), la varianza (verde) y el MSE (violeta) para Lasso en datos simulados.\n",
    "\n",
    "Derecha: Comparación del Sesgo cuadrado, la varianza y el MSE, entre **Lasso (llena)** y **Ridge (punteada)**.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_lasso_ridge_dos_coeficientes.png\" align=\"center\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic Net \n",
    "\n",
    "----\n",
    "\n",
    "<p style=\"font-size:20px;\">\n",
    "$$ \\lambda_1 . \\lVert \\beta \\lVert_1 + \\lambda_2 . \\lVert \\beta \\rVert_2^2 = \\lambda . (\\lVert \\beta \\rVert_1 + \\alpha \\lVert \\beta \\rVert_2^2)$$\n",
    "</p>\n",
    "\n",
    "* ElasticNet **combina linealmente** lo mejor de ambos mundos. \n",
    "\n",
    "* El parámetro $\\lambda$ regula la complejidad del modelo. El parámetro $\\alpha$ regula la importancia relativa de Lasso vs. Ridge. \n",
    "\n",
    "* Es posible obtener soluciones parsimoniosas y bien condicionadas. \n",
    "\n",
    "* No free lunch!: ahora hay que calibrar dos hiperparámetros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Cross Validation\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Selección de Hiperparametros para Ridge y Lasso\n",
    "\n",
    "---\n",
    "\n",
    "* Se necesita un método para poder **seleccionar el mejor valor para el hiperparámetro $\\lambda$**\n",
    "\n",
    "* **Cross-validation** es una manera simple de atacar este problema. \n",
    "    Se elige un rango de valores que puede tomar el hiperparámetro, y se computan los errores que devuelve cross-validation para cada valor en el rango.\n",
    "\n",
    "* Se elige el hiperparámetro asociado al menor error computado.\n",
    "\n",
    "* Finalmente, reentrenamos el modelo con el valor elegido para el hiperparámetro .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Validación Cruzada. Repaso\n",
    "\n",
    "---\n",
    "\n",
    "¿Cómo funciona?\n",
    "\n",
    "1. Hacemos el split train/validación y test\n",
    "\n",
    "2. Dividimos el dataset de train/validación en k grupos (generalmente, 5 o 10 suele ser la medida convencional) del mismo tamaño.\n",
    "\n",
    "3. En la iteración i-ésima, el i-esimo grupo generado funciona como un conjunto de validación; el resto de los grupos es el conjunto de training\n",
    "\n",
    "4. Entrenamos un modelo sobre el conjunto de training definido en 3.\n",
    "\n",
    "5. Hacemos las predicciones sobre el conjunto de validación definido en 3. y calculamos el error sobre este conjunto\n",
    "\n",
    "6. Repetimos los pasos 3. a 5. k veces, variando así los conjuntos de entrenamiento y de validación en cada iteración.\n",
    "\n",
    "7. Promediamos los k errores obtenidos (un valor de error en cada una de las iteraciones).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_cross_validation.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "----\n",
    "\n",
    "Queremos predecir el valor de la variable `Balance` en el dataset Credit (https://www.kaggle.com/avikpaul4u/credit-card-balance).\n",
    "\n",
    "Para eso vamos a usar como variables predictoras `Income` `Limit` `Rating` `Age` y regularización de Ridge.\n",
    "\n",
    "Vamos a sumar como variable predictora a `Rating_2` que es el resultado de 2 * `Rating` para ejemplificar el efecto de la regularización sobre un predictor colineal (recordemos los supuestos de colinealidad y regresión lineal)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "¿Cuál es el mejor valor de $\\lambda$ (o $\\alpha$)?\n",
    "\n",
    "¿Cuál es el valor de $R^2$ del modelo en test?\n",
    "\n",
    "Escribamos la ecuación que predice el valor de `Balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/Credit.csv')\n",
    "\n",
    "data['Rating_2'] = 2 * data.Rating\n",
    "\n",
    "X = data[['Income', 'Limit', 'Rating', 'Age', 'Rating_2']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "y = data['Balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18\n",
      "0.8620335336980528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.3, random_state = 117)\n",
    "\n",
    "model_ridge_cv = linear_model.RidgeCV(alphas= [0.3, 0.5, 1.0, 1.1, 1.15, 1.17, 1.18, 1.19, 1.2, 1.21, 1.22, 1.3, 1.4, 1.5, 10.0], \n",
    "                                   fit_intercept=True, normalize=False, cv=10)\n",
    "\n",
    "model_fit_ridge_cv = model_ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(model_fit_ridge_cv.alpha_)\n",
    "\n",
    "print(model_fit_ridge_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-258.48007026  148.8401642   224.7354755   -12.91648578  224.7354755 ]\n",
      "512.1779140289367\n",
      "0.8839840155578678\n"
     ]
    }
   ],
   "source": [
    "best_alpha = model_fit_ridge_cv.alpha_\n",
    "model_ridge = linear_model.Ridge(alpha = best_alpha, fit_intercept = True, normalize = False)\n",
    "model_fit_ridge = model_ridge.fit(X_train, y_train)\n",
    "print(model_fit_ridge.coef_)\n",
    "print(model_fit_ridge.intercept_)\n",
    "print(model_fit_ridge.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8609969065160658"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fit_ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El mejor valor de $\\lambda$ es 1.18\n",
    "\n",
    "El valor de $R^2$ en test es 0.86\n",
    "\n",
    "La ecuación que predice el valor de `Balance` es\n",
    "\n",
    "$$Balance = 512.18 -258.48 . \\tilde{Income} + 148.84 .\\tilde{Limit} +  224.74 . \\tilde{Rating}  -12.92 . \\tilde{Age} + 224.74 . \\tilde{Rating\\_2}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusión\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusiones\n",
    "\n",
    "---\n",
    "\n",
    "* La **regularización nos ayuda a evitar el sobreajuste** limitando la complejidad del modelo\n",
    "\n",
    "* Matemáticamente lo logra **penalizando la complejidad** dentro de la función de costo\n",
    "\n",
    "* Modelos con regularización suelen tener **mayor poder de generalización**\n",
    "\n",
    "* Para determinar el valor de los **hiperparámetros** usados para regularizar, usamos **validación cruzada** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hands-on\n",
    "\n",
    "----\n",
    "\n",
    "**Repitamos el caso del ejemplo usando Lasso:**\n",
    "\n",
    "Queremos predecir el valor de la variable `Balance` en el dataset Credit (https://www.kaggle.com/avikpaul4u/credit-card-balance).\n",
    "\n",
    "Para eso vamos a usar como variables predictoras `Income` `Limit` `Rating` `Age` y regularización de Ridge.\n",
    "\n",
    "Vamos a sumar como variable predictora a `Rating_2` que es el resultado de 2 * `Rating` para ejemplificar el efecto de la regularización sobre un predictor colineal (recordemos los supuestos de colinealidad y regresión lineal)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "¿Cuál es el mejor valor de $\\lambda$ (o $\\alpha$)?\n",
    "\n",
    "¿Cuál es el valor de $R^2$ del modelo en test?\n",
    "\n",
    "Escribamos la ecuación que predice el valor de `Balance`\n",
    "\n",
    "Comparemos los resultados de Lasso con los valores de $R^2$ y los coeficientes obtenidos con un modelo de regresión lineal múltiple sin regularización.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solución\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/Credit.csv')\n",
    "\n",
    "data['Rating_2'] = 2 * data.Rating\n",
    "\n",
    "X = data[['Income', 'Limit', 'Rating', 'Age', 'Rating_2']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "y = data['Balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.8841082428543836\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.3, random_state = 117)\n",
    "\n",
    "model_lasso_cv = linear_model.LassoCV(alphas= [0.05, 0.06, 0.07, 0.08, 0.09, 0.95, 0.97, 0.1, 0.12, 0.15, 0.2, 0.3, 1.0, 10.0], \n",
    "                                   fit_intercept=True, normalize=False, cv=10, max_iter=1e4)\n",
    "\n",
    "model_fit_lasso_cv = model_lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(model_fit_lasso_cv.alpha_)\n",
    "\n",
    "print(model_fit_lasso_cv.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-262.78233322  106.98310457  493.91231515  -12.87524297    1.40889939]\n",
      "512.1739688178859\n",
      "0.860744884074875\n"
     ]
    }
   ],
   "source": [
    "best_alpha = model_fit_lasso_cv.alpha_\n",
    "model_lasso = linear_model.Lasso(alpha = best_alpha, fit_intercept = True, normalize = False)\n",
    "model_fit_lasso = model_lasso.fit(X_train, y_train)\n",
    "print(model_fit_lasso.coef_)\n",
    "print(model_fit_lasso.intercept_)\n",
    "print(model_fit_lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El mejor valor de $\\lambda$ es 0.1\n",
    "\n",
    "El valor de $R^2$ en test es 0.86\n",
    "\n",
    "La ecuación que predice el valor de `Balance` es\n",
    "\n",
    "$$Balance = 512.17 - 262.78 . \\tilde{Income} + 106.98 . \\tilde{Limit}  + 493.91 . \\tilde{Rating} - 12.88 . \\tilde{Age}  + 1.41 . \\tilde{Rating\\_2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparemos los resultados de Lasso con los valores de $R^2$ y los coeficientes obtenidos con un modelo de regresión lineal múltiple sin regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-263.21033849   99.64521564  251.53270497  -13.00094277  251.53270497]\n",
      "512.1631513009786\n",
      "0.8606693539960668\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "\n",
    "model_fit = model.fit(X_train, y_train)\n",
    "print(model_fit.coef_)\n",
    "print(model_fit.intercept_)\n",
    "print(model_fit.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observemos cómo se redujo el valor del coeficiente de `Rating_2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias y Material Adicional\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M3/CLASE_23_Regularizacion/Presentacion/img/M3_CLASE_23_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Referencias y Material Adicional\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.kaggle.com/avikpaul4u/credit-card-balance\" target=\"_blank\">Credit Card Balance</a>\n",
    "\n",
    "<a href=\"https://www.statlearning.com/\" target=\"_blank\">An Introduction to Statistical Learning</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/what-is-regularization-and-how-do-i-use-it-f7008b5a68c6\" target=\"_blank\"> Regularization with Ridge, Lasso, and Elastic Net Regressions</a>\n",
    "\n",
    "<a href=\"https://people.eecs.berkeley.edu/~jrs/189/\" target=\"_blank\">L1 L2 solutions for least squares</a>\n",
    "\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=Xm2C_gTAl8c\" target=\"_blank\">Ridge vs Lasso Regression, Visualized!!!</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/bias-and-variance-in-linear-models-e772546e0c30\" target=\"_blank\">Bias and variance in linear models</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
