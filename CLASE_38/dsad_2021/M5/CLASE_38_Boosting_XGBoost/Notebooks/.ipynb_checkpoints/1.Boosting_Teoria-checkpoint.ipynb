{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:    \n",
    "    args = \"conda-forge\"\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\" {args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "Si la instalación de lightgbm da error, ejecutar en Anaconda Prompt \n",
    "\n",
    "`conda install -c conda-forge lightgbm=2.3.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de Árboles de Decisión - Parte III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_introduccion\"></a> \n",
    "## Introducción\n",
    "Como mencionamos en la parte I, existen modelos de árboles de decisión simples, y árboles de decisión de Ensamble. Vimos también que los primeros, si bien son fáciles de interpretar, no pertenecen al grupo de modelos que ofrecen mayor precisión y que ésto se debía en gran medida a la varianza propia de dichos modelos. Como alternativa superadora surgieron los árboles de decisión por ensamble. Los modelos de Ensamble logran reducir la varianza inherente de los árboles de decisión y han logrados muy buenos resultados en cuanto a la precisión alcanzada, sin embargo ésto es a costa de perder la facilidad de interpretación que poseían los árboles de decisión simples.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_boosting\"></a> \n",
    "### Boosting:\n",
    "Boosting es la tercera técnica de agregación que veremos. La principal diferencia con las técnicas que vimos anteriormente, es que en ellas se entrenaban los modelos independientemente para luego generar un meta-modelo. En el caso de Boosting, se entrenan los modelos de manera secuencial donde cada modelo aprende de los errores del modelo predecesor. \n",
    "\n",
    "Las técnicas de boosting más conocidas son:\n",
    "- ADA Boost\n",
    "- Gradient Boosting\n",
    "- XG Boost\n",
    "\n",
    "Veamos cada una de ellas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) ADA Boost:**\n",
    "\n",
    "El nombre ADA proviene de Adaptative Boosting, que hace referencia a su capacidad de Adaptar la importancia de los predictores asignándole mayor peso a aquellos sobre los que se comete más error. Es importante destacar que mientras en Random Forest mencionamos que a los árboles no se los poda, en el casode Adaboost sucede todo lo contrario: se suelen usar árboles de 1 nodo raíz y 2 nodos hojas. A este tipo de árboles se los conoce como **stump**. Por otro lado, mientras que en Random Forest cada árbol tenía igual voto sobre la predicción final, en el caso de ADA Boost tenemos que los votos de los **stumps** pueden tener más pesos unos que otros.  \n",
    "\n",
    "Tal como mencionamos los métodos de boosting trabajan por definición secuencialmente, con lo cual cada **stump** va a \"aprender\" de la secuencia anterior. Al primer stump se lo va a entrenar con un dataset al cual se le va a asignar los mismos pesos a cada observación (fila). De esta manera si nuestro dataset tiene un total de K observaciones, entonces cada observación tendrá un peso de 1/K siendo todas igual de \"importantes\". \n",
    "\n",
    "Luego se elige el feature que genera la menor entropía o gini y se crea el primer stump con dicho feature. Calculamos ahora la importancia de dicho árbol (o cuanto peso tendrá su voto sobre la predicción del meta-modelo) dependiendo de la cantidad de error que cometió. Este error lo calculamos sumando los pesos de todas las observaciones que fueron mal clasificadas (este valor va a estar dentro del rango entre 0 y 1). \n",
    "- Si es 1 significa que no logró clasificar nada correctamente y por ende tendra muy poco voto en la predicción final.\n",
    "- Si es 0 significa que clasifico todo perfectamente y por ende tendrá mayor voto en la predicción final.\n",
    "\n",
    "A su vez, para asegurarnos que el siguiente stump pueda aprender del precursor, realizamos un ajuste de los pesos de cada observación del dataset. Ahora ya no serán todas las observaciones de igual peso o importancia, sino que se le dará mayor peso a aquellos observaciones que fueron mal clasificadas, y consecuentemente se le restará peso a aquellas que fueron correctamente clasificadas, de modo que siempre la suma total de pesos sea igual a 1.\n",
    "\n",
    "Ahora el segundo Stump va a utilizar weighted Gini index para seleccionar la mejor partición, y a continuación se repiten todos los pasos:\n",
    "- se calcula el error total de este stump para asignarle el peso a su voto.\n",
    "- se vuelven a recalcular los pesos de cada observacion para entrenar al siguiente stump.\n",
    "\n",
    "\n",
    "<img style=\"float: center;\" src=\"img/Adaboost_01.png\"> \n",
    "\n",
    "En este [link](https://www.youtube.com/watch?v=k4G2VCuOMMg) podemos ver el algoritmo en acción.\n",
    "\n",
    "Este algoritmo es muy potente pero tiene desventajas como:\n",
    "\n",
    "* Puede producir overfitting (el peso a los outliers va creciendo)\n",
    "* No es interpretable\n",
    "* No es multiclase (existen variantes como Adaboost.M1 que sí lo son)\n",
    "\n",
    "\n",
    "La predicción del meta-modelo estará luego conformada de la siguiente manera:\n",
    "- Clasificador: \n",
    "    - Voto con pesos\n",
    "    - En sklearn: `AdaBoostClassifier`\n",
    "- Regresión: \n",
    "    - Promedio ponderado\n",
    "    - En sklearn: `AdaBoostRegressor`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Gradient Boost:**\n",
    "\n",
    "Gradient boosting es un método de aprendizaje lento donde los sucesivos modelos de árboles de decisión son entrenados para predecir los residuales del árbol antecesor permitiendo que los resultados de los modelos subsiguientes sean agregados y corrijan los errores promediando las predicciones. Para determinar los parámetros que tendrán cada uno de los árboles de decisión agregados al modelo se utiliza un procedimiento descenso por gradiente que minimizará la función de perdida. De esta forma se van agregando árboles con distintos parámetros de forma tal que la combinación de ellos minimiza la pérdida del modelo y mejora la predicción. \n",
    "\n",
    "La diferencia con adaboost es que ya no pesamos cada punto independientemente, sino que proponemos una función de error cuyo gradiente tenemos que minimizar. El hiperparámetro de Learning Rate ($\\eta$) es un escalar entre 0 < $\\eta$ < 1 que multiplica los residuales para asegurar convergencia. A medida que se reduce el valor de $\\eta$ es recomendable aumentar el número de estimadores N.\n",
    "\n",
    "<img style=\"float: center;\" src=\"img/Gradientboost_01.png\"> \n",
    "\n",
    "La predicción del meta-modelo estará luego conformada de la siguiente manera:\n",
    "\n",
    "$y_{pred} = y_1 + \\eta r_1 + ... +  \\eta r_N$\n",
    "\n",
    "\n",
    "Árboles de decisión con Gradient boosting es uno de los modelos más poderosos y más utilizados para problemas de aprendizaje supervisado. Su principal inconveniente es que requieren un ajuste cuidadoso de los parámetros y puede requerir mucho tiempo de entrenamiento.\n",
    "\n",
    "En sklearn:  \n",
    "- Regresión: `GradientBoostingRegressor`\n",
    "- Clasificador: `GradientBoostingClassifier`\n",
    "\n",
    "La implementación den Scikit-Learn toma los siguientes parámetros:\n",
    "\n",
    "- `base_estimator`: el estimador sobre el cual se va a construir el ensamble. Por efecto, son árboles de decisión.\n",
    "- `n_estimators`: cantidad de estimadores que se van a utilizar\n",
    "- `learning_rate`: un numero bajo de Learning rate asegura convergencia del descenso del gradiente, pero aumenta los tiempos de entrenamiento. Como mencionamos anteriormente, a medida que se reduce el valor de $\\eta$ es recomendable aumentar el número de estimadores N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) XG Boost:**\n",
    "\n",
    "XGBoost significa e**X**treme **G**radient Boosting. Es el algoritmo que ha estado dominando recientemente los problemas Machine learning y las competiciones de Kaggle con datos estructurados. Es un caso especifico de gradient boosting donde se aplica Regularizacion a la funcion de costo lo cual ayuda a evitar overfitting.  \n",
    "\n",
    "Internamente, XGBoost representa todos los problemas como un caso de modelado predictivo de regresión que sólo toma valores numéricos como entrada. Si nuestros datos están en un formato diferente, primero vamos a tener que transformarlos para poder hacer uso de todo el poder de esta librería. El hecho de trabajar sólo con datos numéricos es lo que hace que esta librería sea tan eficiente.\n",
    "\n",
    "Para utilizar XGBoost deben importar la libreria: `import xgboost as xgb`\n",
    "- Regresión: `xgb.XGBRegressor()`\n",
    "- Clasificador: `xgb.XGBClassifier()`\n",
    "\n",
    "Los argumentos que toma como input son ya conocidos:\n",
    "\n",
    "- `learning_rate`: un numero bajo de Learning rate asegura convergencia del descenso del gradiente, pero aumenta los tiempos de entrenamiento. Como mencionamos anteriormente, a medida que se reduce el valor de $\\eta$ es recomendable aumentar el número de estimadores N.\n",
    "- `n_estimators`: cantidad de estimadores que se van a utilizar\n",
    "- `criterion`: define el criterio de impureza para evaluar la calidad de las particiones\n",
    "- `max_features`: la cantidad de features que extraerá para entrenar cada `base_estimator`. Por default es igual a `sqrt(X.shape[1])`\n",
    "- `bootstrap` y `bootstrap_features`: controla si tanto los n_samples como las features son extraidos con reposición.\n",
    "- `max_depth`: la pronfundidad máxima del árbol\n",
    "- `min_samples_leaf`: el número mínimo de n_samples para constituir una hoja del árbol (nodo terminal)\n",
    "- `min_samples_split`: el número mínimo de n_samples para realizar un split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medición de Performance en modelos de clasificación:\n",
    "A continuación aconsejamos ver el siguiente video que explica las Curvas ROC y el AUC como mediciones de performance de los modelos de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AcbbkCL0dlo\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section_referencias_2\"></a> \n",
    "## Referencias \n",
    "\n",
    "[volver a TOC](#section_toc_2)\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/index.html\n",
    "\n",
    "https://relopezbriega.github.io/blog/2017/06/10/boosting-en-machine-learning-con-python/\n",
    "\n",
    "https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
