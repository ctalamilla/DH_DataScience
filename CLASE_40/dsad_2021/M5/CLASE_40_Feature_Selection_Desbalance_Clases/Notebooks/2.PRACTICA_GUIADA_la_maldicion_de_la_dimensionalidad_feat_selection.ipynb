{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica Guiada\n",
    "\n",
    "## La Maldición de la Dimensionalidad y Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_TOC\"></a> \n",
    "\n",
    "#### Tabla de Contenidos\n",
    "[1- Primera parte: La maldicion de la dimensionalidad](#section_maldicion)\n",
    "\n",
    "$\\hspace{.5cm}$[1.1- Ejercicio: proporción de outliers](#section_outliers)\n",
    "\n",
    "$\\hspace{.5cm}$[1.2- Caso de análisis: clasificación](#section_clasificacion)\n",
    "\n",
    "\n",
    "[2- Segunda parte: Feature selection](#section_feature_selection)\n",
    "\n",
    "$\\hspace{.5cm}$[2.1- Filter methods](#section_filter)\n",
    "\n",
    "$\\hspace{1cm}$[2.1.1- Features con baja varianza](#section_varianza)\n",
    "\n",
    "$\\hspace{1cm}$[2.1.2- Select K-Best](#section_kbest)\n",
    "\n",
    "$\\hspace{1cm}$[2.1.3- Select K-Percentile](#section_kpercentile)\n",
    "\n",
    "$\\hspace{.5cm}$[2.2- Wrapper methods](#section_wrapper)\n",
    "\n",
    "$\\hspace{1cm}$[2.2.1- Eliminación Recursiva de Features (RFE)](#section_rfe)\n",
    "\n",
    "$\\hspace{1cm}$[2.2.2- Eliminación Recursiva de Features con cross validation (RFECV)](#section_rfecv)\n",
    "\n",
    "$\\hspace{.5cm}$[2.3- Feature Selection como parte de un pipeline](#section_pipeline)\n",
    "\n",
    "$\\hspace{.5cm}$[2.4- Algoritmos genéticos](#section_genetic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"section_maldicion\"></a> \n",
    "\n",
    "## 1- Primera Parte: La maldición de la dimensionalidad\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "<a id=\"section_outliers\"></a> \n",
    "\n",
    "### 1.1- Proporción de outliers\n",
    "\n",
    "Empecemos con un ejercicio: si tenemos una variable distribuida uniformemente en un hipercubo de $d$ dimensiones, ¿qué propoción de outliers esperamos encontrar? \n",
    "\n",
    "Podemos definir outliers como aquellos puntos que toman valores extremos en alguna de las $d$ dimensiones. \n",
    "\n",
    "Por ejemplo, si pensamos en dos dimensiones, se nos forma un marco cuadrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=500;\n",
    "\n",
    "X=np.random.uniform(size=(N,2)); # generamos 500 puntos en un espacio de dos dimensiones, distribuidos uniformemente\n",
    "\n",
    "plt.plot(X[:,0],X[:,1],'o',ms=2)\n",
    "x = [0.0,1.0]\n",
    "p = 0.01 # Si alguna de las coordenadas está en el percentil 1 o 99 del eje, lo consideramos un outlier\n",
    "\n",
    "plt.fill_between(x,0,p,alpha=0.2,color='k')\n",
    "plt.fill_between(x,1-p,1,alpha=0.2,color='k')\n",
    "plt.fill_betweenx(x,0,p,alpha=0.2,color='k')\n",
    "plt.fill_betweenx(x,1-p,1,alpha=0.2,color='k',label='Region de outliers');\n",
    "plt.legend(loc=(0,1));\n",
    "\n",
    "n_outliers=np.sum(np.sum((X <p) |( X >(1-p)),axis=1));\n",
    "\n",
    "print('Número de outliers:',n_outliers, ' de ',N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, ¿qué pasa a medida que aumentamos la dimensionalidad del dataset? Dado que no podemos graficar en más de tres dimensiones, lo que haremos será graficar la evolución de la proporción de outliers para cada nivel de dimensionalidad.\n",
    "\n",
    "Antes de ejecutar la siguiente celda, piensen qué esperan encontrar.\n",
    "\n",
    "\n",
    "<img src='img/source.gif'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N=1000;\n",
    "p=0.01;\n",
    "Ds=np.arange(1,200);\n",
    "p_outliers=[];\n",
    "for d in Ds:\n",
    "    X=np.random.uniform(size=(N,d));\n",
    "    p_outliers.append(np.mean(np.any((X <p) |( X >(1-p)),axis=1)))\n",
    "\n",
    "plt.plot(Ds, p_outliers);\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('Proporción de outliers')\n",
    "plt.xlabel('Dimensiones');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Qué observan?\n",
    "* ¿Qué sucede con la proporción de outliers a medida que el espacio de predictores se incrementa en dimensionalidad?\n",
    "* ¿Cómo lo explican?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de outliers aumenta porque, al agregar dimensiones, aumenta la probabilidad de que en <b>al menos una dimensión </b> la variable tome un valor extremo.\n",
    "\n",
    "En el caso que vimos recién los datos estaban distribuidos uniformemente. En datos de la vida real probablemente esto no sea así, sino que esperamos que haya alguna estructura en el espacio de las features. Es decir, que haya grupos de datos más parecidos entre sí (clusters), correlación entre algunas features, etc. En cualquier problema de machine learning uno quiere aprender esa estructura de los datos. Si agregamos features, es esperable que los datos se alejen entre sí en el espacio ampliado por las nuevas dimensiones. Separar los puntos en un espacio de mayor dimensionalidad puede servir para revelar una estructura, pero si nos pasamos de rosca puede tener el efecto inverso: que se pierda la estructura en un espacio demasiado grande.\n",
    "\n",
    "Por este motivo la performance de los algoritmos dependerá de la dimensionalidad de los datos, y encontrar la dimensionalidad óptima se puede considerar un problema de optimización de hiperparámetros, abordable por medio de cross validation.\n",
    "\n",
    "Repasemos estos conceptos con un ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_clasificacion\"></a> \n",
    "\n",
    "### 1.2- Caso de análisis: clasificación\n",
    "\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "A continuación vamos a ver qué sucede con la performance de una regresión logística sin regularización a medida que aumenta la cantidad de dimensiones.\n",
    "\n",
    "La regresión deberá predecir datos simulados a través de distribuciones normales multivariadas.\n",
    "\n",
    "* La clase 0 tendrá media 0 y las varianzas de todas las dimensiones valdrán 0.5\n",
    "* La clase 1 tendrá media 1 y las varianzas de todas las dimensiones valdrán 0.5\n",
    "\n",
    "En una y en dos dimensiones, los datos se verían así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=2\n",
    "size0 =int(np.round(0.5*N)) ; size1 = N-size0;\n",
    "loc0 = np.ones(d); loc1 = np.zeros(d);\n",
    "sigma0 = 0.5; sigma1 = 0.5; \n",
    "C0 = sigma0*np.diag(np.ones(d)); C1 = sigma1*np.diag(np.ones(d))\n",
    "loc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.multivariate_normal(loc0,C0,size0)\n",
    "sns.scatterplot(x=X[:,0],y=X[:,1])\n",
    "Y=np.random.multivariate_normal(loc1,C1,size1)\n",
    "sns.scatterplot(x=Y[:,0],y=Y[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dos_multinormales(d,N):\n",
    "    # d es el número de dimensiones, N el número de puntos. Las clases estarán balanceadas.\n",
    "    size0 =int(np.round(0.5*N)) ; size1 = N-size0;\n",
    "    loc0 = np.ones(d); loc1 = np.zeros(d);\n",
    "    sigma0 = 0.5; sigma1 = 0.5; \n",
    "    C0 = sigma0*np.diag(np.ones(d)); C1 = sigma1*np.diag(np.ones(d))\n",
    "    \n",
    "    #Generamos datos multivariados\n",
    "    x0 = np.random.multivariate_normal(loc0,C0,size0);\n",
    "    x1 = np.random.multivariate_normal(loc1,C1,size1);\n",
    "    X=np.concatenate((x0,x1),axis=0)\n",
    "    #Creamos la matriz de features\n",
    "    dfX = pd.DataFrame(X);\n",
    "    dfX.columns = ['dim'+str(i) for i in range(d)];\n",
    "    dfX['class']=0\n",
    "    dfX.loc[x0.shape[0]:,'class']=1\n",
    "    return dfX\n",
    "\n",
    "ax=plt.subplots(nrows=1,ncols=2,figsize=(12,5));\n",
    "\n",
    "\n",
    "\n",
    "dfX=dos_multinormales(1,100);\n",
    "\n",
    "hue_data = dfX['class'].values\n",
    "\n",
    "sns.scatterplot(data=dfX,x='dim0',y=np.zeros(len(dfX)),hue=hue_data,alpha=.7,ax=ax[1][0]);\n",
    "\n",
    "dfX=dos_multinormales(2,100);\n",
    "sns.scatterplot(data=dfX,x='dim0',y='dim1',hue=hue_data,ax=ax[1][1]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión logística sobre datos de distintas dimensiones\n",
    "\n",
    "Generamos una lista de posibles dimensiones y para cada uno de estos valores generamos un dataset simulado con las características descriptas arriba.\n",
    "\n",
    "Sobre estos datos corremos y evaluamos una regresión logística para cada dimensionalidad. La métrica de evaluación será el promedio del score de test, utilizando cross validation con una partición de 5 folds en todos los casos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "score = []\n",
    "# Evaluamos el modelo en todas estas dimensiones\n",
    "ds = [1,2,3,5,10,30,40,50,60,70,120,200,300]\n",
    "N=50;\n",
    "np.random.seed(1)\n",
    "\n",
    "for d  in ds:\n",
    "    dfX=dos_multinormales(d,N);    \n",
    "    X = dfX.drop('class',axis=1)\n",
    "    y = dfX['class']    \n",
    "    # Guardamos la media del score de cross-validation\n",
    "    model = LogisticRegression(C = 1e10,n_jobs=4);\n",
    "    kf = KFold(5, shuffle=True, random_state=0);\n",
    "    score.append(np.mean(cross_val_score(model,X,y,cv=kf)));\n",
    "    \n",
    "plt.plot(ds,score,'o-');\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que a partir de cierta cantidad de dimensiones, la performance empieza a caer fuertemente debido al exceso de dimensiones en relación a la cantidad de datos disponibles. Si el volumen de datos aumenta, la cantidad óptima de dimensiones también aumenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "# Evaluamos el modelo en todas estas dimensiones\n",
    "ds = [1,2,3,5,10,30,40,50,60,70,120,200,300]\n",
    "N=200;\n",
    "np.random.seed(1)\n",
    "for d  in ds:\n",
    "    dfX=dos_multinormales(d,N);    \n",
    "    X = dfX.drop('class',axis=1)\n",
    "    y = dfX['class']    \n",
    "    # Guardamos la media del score de cross-validation\n",
    "    model = LogisticRegression(C = 1e10,n_jobs=4);\n",
    "    kf = KFold(5, shuffle=True, random_state=0);\n",
    "    score.append(np.mean(cross_val_score(model,X,y,cv=kf)));\n",
    "    \n",
    "plt.plot(ds,score,'o-');\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"section_feature_selection\"></a> \n",
    "\n",
    "## 2- Segunda Parte: Feature Selection\n",
    "\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "Por lo discutido hasta aquí, dado que los datos disponibles para entrenar nuestros modelos son finitos, muchas veces convendrá reducir la dimensionalidad de los mismos para evitar el overfitting y optimizar la performance del modelo. En el ejemplo anterior, el problema se reducía a agregar o quitar dimensiones independientes e igualmente relevantes entre sí, dado que estábamos generando datos multinormales con idéntica varianza y media en todas las dimensiones. En un dataset real, sin embargo, no esperamos que todas las dimensiones (features) sean igualmente relevantes. \n",
    "\n",
    "¿Cómo hacemos entonces para decidir cuál subset de features optimiza la performance de nuestro modelo?\n",
    "\n",
    "Un abordaje de fuerza bruta a esta pregunta sería probar todas las combinaciones posibles de features y elegir la que de un mejor score en cross validation. El problema es que si tenemos n features, el número de subsets posibles escala como $2^n$ y rápidamente se vuelve impracticable el cómputo. Por otro lado, cuando el número de modelos que estamos comparando es muy grande para el tamaño de nuestro dataset, corremos el riesgo de hacer overfitting aún cuando estemos haciendo cross validation.\n",
    "\n",
    "Veremos a continuación algunas técnicas alternativas para hacer selección de features. Para ello veremos algunos métodos implementados en Scikit-Learn. Pueden ahondar en sus variantes en la [documentación oficial del Scikit-Learn dedicada al tema](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection).\n",
    "\n",
    "Existen diferentes \"familias\" de técnicas de feature selection. Veamos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wK-A8_HXdvTr"
   },
   "source": [
    "\n",
    "<a id=\"section_filter\"></a> \n",
    "\n",
    "### 2.1 Filter Methods\n",
    "\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "Esta familia de métodos consiste en seleccionar features por sus propiedades estadísticas y su relación con la variable objetivo. Veremos a continuación algunos ejemplus univariados, es decir que no contemplan la covarianza entre features.\n",
    "\n",
    "\n",
    "<a id=\"section_varianza\"></a> \n",
    "#### 2.1.1 Features con baja varianza\n",
    "\n",
    "Un primer caso (casi trivial) sería la remoción de aquellos features o características que no aporten \"información\" a nuestro dataset. Una forma de lograr la información es a través de la varianza. \n",
    "\n",
    "Entonces, lo que haremos será remover todos aquellos features que tengan baja varianza (es decir, baja variabilidad). Esto haremos definiendo un `threshold` por debajo del cual eliminaremos los features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6qlaKSr6dvTt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import random\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "83AWFa4KdvTz"
   },
   "outputs": [],
   "source": [
    "# Importamos los datasets Iris y Boston, que ya conocemos:\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "boston = datasets.load_boston()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empecemos a trabajar con el dataset Iris. Creamos un DataFrame con los datos:\n",
    "\n",
    "df1 = pd.DataFrame(np.concatenate((iris.data, iris.target.reshape(-1,1)),1),\n",
    "                  columns = iris.feature_names + ['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CL2fjVZ8dvT5"
   },
   "source": [
    "* Si aplicásemos un `threshold = 0.5` la única variable que debería ser excluida sería `sepal_width`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ud7e8YjFdvT5"
   },
   "outputs": [],
   "source": [
    "df1.apply(np.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WQJ4aiyodvT_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Instanciamos la clase VarianceThreshold definiendo un threshold=0.5\n",
    "\n",
    "fet_sel = VarianceThreshold(threshold=0.5)\n",
    "\n",
    "# Fiteamos a fet_sel con nuestro df1:\n",
    "\n",
    "fet_sel.fit(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXOE60fhdvUD"
   },
   "source": [
    "* Usando el método `get_support` podemos consultar un array booleano que marca las variables que no han sido excluidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gYWwDl2NdvUF"
   },
   "outputs": [],
   "source": [
    "fet_sel.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SymDbDskdvUK"
   },
   "outputs": [],
   "source": [
    "# Podemos filtrar las columnas que no superen el umbral de varianza:\n",
    "\n",
    "df1.columns[fet_sel.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaOAcVSCdvUO"
   },
   "source": [
    "* Si quisiéramos generar un nuevo array con los datos, simplemente podemos llamar al método `.transform(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BtolyYwgdvUQ"
   },
   "outputs": [],
   "source": [
    "df1_reduced = pd.DataFrame(fet_sel.transform(df1), columns = df1.columns[fet_sel.get_support()])\n",
    "df1_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFnLX4jXdvUU"
   },
   "source": [
    "\n",
    "<a id=\"section_kbest\"></a> \n",
    "#### 2.1.2 Select K-best\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "Este método toma dos argumentos:\n",
    "\n",
    "* `score_func`: una función que devuelve algún score entre $X$ y $Y$:\n",
    "\n",
    "    * Para regresión: f_regression, mutual_info_regression\n",
    "    * Para clasificación: f_classif, mutual_info_classif\n",
    "\n",
    "\n",
    "* `k`: la cantidad de \"mejores\" features que serán seleccionadas\n",
    "\n",
    "\n",
    "#### `F- Test`\n",
    "\n",
    "\n",
    "Tanto f_regression como f_classif se basan en un test estadístico llamado F-test. \n",
    "\n",
    "En el caso de <b>clasificación</b>, la hipótesis nula del test es que agrupar los datos por categorías no permite explicar una fracción significativa de la varianza de los datos, por este motivo al test se lo conoce como análisis de varianza (ANOVA). El test funciona de esta manera: dados los datos, se construye un estadístico llamado F de la siguiente forma\n",
    "\n",
    "\\begin{align}\n",
    "F=&\\frac{\\text{explained variance}}{\\text{unexplained variance}} = \\frac{\\text{between group variability}}{\\text{within group variability}}\\\\\n",
    "    &=\\frac{ \\frac{1}{K-1}\\sum_{i=1}^K n_i (\\bar{X}_i-\\bar{X})^2 }{\\frac{1}{N-K}\\sum_{i=1}^K \\sum_{j=1}^{n_i} (X_{ij}-\\bar{X}_i)^2}\n",
    "\\end{align}\n",
    "\n",
    "en donde $K$ es el número de grupos (clases),$N$ es el tamaño de la muestra, $n_i$ es la cantidad de observaciones dentro de la clase i-ésima, $\\bar{X}$ es el valor medio de toda la muestra y $\\bar{X}_i$ es el promedio dentro de la clase i-ésima.\n",
    "\n",
    "Bajo la hipótesis nula, si además se cumplen los requisitos de normalidad y homosedasticidad (igual varianza entre los grupos), entonces el estadístico F sigue una distribución conocida (F-distribution). De esta forma si F se aparta mucho del valor esperado dada esa distribución, podemos rechazar la hipótesis nula. El p-valor es un estadístico que indica cuán probable es obtener un valor de F si la hipótesis nula fuera correcta. \n",
    "\n",
    "Al hacer el test entonces obtenemos por resultado un valor de F y un p-valor, mientras mayor sea F y consecuentemente menor sea el p-valor, podremos rechazar la hipótesis nula con mayor confianza.\n",
    "\n",
    "En el contexto de feature selection, lo que hacemos es un test para cada feature y obtener el score F y el pvalor. Es decir, cuánto sirven las clases para explicar la varianza que se observa en la feature X. Luego se rankean las features en función del score F.\n",
    "\n",
    "En el caso de <b>regresión</b>, la idea es parecida. Uno quiere rankear las features según cuánto sirven para predecir la variable target. En la versión más simple, lo que se hace es un test estadístico en donde se compara la performance de un modelo lineal que incluye a X como variable regresora, respecto de un modelo que sólo tiene intercepto.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Modelo 0:}\\quad Y&=\\beta_0 \\\\\n",
    "\\text{Modelo 1:} \\quad Y&=\\beta_0+\\beta_1 X\n",
    "\\end{align}\n",
    "\n",
    "Al ajustar cada modelo, obtenemos una medida de performance a partir de la suma de errores cuadráticos:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{RSS}_0 =& \\sum_{i=1}^N (Y_i - \\hat{Y}_{i0})^2 \\\\\n",
    "\\text{RSS}_1 =& \\sum_{i=1}^N (Y_i - \\hat{Y}_{i1})^2\n",
    "\\end{align}\n",
    "\n",
    "en donde $\\hat{Y}_{0}$ es la predicción del modelo 0 para el dato i-ésimo. El estadístico F se construye como\n",
    "\n",
    "\\begin{equation}\n",
    "F= \\frac{N-p_1}{p_1-p_0}\\frac{ RSS_0 - RSS_1 }{ RSS_1 }\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "en donde $p_0$ y $p_1$ son el número de parámetros en cada modelo (en este caso 1 y 2). La hipótesis nula del test supone que agregar el parámetro $\\beta1$ no mejora significativamente la performance. Nuevamente, mientras mayor es F (más diferencia hay entre $RSS_0$ y $RSS_1$ y podremos rechazar la hipótesis nula con más confianza.\n",
    "\n",
    "Es importante notar que estos tests capturan la dependencia lineal entre una variable target y cada uno de los regresores posibles. Esto es así tanto en el caso de regresión como en el de clasificación. Sin embargo, puede ser que una feature sea relevante por tener una dependencia no lineal con la variable target. Las métricas basadas en información mutua son una opción para capturar esas no linealidades.\n",
    "\n",
    "#### Mutual information\n",
    "\n",
    "En este caso el score no tiene que ver con un test estadístico, sino que se calcula directamente una métrica de dependencia estadística entre cada feature y la variable target, llamada información mutua. No ahondaremos aquí en la definición matemática, pero para dar una idea intuitiva, la información mutua entre dos variables estadísticamente independientes es cero. Por el contrario mientras mayor sea la IM, mayor dependencia entre las variables.\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Para facilitar el desarrollo hemos generado una función que ejecuta los métodos para regresion, extrae los atributos e imprime los resultados. \n",
    "\n",
    "* La función toma un dataframe como input\n",
    "* Luego instancia el método `SelectKBest` con los parámetro `score_func=f_regression` y `k=2`\n",
    "* Realiza el `.fit` del método\n",
    "* Guarda los resultados (`scores`, `pvalues`, `get_support` y las columnas) en un dataframe y lo retorna. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GZ48vN81dvUX"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression\n",
    "\n",
    "df2 = pd.DataFrame(np.concatenate((boston.data, boston.target.reshape(-1,1)),1),\n",
    "                  columns = list(boston.feature_names) + ['price'])\n",
    "\n",
    "def select_kbest_reg(data_frame, target, k=2):\n",
    "    \"\"\"\n",
    "    Seleccionado K-Best features para regresión\n",
    "    :param data_frame: Un dataframe con datos\n",
    "    :param target: target en el dataframe\n",
    "    :param k: cantidad deseada de features\n",
    "    :devuelve un dataframe llamado feature_scores con los scores para cada feature\n",
    "    \"\"\"\n",
    "    feat_selector = SelectKBest(f_regression, k=k)    \n",
    "    _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n",
    "    \n",
    "    feat_scores = pd.DataFrame()\n",
    "    feat_scores[\"F Score\"] = feat_selector.scores_\n",
    "    feat_scores[\"P Value\"] = feat_selector.pvalues_\n",
    "    feat_scores[\"Support\"] = feat_selector.get_support()\n",
    "    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n",
    "    \n",
    "    return feat_scores\n",
    "\n",
    "kbest_feat = select_kbest_reg(df2, \"price\", k=4)\n",
    "kbest_feat = kbest_feat.sort_values([\"F Score\", \"P Value\"], ascending=[False, False])\n",
    "kbest_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xh93hk0odvUc"
   },
   "source": [
    "* Puede verse entonces que si quisiéramos seleccionar las mejores 4 variables estas serían `LSTAT`,`RM`, `PTRATIO` e `INDUS`.\n",
    "\n",
    "* Hasta aquí solamente ordenamos e imprimimos los resultados del `SelectKBest` ¿Cómo podríamos hacer para realizar efectivamente la selección?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EuGvA8hhdvUe"
   },
   "outputs": [],
   "source": [
    "select = kbest_feat.loc[kbest_feat['Support'] == True,'Attribute']\n",
    "df2_reduced = df2[select]\n",
    "df2_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWxN6Z5odvUi"
   },
   "source": [
    "* Si quisiéramos trabajar con un problema de clasificación podríamos simplemente cambiar el `score_func` a alguna función de scoring adecuada: `f_classif`, `mutual_info_classif`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lL_7ojUVdvUj"
   },
   "source": [
    "<a id=\"section_kpercentile\"></a> \n",
    "#### 2.1.3 Select K-percentile\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "Este método selecciona features en base a los percentiles (definidos por el usuario) de los scores máximos. Fíjense que el ranking de features es el mismo que en el ejemplo anterior, pero seleccionamos sólo el mejor 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "T29lVT5YdvUk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "def select_percentile(data_frame, target, percentile=15):\n",
    "\n",
    "    feat_selector = SelectPercentile(f_regression, percentile=percentile)\n",
    "    _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n",
    "    \n",
    "    feat_scores = pd.DataFrame()\n",
    "    feat_scores[\"F Score\"] = feat_selector.scores_\n",
    "    feat_scores[\"P Value\"] = feat_selector.pvalues_\n",
    "    feat_scores[\"Support\"] = feat_selector.get_support()\n",
    "    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n",
    "    \n",
    "    return feat_scores\n",
    "\n",
    "per_feat = select_percentile(df2, \"price\", percentile=20)\n",
    "per_feat = per_feat.sort_values([\"F Score\", \"P Value\"], ascending=[False, False])\n",
    "per_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHXm1ChidvUu"
   },
   "source": [
    "\n",
    "<a id=\"section_wrapper\"></a> \n",
    "\n",
    "### 2.2- Wrapper Methods\n",
    "\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "<a id=\"section_rfe\"></a> \n",
    "#### 2.2.1 Eliminación Recursiva de Features (RFE)\n",
    "\n",
    "Este método toma como input un estimador externo para tratar de cuantificar el peso (la importancia) de cada feature. El método va eliminando features sucesivamente para ir quedándose con sets cada vez más pequeño de features.\n",
    "\n",
    "En primera instancia, el método entrena el estimador sobre el total de features y se cuantifica la importancia de cada feature a través de algún atributo del tipo `coef_` o `feature_importance`. El feature menos importante  se elimina del set y se vuelve a entrenar con los restantes. El proceso se repite de forma recursiva hasta que se llega al número de features definido previamente.\n",
    "\n",
    "El método `RFE()` toma los siguientes argumentos:\n",
    "\n",
    "* `estimator`: el estimador sobre el cual se va a realizar la selección (podría ser cualquier método supervisado que tenga un `.fit` que provea como importancia de cada feature un método `coef_` o `feature_importance`)\n",
    "* `n_features_to_select`: la cantidad de features que se busca seleccionar\n",
    "* `steps`: la cantidad de features que se elimina en cada iteración (puede pasarse en relativo o en absoluto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2.drop('price',axis = 1),df2['price'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "    \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "estimator = LinearRegression()\n",
    "selector = RFE(estimator, n_features_to_select=5, step = 1)\n",
    "_ = selector.fit(X_train, y_train)\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "scores[\"Attribute Name\"] = df2.drop('price',axis = 1).columns\n",
    "scores[\"Ranking\"] = selector.ranking_\n",
    "scores[\"Support\"] = selector.support_\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXu_okzedvU9"
   },
   "source": [
    "* Ahora bien... ¿cuántos features tenemos que seleccionar? Ese es un problema que podemos resolver con... CrossValidation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLR0Qa1fdvU_"
   },
   "source": [
    "<a id=\"section_rfecv\"></a> \n",
    "#### 2.2.2 Eliminación Recursiva de Features con Cross Validation (RFECV)\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjWh4zLSdvVA"
   },
   "source": [
    "Por suerte, hay un método que nos permite seleccionar mediante CrossValidation la cantidad de features a retener.\n",
    "\n",
    "El método `RFECV()` toma los siguientes argumentos:\n",
    "   * `estimator`: análogo a `RFE`, un estimador de aprendizaje supervisado con un atributo `coef_` o `feature_selection` \n",
    "   * `steps`: la cantidad de features que se elimina en cada iteración (puede pasarse en relativo o en absoluto).\n",
    "   * `cv`: determina el método de valdiación cruzada. Puede pasarse un iterador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "O-7OlAPjdvVC"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lbMsTVUpdvVI"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "estimator = LinearRegression()\n",
    "selector = RFECV(estimator, step = 1, cv=kf, scoring = 'neg_mean_squared_error', verbose=2)\n",
    "selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaVWfvRddvVN"
   },
   "source": [
    "* Podemos conocer cuáles son las variables seleccionadas luego del proceso de validación cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Iii7_flmdvVP"
   },
   "outputs": [],
   "source": [
    "df2.drop('price', axis=1).loc[:,selector.support_].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kro7FlEdvVY"
   },
   "source": [
    "<a id=\"section_pipeline\"></a> \n",
    "\n",
    "### 2.3- Feature Selection como parte de un pipeline\n",
    "\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "El RFECV permite hacer cross validation optimizando el número de features. Si tuviéramos que optimizar otros hiperparámetros del modelo, podríamos meter todo en un pipeline. \n",
    "\n",
    "Por ejemplo, incluyamos el hiperparámetro fit_intercept en la optimización de una regresión lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oKgM29mhdvVa"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "i72zKME6dvVe"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle = True)\n",
    "estim = LinearRegression()\n",
    "select = RFE(estim, step = 1, verbose = 1)\n",
    "\n",
    "pipe = Pipeline([\n",
    "  ('feat_sel', select),\n",
    "  ('reg', estim)])\n",
    "\n",
    "param_grid = {'feat_sel__n_features_to_select' : np.arange(1,len(df2.columns)),'reg__fit_intercept':[True,False] }\n",
    "\n",
    "gridcv = GridSearchCV(pipe, param_grid, cv=kf, verbose=1, n_jobs = 3)\n",
    "\n",
    "gridcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el mejor modelo incluye el intercepto y los 13 features como regresores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nyMomuoNdvVn"
   },
   "outputs": [],
   "source": [
    "gridcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos cuán bien performa el modelo seleccionado en el set de testeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = gridcv.predict(X_test)\n",
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_genetic\"></a> \n",
    "\n",
    "### 2.4- Algoritmos genéticos\n",
    "\n",
    "[Volver al índice](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos genéticos son heurísticas de búsqueda, inspiradas en el proceso de “selección natural”.\n",
    "\n",
    "En el contexto de  feature selection, uno podría pensar que cada feature puede estar o no presente, de modo que dados N features, un modelo es una secuencia de N 0's y 1's, que llameremos <b>genes</b>. A un modelo, es decir una secuencia de genes, lo llamaremos <b>cromosoma</b>. \n",
    "\n",
    "<img src=\"img/genetic_algorithm.png\" width=450>\n",
    "\n",
    "El algoritmo funciona generando una <b>población</b> de modelos. Cada modelo tiene un fitness-score asociado, que es una medida de performance: puede ser accuracy para modelos de clasificación, $r^2$ para regresión, etc. La idea es \"aparear\" los mejores modelos, generando una descendencia que es una combinación de los genes de sus progenitores y volver a seleccionarlos de acuerdo a su fitness iterativamente. \n",
    "\n",
    "Aquí presentamos una implementación simple del algoritmo. El código fuente se encuentra en \n",
    "[acá](https://github.com/dawidkopczyk/genetic/blob/master/genetic.py).\n",
    "\n",
    "La implementación que presentamos aquí define la clase GeneticSelector que tiene los siguientes parámetros:\n",
    "\n",
    "* estimator: el estimador que va a usar para medir fitness. Por ejemplo LinearRegression.\n",
    "* n_gen: el número de generaciones que se van a generar (número de iteraciones del algoritmo)\n",
    "* size: tamaño de la población\n",
    "* n_best: en cada iteración se seleccionan los n_best cromosomas para actualizar la población.\n",
    "* n_rand: además de los n_best, se seleccionan n_rand cromosomas al azar.\n",
    "* n_children: número de nuevos cromosomas que se generan en cada apareamiento.\n",
    "* mutation_rate: probabilidad de cambiar cada gen espontáneamente.\n",
    "* cv: default 5, argumento para la función cross_val_score de sklearn.\n",
    "* scoring: medida de fitness de los cromosomas. Por ejemplo \"neg_mean_squared_error\" para modelos de regresión, o \"accuracy\" para clasificación. Dentro de la clase se multiplica la medida por -1 convirtiéndola en una medida de error. \n",
    "\n",
    "\n",
    "En esta implementación, el proceso es el siguiente:\n",
    "\n",
    "* se seleccionan los n_best cromosomas y se los combina otros n_rand elegidos al azar.\n",
    "* se aparean en parejas elegidas al azar. El apareamiento de dos cromosomas c1 y c2 genera un nuevo cromosoma c3 de la siguiente forma: se instancia c3=c1 y con probabilidad 0.5 se cambia cada gen por el correspondiente de c2.\n",
    "* se producen mutaciones aleatorias en todos los cromosomas: con probabilidad baja (mutation_rate) cada gen puede cambiar de valor.\n",
    "* se repite lo anterior un número (n_gen) de veces.\n",
    "\n",
    "\n",
    "Veamos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Genetic import GeneticSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# CV MSE before feature selection\n",
    "#==============================================================================\n",
    "est = LinearRegression()\n",
    "score = -1.0 * cross_val_score(est, X_train, y_train, cv=5,scoring='neg_mean_squared_error')\n",
    "print(\"CV MSE before feature selection: {:.2f}\".format(np.mean(score)))\n",
    "\n",
    "sel = GeneticSelector(estimator=LinearRegression(), \n",
    "                      n_gen=15, size=200, n_best=40, n_rand=40, \n",
    "                      n_children=5, mutation_rate=0.05)\n",
    "\n",
    "sel.fit(X_train, y_train)\n",
    "sel.plot_scores()\n",
    "score = -1.0 * cross_val_score(est, X_train[:,sel.support_], y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"CV MSE after feature selection: {:.2f}\".format(np.mean(score)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico muestra el score promedio de la población y el del mejor cromosoma a lo largo de las generaciones. Recuerden que el score está convertido a una medida de error, por eso buscamos minimizarlo.\n",
    "\n",
    "Podemos ver cuáles fueron las features seleccionadas usando el atributo sel.support del selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features Seleccionadas:')\n",
    "\n",
    "print(boston.feature_names[sel.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
