{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_portada.jpg\" align=\"center\" width=\"70%\"/></div>\n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 5</div>\n",
    "            <div class=\"div-contenido\">Clases desbalanceadas / Feature Selection\n",
    "\n",
    "</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- Clases desbalanceadas. Qué son. Métodos para resolverlo: Undersampling. Oversampling. Class weighting.\n",
    "    \n",
    "- La maldición de la dimensionalidad. Alta dimensionalidad.\n",
    "\n",
    "- Feature selection. Features con baja varianza. Filter Methods.  Wrapper Methods. \n",
    "\n",
    "- Algoritmos genéticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Clases desbalanceadas\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "\n",
    "En los problemas de clasificación a veces encontramos datasets donde alguna de sus clases es  **minoritaria**, porque disponemos pocos casos. \n",
    "\n",
    "Esto provoca un **desbalanceo en las clases** (*imbalanced data*), no siempre detectado por los modelos, que tratan de predecir correctamente a las clases mayoritarias.\n",
    "\n",
    "Algunos ejemplos se encuentran en el diagnóstico de enfermedades donde solemos encontrar miles de registros con *pacientes negativos* y unos pocos *casos positivos*.\n",
    "\n",
    "En los modelos de detección de fraude tenemos muchas muestras de clientes *honestos* y pocos casos etiquetados como *fraudulentos*, justamente los que necesitamos predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Supongamos que entrenamos un modelo de clasificación con 990 imágenes de gatitos y sólo 10 de perros.\n",
    "\n",
    "Lo más probable que la red se limite a *responder siempre “tu foto es un gato”*, pues logra **un acierto del 99%** en su fase de entrenamiento.\n",
    "\n",
    "Lo podemos graficar en la matriz de confusión que generaría el modelo.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_001_matrix.JPG\" alt=\"matrix\" width=50% height=40% />\n",
    "\n",
    "<p style=\"font-size:70%;\">https://www.aprendemachinelearning.com/clasificacion-con-datos-desbalanceados/</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "\n",
    "Veremos tres técnicas para combatir el desbalance de las clases:\n",
    "\n",
    "- Hacer un resampling: \n",
    "\n",
    "    - Aumentando los casos de la clase minoritaria. **Oversampling**.\n",
    "    \n",
    "    - Descartando casos de la clase mayoritaria. **Undersampling**.\n",
    "\n",
    "- Equilibrar la clase minoritaria dandole un peso mayor. **Class weighting**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_002_resampling.png\" alt=\"resampling\" width=55% height=40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Dataset\n",
    "\n",
    "---\n",
    "Vamos a trabajar con datos de incumplimientos de los clientes de tarjetas de crédito.\n",
    "\n",
    "Las **observaciones** son *clientes* de tarjetas de crédito de Taiwán.\n",
    "\n",
    "La **clase** representa si la persona está en incumplimiento (1) o no (0).\n",
    "\n",
    "Para consultar detalles del dataset y sus atributos ver <a href=\"https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\">aquí</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2PHSRoPK8OK",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,plot_confusion_matrix,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ylvmz7hrK8OR",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_original = pd.read_excel('../Data/default_credit_card_clients.xls')\n",
    "df_original.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYvvW3HlK8OW",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Convertimos las variables categóricas en variables dummies o indicadoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df_original,columns=['EDUCATION','MARRIAGE'],drop_first=True);\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYvvW3HlK8OW",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Se observa que las clases están **desbalanceadas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlIfUosgK8OY",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pd.value_counts(df['Class'], sort = True, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puQ9YKZCK8OH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelo\n",
    "\n",
    "---\n",
    "Vamos a aplicar en todos los casos un modelo de Regresión Logística, y mediremos su performance.\n",
    "\n",
    "Preparamos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state = 123)\n",
    "\n",
    "scaler=StandardScaler()  \n",
    "X_train_sc=scaler.fit_transform(X_train) # Estandarizamos los datos     \n",
    "X_test_sc=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# definimos una función que crea el modelo que usaremos cada vez\n",
    "def run_model(X_train, y_train):\n",
    "    clf_base = LogisticRegression(penalty='none',random_state=123,max_iter=500)\n",
    "    clf_base.fit(X_train, y_train)\n",
    "    return clf_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# definimos una función para mostrar los resultados\n",
    "def mostrar_resultados(y_test, pred_y):\n",
    "    conf_matrix = confusion_matrix(y_test, pred_y); \n",
    "    \n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.heatmap(conf_matrix,  annot=True, fmt=\"d\");\n",
    "    plt.title(\"Confusion matrix\"); plt.ylabel('True class'); plt.xlabel('Predicted class')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"METRICS\")\n",
    "    print (classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puQ9YKZCK8OH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelo sin modificar el dataset\n",
    "\n",
    "---\n",
    "Apliquemos el modelo sobre el dataset original.\n",
    "\n",
    "Observamos un *aceptable accuracy (0.85)*. Pero los valores de *precision y recall* nos muestran que el modelo es *bueno prediciendo la clase mayoritaria*, pero **muy malo para la clase minoritaria**. \n",
    "\n",
    "La matriz de confusión también nos indica un *valor alto de casos mal clasificados para la clase 1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model_original = run_model(X_train_sc, y_train)\n",
    "y_pred = model_original.predict(X_test_sc)\n",
    "mostrar_resultados(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Undersampling\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler\">imbalanced-learn</a> es un módulo Python que ayuda a balancear los datasets.\n",
    "\n",
    "Veamos primero como realiza <span style='color: #FF0000;'>undersampling</span>, es decir, como genera un *subset de datos de **train** con clases balanceadas*, **descartando casos de la clase mayoritaria**. \n",
    "\n",
    "El método <a href=\"https://imbalanced-learn.org/stable/under_sampling.html\">RandomUnderSampler</a> toma observaciones de la clase mayoritaria al azar, con o sin reposición.\n",
    "\n",
    "Tiene un hiperparámetro `sampling_strategy` que toma los valores:\n",
    "\n",
    "* 'minority': hacer resample hasta balancear las clases.\n",
    "* número entre 0 y 1: ratio entre las clases mayoritaria y minoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersampler=RandomUnderSampler(sampling_strategy='majority',random_state=123); # iguala las clases\n",
    "\n",
    "X_train_us,y_train_us=undersampler.fit_resample(X_train,y_train);\n",
    "\n",
    "scaler=StandardScaler()  \n",
    "X_train_us_sc=scaler.fit_transform(X_train_us) # Estandarizamos los datos     \n",
    "X_test_us_sc=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observemos que el nuevo train set, al usar `sampling_strategy='majority'`, tiene **la misma cantidad de observaciones por clase**.\n",
    "\n",
    "Disminuyó la cantidad de observaciones en la clase mayoritaria, hasta obtener una cantidad igual al total original de la clase minoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('NUEVA Composición del training set:\\n', y_train_us.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('ANTERIOR Composición del training set:\\n', y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelo aplicando Undersampling\n",
    "\n",
    "---\n",
    "Apliquemos el modelo sobre este nuevo dataset.\n",
    "\n",
    "Observamos un descenso de *accuracy* desde 0.85 a 0.69, pues al **reducir fuertemente la cantidad de casos de la clase mayoritaria**, desmejora la predicción para esta clase. La baja en el *recall* de la clase mayoritaria también nos indica este problema.\n",
    "\n",
    "Por el contrario, la clase minoritaria (1) aumentó el *recall* a 0.66, pues predice mejor a los casos reales. Sin embargo, *precision* bajó de 0.69 a 0.30, mostrando un alto número de falsos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model_us = run_model(X_train_us_sc, y_train_us)\n",
    "y_pred = model_us.predict(X_test_us_sc)\n",
    "mostrar_resultados(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Oversampling\n",
    "\n",
    "---\n",
    "\n",
    "Ahora probamos lo contrario, <span style='color: #FF0000;'>oversampling</span>, que significa generar un *subset de datos de **train** con clases balanceadas*, **aumentando los casos de la clase minoritaria**. \n",
    "\n",
    "El método <a href=\"https://imbalanced-learn.org/stable/over_sampling.html\">RandomOverSampler</a> crea observaciones de la clase minoritaria con reposición.\n",
    "\n",
    "Con el hiperparámetro `sampling_strategy` realizamos lo mismo que antes con undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversampler=RandomOverSampler(sampling_strategy='minority',random_state=123); # iguala las clases\n",
    "\n",
    "X_train_os,y_train_os=oversampler.fit_resample(X_train,y_train);\n",
    "\n",
    "scaler=StandardScaler()  \n",
    "X_train_os_sc=scaler.fit_transform(X_train_os) # Estandarizamos los datos     \n",
    "X_test_os_sc=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observemos que el nuevo train set, al usar `sampling_strategy='majority'`, nuevamente tiene **la misma cantidad de observaciones por clase**.\n",
    "\n",
    "Pero ahora igualó la cantidad de la clase mayoritaria con casos repetidos de la clase minoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('NUEVA Composición del training set:\\n', y_train_os.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('ANTERIOR Composición del training set:\\n', y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelo aplicando Oversampling\n",
    "\n",
    "---\n",
    "Aplicando el modelo sobre este nuevo dataset, vemos que la performance con oversampling es prácticamente igual a la obtenida con el dataset generado con *undersampling*.\n",
    "\n",
    "No nos olvidemos que *podemos jugar con valores entre 0 y 1* sobre el parámetro `sampling_strategy` para mejorar la performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model_os = run_model(X_train_os_sc, y_train_os)\n",
    "y_pred = model_os.predict(X_test_os_sc)\n",
    "mostrar_resultados(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckQn2V47K8P3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Oversampling - SMOTE\n",
    "\n",
    "---\n",
    "El algoritmo <a href=\"https://imbalanced-learn.org/stable/references/over_sampling.html#smote-algorithms\">SMOTE</a> (Synthetic Minority Oversample Technique) realiza oversampling, generando **muestras simuladas de la clase minoritaria**.\n",
    "\n",
    "La idea es considerar de a pares casos de la clase minoritaria, y generar aleatoriamente un punto que se encuentre en el segmento que los une.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_003_smote.png\" alt=\"smote\" width=55% height=40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckQn2V47K8P3",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pero *SMOTE* solo funciona con **variables continuas**.\n",
    "\n",
    "Por suerte, existe el método <a href=\"https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html\">SMOTENC</a> (NC por Nominal and Continuous) que trabaja con **features continuas y categóricas**.\n",
    "\n",
    "Tenemos que informar las variables categóricas como una lista con la posición de cada feature en el dataset, en el parámetro `categorical_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "smote=SMOTENC(categorical_features=[1, 2, 4, 5, 6, 7, 8, 9],sampling_strategy='minority',random_state=123);\n",
    "\n",
    "X_train_sm,y_train_sm = smote.fit_resample(X_train,y_train);\n",
    "\n",
    "scaler=StandardScaler()  \n",
    "X_train_sm_sc=scaler.fit_transform(X_train_sm) # Estandarizamos los datos     \n",
    "X_test_sm_sc=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observemos que el nuevo train set tiene la misma cantidad de observaciones por clase.\n",
    "\n",
    "Pero ahora igualó la cantidad de la clase mayoritaria con **casos inventados de la clase minoritaria**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('NUEVA Composición del training set:\\n', y_train_sm.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelo aplicando SMOTENC\n",
    "\n",
    "---\n",
    "En este modelo, la performance sigue prácticamente igual a las obtenidas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model_sm = run_model(X_train_sm_sc, y_train_sm)\n",
    "y_pred = model_sm.predict(X_test_sm_sc)\n",
    "mostrar_resultados(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBiqvT9QK8QQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Class weighting\n",
    "\n",
    "---\n",
    "\n",
    "Otra técnica que se encuentra en muchos algoritmos de machine learning es **penalizar o darle un peso a cada label de la clase**.\n",
    "\n",
    "Se puede usar para imponer un costo al modelo cuando falla al clasificar la clase minoritaria, balanceando las clases.\n",
    "\n",
    "La regresión logística admite el hiperparámetro `class_weight`, un diccionario donde se definen los pesos por label.\n",
    "\n",
    "Con `class_weight = 'balanced'` los pesos asignados son la inversa de la frecuencia de cada label.\n",
    "\n",
    "Es otra forma de balancear las clases, sin hacer resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8ZXAmXcK8QR",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight = 'balanced',penalty='none',random_state=123,max_iter=500)\n",
    "\n",
    "scaler=StandardScaler()\n",
    "\n",
    "X_train_sc=scaler.fit_transform(X_train);\n",
    "X_test_sc=scaler.transform(X_test);\n",
    "\n",
    "model.fit(X_train_sc,y_train)\n",
    "y_pred = model.predict(X_test_sc)\n",
    "mostrar_resultados(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBiqvT9QK8QQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resumen\n",
    "\n",
    "---\n",
    "Los modelos de Machine Learning generalmente están sesgados hacia la clase mayoritaria. Y en general requerimos predecir correctamente la clase minoritaria.\n",
    "\n",
    "Resolver esta problemática es relevante, pues se presentan en muchas situaciones de la vida real: análisis financieros (fraudes, Churn, Default), análisis biomédicos (outliers), seguridad (intrusión), etc.\n",
    "\n",
    "Todas las técnicas que vimos ayudan a *mejorar el recall*. Debemos probarlas, modificar sus parámetros, para llegar a la mejor performance para el dataset que estamos trabajando.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_005_tabla.JPG\" alt=\"tabla\" width=100% height=80% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> La maldición de la dimensionalidad\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "A priori, podemos pensar que una mayor cantidad de features mejora los modelos, pero no necesariamente es así.\n",
    "\n",
    "Definimos a un dataset como de **alta dimensionalidad**, si tiene un número de features del orden de *cientos o más*. \n",
    "\n",
    "En general, al agregar features *se mejora hasta cierto punto o umbral*, a partir del cual el clasificador comienza a empeorar su performance.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_006_grafico_alta_dim.png\" alt=\"grafico_alta_dim\" width=40% height=35% />\n",
    "\n",
    "La frase, atribuida a Richard Bellman, **La maldición de la dimensionalidad** refiere a los problemas de entrenamiento de modelos debidos a una alta dimensionalidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "---\n",
    "\n",
    "Veamos con un simple ejemplo el efecto de agregar dimensiones a un dataset que **no le incrementamos la cantidad de observaciones**.\n",
    "\n",
    "Supongamos que tenemos diez imágenes de gatos y perros usando el modelo de color RGB, que los representa con los colores *Rojo, Verde y Azul*.\n",
    "\n",
    "Y tenemos que construir un clasificador en base a estas imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si usamos solo el *feature “rojo”* nuestro clasificador no funcionará bien.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_007_ejemplo.png\" alt=\"grafico_alta_dim\" width=30% height=25% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Agreguemos la *feature “verde”* y las observaciones se empiezan a separar.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_008_ejemplo.png\" alt=\"grafico_alta_dim\" width=30% height=25% />\n",
    "\n",
    "Consideremos la *tercera feature “azul”*. Logramos mayor dispersión.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_009_ejemplo.png\" alt=\"grafico_alta_dim\" width=30% height=25% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora podemos encontrar un plano que separa perfectamente gatos y perros y que es la combinación lineal de “rojo”, “verde” y “azul”.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_010_ejemplo.png\" alt=\"grafico_alta_dim\" width=30% height=25% />\n",
    "\n",
    "Hasta acá parece que agregar features mejora la capacidad de clasificación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "---\n",
    "Al agregar features, la dimensionalidad del espacio de predictores se incrementa y **los datos se hacen cada vez más dispersos (sparse)**.\n",
    "\n",
    "Es cada vez *más fácil encontrar un hiperplano que separe a las clases*, porque la probabilidad de encontrar un dato mal clasificado (del lado equivocado del hiperplano) se hace infinitamente pequeña.\n",
    "\n",
    "**Y corremos el riesgo de caer en el overfitting**.\n",
    "\n",
    "En este ejemplo, un clasificador lineal en dos dimensiones, **es más simple y generaliza mejor** que el clasificador lineal en tres dimensiones.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_011_ejemplo.png\" alt=\"grafico_alta_dim\" width=30% height=25% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Proporción de outliers\n",
    "\n",
    "---\n",
    "Otro problema es que, al crecer la dimensionalidad del problema, cada vez más casos se ubican en los extremos del espacio.\n",
    "\n",
    "El número de outliers aumenta porque, *al agregar dimensiones, aumenta la probabilidad* de que <b>al menos en una dimensión </b> la variable tome un valor extremo.\n",
    "\n",
    "Al separarse los puntos en el espacio de muchas dimensiones, **se puede perder la estructura que buscamos encontrar en un espacio demasiado grande**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "---\n",
    "Vamos a comprobarlo:\n",
    "\n",
    "Dada una variable distribuida uniformemente en un hipercubo de $d$ dimensiones, *veamos como crecen los outliers*.\n",
    "\n",
    "Los definimos como aquellos puntos que toman valores extremos en alguna de las $d$ dimensiones. Es decir, si **alguna de sus coordenadas** está en el percentil 1 o 99 del eje.\n",
    "\n",
    "Con dos dimensiones, creamos un marco cuadrado. Encontramos unos pocos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N=500;\n",
    "\n",
    "X=np.random.uniform(size=(N,2)); # generamos 500 puntos en un espacio de dos dimensiones, distribuidos uniformemente\n",
    "\n",
    "plt.figure(figsize=(5, 3)); plt.plot(X[:,0],X[:,1],'o',ms=2);x = [0.0,1.0]\n",
    "p = 0.01 # Si alguna de las coordenadas está en el percentil 1 o 99 del eje, lo consideramos un outlier\n",
    "\n",
    "plt.fill_between(x,0,p,alpha=0.2,color='k'); plt.fill_between(x,1-p,1,alpha=0.2,color='k')\n",
    "plt.fill_betweenx(x,0,p,alpha=0.2,color='k'); plt.fill_betweenx(x,1-p,1,alpha=0.2,color='k',label='Region de outliers');\n",
    "plt.legend(loc=(0,1)); plt.show()\n",
    "\n",
    "n_outliers=np.sum(np.sum((X <p) |( X >(1-p)),axis=1));\n",
    "print('Número de outliers:',n_outliers, ' de ',N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "---\n",
    "Aumentando la dimensionalidad del dataset, *para un número fijo de casos*, vemos **como crece la proporción de outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N=1000; p=0.01; Ds=np.arange(1,200);\n",
    "p_outliers=[];\n",
    "for d in Ds:\n",
    "    X=np.random.uniform(size=(N,d));\n",
    "    p_outliers.append(np.mean(np.any((X <p) |( X >(1-p)),axis=1)))\n",
    "\n",
    "plt.figure(figsize=(6, 4)); plt.plot(Ds, p_outliers); \n",
    "plt.ylim([0,1]); plt.ylabel('Proporción de outliers'); plt.xlabel('Dimensiones');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Feature Selection\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "Dado el punto anterior, como podemos resolver los problemas cuando tenemos alta dimensionalidad?\n",
    "\n",
    "Evidentemente, **debemos reducir el número de features**.\n",
    "\n",
    "¿Pero cómo seleccionamos el subset de features que optimiza la performance de nuestro modelo?\n",
    "\n",
    "Un enfoque de *fuerza bruta* haría una búsqueda exhaustiva de todas las combinaciones posibles de features para encontrar el mejor conjunto. *Impracticable en términos computacionales*.\n",
    "\n",
    "**Feature Selection** busca identificar y seleccionar las variables más relevantes para el entrenamiento de los modelos. \n",
    "\n",
    "También se puede ver como el proceso enfocado en *remover las variables que no traen información o son redundantes*.\n",
    "\n",
    "Veamos algunos <a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#\">métodos implementados en Scikit-Learn</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "---\n",
    "Vamos a trabajar con un archivo con predicciones de supervivencia de pacientes que tuvieron ataques cardiacos previamente.\n",
    "\n",
    "- Tenemos variables categóricas que indican factores de riesgo (1): *anemia, diabetes, high_blood_pressure, smoking*.\n",
    "\n",
    "- *sex*, indica 0 - mujer, 1 - hombre. *age*, la edad del paciente.\n",
    "\n",
    "- *creatinine_phosphokinase, platelets, serum_creatinine, serum_sodium* son medidas de componentes de la sangre.\n",
    "\n",
    "- *ejection_fraction* mide como trabaja el corazón. *time*, los días de seguimiento.\n",
    "\n",
    "- *DEATH_EVENT* es la variable target donde 0 - indica supervivencia, 1 - falleció.\n",
    "\n",
    "Para más detalles ver <a href=\"https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\">aquí</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2PHSRoPK8OK",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,plot_confusion_matrix,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_heart = pd.read_csv('../Data/heart_failure_clinical_records_dataset.csv')\n",
    "print('Total de filas: ',df_heart.shape[0],'Total de columnas: ',df_heart.shape[1])\n",
    "df_heart.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter Methods\n",
    "\n",
    "---\n",
    "Buscan rankear las features en función de su **\"importancia”**.\n",
    "\n",
    "Habitualmente, se define un *umbral* para los scores, por debajo del cual las variables son consideradas poco relevantes y se filtran.\n",
    "\n",
    "Los métodos varían de acuerdo a *como miden la relevancia* de las features. \n",
    "\n",
    "Sólo **después** de encontrar las mejores features, se puede generar los modelos.\n",
    "\n",
    "Veremos algunas implementaciones en <a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#\">Scikit-Learn</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Features con baja varianza\n",
    "\n",
    "---\n",
    "La clase <a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\">VarianceThreshold</a> remueve las features que **no aporten información a nuestro dataset**, calculando su varianza.\n",
    "\n",
    "Es decir, se filtran todos las features que tengan una varianza (variabilidad) menor a un `threshold` (umbral) definido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "Vamos a aplicarlo a nuestro dataset.\n",
    "\n",
    "Calculamos primero la varianza de cada feature, para validarlo después con el método. Las primeras siete features superan un valor de 0.5, es cual será el umbral que usaremos con `VarianceThreshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ud7e8YjFdvT5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "df_heart.apply(np.var).sort_values(ascending=False)[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ajustamos el dataset a la clase, con el hiperparámetro `threshold` = 0.5.\n",
    "\n",
    "`fit_transform` devuelve un array donde las columnas son solo las features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WQJ4aiyodvT_",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "vt = VarianceThreshold(threshold=0.5) # Instanciamos la clase con un threshold=0.5\n",
    "df_heart_vt = vt.fit_transform(df_heart)\n",
    "df_heart_vt[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El método `get_support()` devuelve una máscara con las features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WQJ4aiyodvT_",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vt.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si lo aplicamos al dataset podemos conocer cuales son."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WQJ4aiyodvT_",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_heart.columns[vt.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finalmente, podemos reconstruir un dataframe solo con las features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BtolyYwgdvUQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_heart_reduced_vt = pd.DataFrame(df_heart_vt, columns = df_heart.columns[vt.get_support()])\n",
    "df_heart_reduced_vt.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Selección de features univariados \n",
    "\n",
    "---\n",
    "Los métodos de *Univariate feature selection* realizan un ranking de las features a partir de tests estadísticos univariados.\n",
    "\n",
    "Analizan sus propiedades estadísticas y *cuan fuerte es su relación con la variable target*. \n",
    "\n",
    "Son *univariados* pues no contemplan *interacción entre features*. Se evalúa una variable a la vez con la clase.\n",
    "\n",
    "Los tests estadísticos más usados son:\n",
    "- Pearson’s Correlation Coefficient. Llamado como la función `f_regression()`\n",
    "- ANOVA: `f_classif()`\n",
    "- Chi-Squared: `chi2()`\n",
    "- Mutual Information: `mutual_info_classif()` and `mutual_info_regression()`\n",
    "\n",
    "Veamos dos métodos que implementa <a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\">Scikit-learn</a>:\n",
    "\n",
    "- SelectKBest.\n",
    "- SelectPercentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SelectKBest \n",
    "\n",
    "---\n",
    "La clase <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\">SelectKBest</a> usa una suite de test estadísticos para seleccionar las features más significativas.\n",
    "\n",
    "Necesita dos hiperparámetros:\n",
    "\n",
    "* `score_func`: una función que devuelve algún score entre $X$ (las features) e $Y$ (la variable target).\n",
    "\n",
    "* `k`: la cantidad de \"mejores\" features que deben ser seleccionadas.\n",
    "\n",
    "Como vimos, las funciones implementan los tests estadísticos. Para clasificación son f_classif, mutual_info_classif, chi2. Para regresión, f_regression, mutual_info_regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo \n",
    "\n",
    "---\n",
    "Vamos a usar el *test estadístico Chi-cuadrado* para seleccionar las 10 mejores features del dataset.\n",
    "* `score_func` = chi2.\n",
    "* `k` = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Previamente generamos la matriz de features y la variable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = df_heart.drop('DEATH_EVENT',axis=1)\n",
    "y = df_heart['DEATH_EVENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El método `fit_transform` aplica el test estadístico y selecciona las 10 features más relevantes, devolviéndolas como un array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "bestfeatures_k = SelectKBest(score_func=chi2, k=10)\n",
    "fit_k = bestfeatures_k.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reconstruimos un dataframe con las 10 features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BtolyYwgdvUQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_heart_reduced_k = pd.DataFrame(fit_k, columns = X.columns[bestfeatures_k.get_support()])\n",
    "df_heart_reduced_k.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Se puede ver el score de cada feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfscores = pd.DataFrame(bestfeatures_k.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "scores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "scores.columns = ['Feature','Score']\n",
    "\n",
    "print(scores.nlargest(10,'Score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SelectPercentile\n",
    "\n",
    "---\n",
    "La clase <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile\">SelectPercentile</a> es similar a `SelectKBest`, pero selecciona los features en base a los percentiles de los scores. Por ejemplo, los que se encuentran en el mejor 20%.\n",
    "\n",
    "Necesita dos hiperparámetros, el primero similar a `SelectKBest`:\n",
    "\n",
    "* `score_func`: una función que devuelve algún score entre $X$ (las features) e $Y$ (la variable target).\n",
    "\n",
    "* `percentile`: porcentaje de features a considerar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Repetimos el ejemplo, para un 20%. \n",
    "* `score_func` = chi2.\n",
    "* `percentile` = 20.\n",
    "\n",
    "El método `fit_transform` aplica el test estadístico y selecciona solo el 20% más relevante de features. En nuestro ejemplo son tres features de las 12 del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "bestfeatures_p = SelectPercentile(chi2, percentile=20)\n",
    "fit_p = bestfeatures_p.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El método `get_support()` devuelve una máscara con las features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bestfeatures_p.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reconstruimos un dataframe solo con las features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BtolyYwgdvUQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_heart_reduced_p = pd.DataFrame(fit_p, columns = X.columns[bestfeatures_p.get_support()])\n",
    "df_heart_reduced_p.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Wrapper methods\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_separador.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wrapper methods\n",
    "\n",
    "---\n",
    "Los métodos seleccionan subconjuntos de features *según la performance* que obtienen al ajustarlos sobre un modelo.\n",
    "\n",
    "Dicho de otra forma, aplica un algoritmo sobre un subconjunto de variables, y de acuerdo a la performance obtenida, *remueve o agrega features*. Luego **vuelve a aplicar recursivamente el algoritmo** hasta llegar a un criterio de parada.\n",
    "\n",
    "Si bien es un proceso más costoso en términos computacionales, **son más exactos** que los *Filter methods*, ya que se evalúan sobre un modelo real. \n",
    "\n",
    "Wrapper methods consideran a la selección del conjunto de features como un problema de búsqueda, donde se aplican distintas estrategias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Diferencias entre ambos tipos de métodos\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_013_diferencias.JPG\" alt=\"diferencias\" width=70% height=60% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eliminación Recursiva de Features (RFE)\n",
    "\n",
    "---\n",
    "*Recursive feature elimination* entrena un **estimador** sobre el total de features y calcula la importancia de cada feature. La feature menos importante *se elimina del set* y se vuelve a entrenar con los restantes. \n",
    "\n",
    "El proceso se repite de forma recursiva hasta que se llega al **número de features definido previamente**.\n",
    "\n",
    "El método `RFE()` tiene los argumentos:\n",
    "\n",
    "* `estimator`: el estimador usado para entrenar y evaluar. Puede ser cualquier algoritmo supervisado que devuelva la importancia de cada feature.\n",
    "* `n_features_to_select`: la cantidad de features final.\n",
    "* `steps`: features que se eliminan por iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Apliquemos el método sobre el dataset de vinos, que tiene como features sus propiedades químicas, y como variable target una clasificación en tres tipos de vinos (0,1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(as_frame=True, return_X_y=True)\n",
    "print('Total de filas: ',X.shape[0],'Total de columnas: ',X.shape[1])\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Estandarizamos las variables. Pero no hace falta dividir en train y test, ya que usamos el algoritmo para validar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()    \n",
    "X_sc = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usamos como clasificador un árbol de decisión.\n",
    "\n",
    "Le informamos, con el hiperparámetro `n_features_to_select` que nos quedamos con 6 features. Y con `step` le decimos que elimine de a uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "rfe = RFE(estimator, n_features_to_select=6, step = 1)\n",
    "rfe.fit(X_sc, y)\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "scores[\"Attribute Name\"] = X.columns; scores[\"Ranking\"] = rfe.ranking_; scores[\"Support\"] = rfe.support_\n",
    "\n",
    "print(scores.sort_values('Ranking'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eliminación Recursiva de Features con Cross Validation (RFECV)\n",
    "\n",
    "---\n",
    "*Recursive feature elimination with cross-validation* es similar a *RFE*, pero **determina la cantidad de features a seleccionar usando CrossValidation**.\n",
    "\n",
    "Los argumentos son similares, pero *ya no informamos cuantas variables finales queremos*. Y por otra parte, le tenemos que decir como hacemos el CV. \n",
    "\n",
    "El método `RFECV()` tiene los argumentos:\n",
    "\n",
    "* `estimator`: el estimador usado para entrenar y evaluar. Puede ser cualquier algoritmo supervisado que devuelva la importancia de cada feature.\n",
    "* `cv`: determina el método de validación cruzada.\n",
    "* `steps`: features que se eliminan por iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "rfecv = RFECV(estimator, cv=kf, step = 1)\n",
    "rfecv.fit(X_sc, y)\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "scores[\"Attribute Name\"] = X.columns; scores[\"Ranking\"] = rfecv.ranking_; scores[\"Support\"] = rfecv.support_\n",
    "\n",
    "print(scores.sort_values('Ranking'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Algoritmos genéticos\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducción\n",
    "\n",
    "---\n",
    "Fueron creados por John Holland en los años 1970. Son llamados así porque se inspiran en la evolución biológica y su base genético-molecular.\n",
    "\n",
    "Básicamente es un método de búsqueda y optimización.\n",
    "\n",
    "Donde el espacio de búsqueda de un problema determinado está formado por todas las posibles soluciones a dicho problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tomemos como ejemplo:\n",
    "\n",
    "El problema del viajante (TSP por sus siglas en inglés Travelling Salesman Problem) responde a la siguiente pregunta: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Dada una lista de ciudades y las distancias entre cada par de ellas, ¿cuál es la ruta más corta posible que visita cada ciudad exactamente una vez y al finalizar regresa a la ciudad origen?**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_014_ag_ejemplo.JPG\" alt=\"ag_ejemplo\" width=50% height=40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "donde tenemos soluciones malas y buenas.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_015_ag_ejemplo.JPG\" alt=\"ag_ejemplo\" width=70% height=60% />\n",
    "\n",
    "El espacio de búsqueda *son todos los posibles caminos sobre el grafo*, que pasen, para simplificar, solo una vez por cada ciudad. La cantidad de caminos es el factorial de 6 (6! = 720).\n",
    "\n",
    "Podemos decir que cada camino es **una solución dentro del espacio total de soluciones**.\n",
    "\n",
    "Y cada camino se puede representar como un vector de 6 elementos, donde cada uno de ellos representa una ciudad.\n",
    "\n",
    "$(x_1,x_2,x_3,x_4,x_5,x_6)$ donde $x_i$ es una ciudad entre 1 y 6, y $x_i \\ne x_j$.\n",
    "\n",
    "Entonces $(1,2,3,4,5,6)$ es el camino óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algoritmo\n",
    "\n",
    "---\n",
    "Los algoritmos genéticos trabajan sobre una población de individuos. *Cada individuo es una solución dentro del espacio de soluciones*.\n",
    "\n",
    "Evolucionan la población (se mueven de un conjunto de individuos a otros) aplicando acciones semejantes a las que actúan en la evolución biológica. *\"El proceso de selección natural”*.\n",
    "\n",
    "Cada iteración o **generación** encuentra nuevos individuos \"más aptos\".\n",
    "\n",
    "El algoritmo termina cuando encuentra la mejor solución, *los mejores individuos*, **no siempre los óptimos**, o bien luego de un número de generaciones.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_016_ag_espacio_sol.JPG\" alt=\"ag_espacio_sol\" width=60% height=45% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cada individuo de la población está representado por un **cromosoma**, *un vector*, compuesto por elementos a los que llamamos **genes**.\n",
    "\n",
    "Las diferentes formas (valores) que puede tomar un gen se denomina **alelo**. \n",
    "\n",
    "La posición física que ocupa cada gen dentro del cromosoma se denomina **locus**.\n",
    "\n",
    "Todos los cromosomas conforman la **población**, o dicho de otra forma, *el espacio de soluciones*.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_017_ag_individuo.png\" alt=\"ag_individuo\" width=60% height=45% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Como representamos nuestro problema de seleccionar las mejores features?**\n",
    "\n",
    "Podemos asumir que *cada gen indica si está presente la feature i*, con un *alelo* de 0 y 1.\n",
    "\n",
    "De modo que el *cromosoma es un vector de 13 genes*, cada uno indicando si contiene o no una de las features de la matriz de features del dataset de vinos.\n",
    "\n",
    "La población es el conjunto de cromosomas, es decir un conjunto de $2^{13}$ elementos.\n",
    "\n",
    "Viendo el enorme volumen de soluciones, los algoritmos genéticos nos brindan la posibilidad de **una búsqueda heurística** de la mejor solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fases\n",
    "\n",
    "---\n",
    "Veamos la **\"evolución\"** de los individuos, hasta llegar a los  *más aptos*.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_018_ag_circuito.png\" alt=\"ag_circuito\" width=90% height=65% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Población inicial**: es un conjunto de individuos de tamaño **size** determinado, seleccionados al azar.\n",
    "\n",
    "- **Fitness**: es una *medida de performance* del modelo cuando lo ajustamos a los individuos.\n",
    "\n",
    "  Puede ser accuracy para modelos de clasificación,  𝑟2  para regresión, etc.\n",
    "\n",
    "  En general, la función de fitness determina *qué tan apto* es un individuo.\n",
    "  \n",
    "  Nos quedamos con los **n_best cromosomas**. \n",
    "  \n",
    "- **Criterio de terminación**: genera una medida del fitness de los cromosomas, un **score**. \n",
    "  Es una *medida de error*; si es menor a cierto threshold (umbral) finaliza el algoritmo.\n",
    "  \n",
    "  También puede ser *un número final de iteraciones alcanzadas*.\n",
    "\n",
    "- **Selección**: a los *n_best cromosomas*, le sumamos *n_rand* cromosomas al azar.\n",
    "  \n",
    "  Luego algunos se seleccionan para generar descendencia, y otros siguen a la etapa de mutación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Pairing, Mating**: selecciona parejas de cromosomas para \"aparearlas\", y generar una nueva descendencia de *n_children* cromosomas.\n",
    "\n",
    "  Se realiza mediante **crossover**, combinando genes de ambos individuos.\n",
    "  \n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_019_ag_crossover.png\" alt=\"ag_crossover\" width=60% height=45% />\n",
    "\n",
    "- **Mutation**: un porcentaje *cambia el valor en algunos de sus genes*.\n",
    "  \n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_020_ag_mutation.png\" alt=\"ag_mutation\" width=60% height=45% />\n",
    "\n",
    "- **Nueva generación**: los nuevos cromosomas generados se evalúan y se seleccionan nuevamente los n_best cromosomas. Se llama a este proceso *elitismo*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementación\n",
    "\n",
    "---\n",
    "Python no dispone de una librería para los algoritmos genéticos, pero vamos a usar una implementación simple del algoritmo. El código fuente se encuentra en \n",
    "[acá](https://github.com/dawidkopczyk/genetic/blob/master/genetic.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Notebooks/Genetic.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Analicemos ahora el problema de clasificación de vinos usando algoritmos genéticos.\n",
    "\n",
    "Medimos la performance del modelo de clasificación con todas las features, antes de comenzar con el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "est = DecisionTreeClassifier()\n",
    "cv_param = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "score = cross_val_score(est, X_sc, y, cv=cv_param, scoring='balanced_accuracy')\n",
    "print(\"Performance con todas las features: {:.2f}\".format(np.mean(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La implementación tiene los parámetros:\n",
    "\n",
    "* *estimator*: el estimador usado para calcular el fitness.\n",
    "* *n_gen*: el número máximo de generaciones (número de iteraciones del algoritmo).\n",
    "* *size*: tamaño de la población a considerar.\n",
    "* *n_best*: en cada iteración se seleccionan los n_best cromosomas.\n",
    "* *n_rand*: además de los n_best, se seleccionan n_rand cromosomas al azar.\n",
    "* *n_children*: número de nuevos cromosomas que se generan en cada apareamiento.\n",
    "* *mutation_rate*: probabilidad de cambiar cada gen espontáneamente.\n",
    "* *cv*: default 5, argumento para la función cross_val_score de sklearn.\n",
    "* *scoring*: medida de fitness de los cromosomas. Por ejemplo \"neg_mean_squared_error\" para modelos de regresión, o \"accuracy\" para clasificación. Dentro de la clase se multiplica la medida por -1 convirtiéndola en una medida de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sel = GeneticSelector(estimator = DecisionTreeClassifier(), \n",
    "                      n_gen = 15, size = 200, n_best = 40, \n",
    "                      n_rand = 40, n_children = 5, mutation_rate = 0.05,\n",
    "                     scoring = 'balanced_accuracy')\n",
    "sel.fit(X_sc, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementación\n",
    "\n",
    "---\n",
    "El gráfico nos muestra como fue bajando el error a medida que itera el algoritmo, es decir, que crea nuevas generaciones.\n",
    "\n",
    "*Average* es el promedio de los scores de los cromosomas de la generación. \n",
    "\n",
    "*Best* es el mejor score de cada generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sel.plot_scores()\n",
    "score = cross_val_score(est, X_sc[:,sel.support_], y, cv = cv_param, scoring='balanced_accuracy')\n",
    "print(\"Performance con las features seleccionadas: {:.2f}\".format(np.mean(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos finalmente cuales son las features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "scores[\"Attribute Name\"] = X.columns; scores[\"Support\"] = sel.support_\n",
    "print(scores.sort_values('Support', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ventajas y desventajas\n",
    "\n",
    "---\n",
    "*Ventajas* \n",
    "\n",
    "- Más rápido y eficiente que los métodos tradicionales.\n",
    "- Permite procesamiento paralelo.\n",
    "- Genera una lista de \"buenas\" soluciones.\n",
    "- Util para grandes espacios de soluciones.\n",
    "\n",
    "*Desventajas*\n",
    "- No es apropiado para problemas simples.\n",
    "- No garantiza llegar a la solución óptima.\n",
    "- La evaluación frecuente del fitness lo hace computacionalmente caro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "---\n",
    "\n",
    "* Los datasets con *clases desbalanceadas* se presentan en muchas situaciones de la vida real. Tenemos que transformarlos para que los algoritmos de ML funcionen correctamente.\n",
    "\n",
    "* *La maldición de la dimensionalidad* se refiere a los problemas que tienen los modelos para entrenar cuando tenemos alta dimensionalidad.\n",
    "\n",
    "* La reducción de features en los datasets nos lleva a los métodos de *features selection*. \n",
    "\n",
    "* Tenemos los *filter methods* que buscan rankear las features en función de un valor de importancia. Habitualmente, se define un umbral para los scores, por debajo del cual las variables son consideradas poco relevantes y se filtran.\n",
    "\n",
    "* Los *wrapper methods* seleccionan subconjuntos de features según la performance que obtienen al ajustarlos sobre un modelo.\n",
    "\n",
    "* Podemos adaptar los *algoritmos genéticos* para seleccionar las mejores features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "----\n",
    "\n",
    "A partir del dataset de vinos, apliquemos los *filter methods*:\n",
    "\n",
    "- SelectKBest, usando el *test estadístico Chi-cuadrado* para seleccionar las 6 mejores features del dataset.\n",
    "\n",
    "- SelectPercentile, para seleccionar las features que se encuentran en el mejor 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(as_frame=True, return_X_y=True)\n",
    "print('Total de filas: ',X.shape[0],'Total de columnas: ',X.shape[1])\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solución\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "----\n",
    "\n",
    "A partir del dataset de vinos, apliquemos los *filter methods*:\n",
    "\n",
    "- SelectKBest, usando el *test estadístico Chi-cuadrado* para seleccionar las 6 mejores features del dataset.\n",
    "\n",
    "- SelectPercentile, para seleccionar las features que se encuentran en el mejor 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(as_frame=True, return_X_y=True)\n",
    "print('Total de filas: ',X.shape[0],'Total de columnas: ',X.shape[1])\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2PHSRoPK8OK",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SelectKBest\n",
    "\n",
    "----\n",
    "Vamos a usar el *test estadístico Chi-cuadrado* para seleccionar las 6 mejores features del dataset.\n",
    "\n",
    "* `score_func` = chi2.\n",
    "* `k` = 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El método `fit_transform` aplica el test estadístico y selecciona las 6 features más relevantes. Las deja como un array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "bestfeatures_k = SelectKBest(score_func=chi2, k=6)\n",
    "fit_k = bestfeatures_k.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reconstruimos un dataframe con las 6 features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BtolyYwgdvUQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced_k = pd.DataFrame(fit_k, columns = X.columns[bestfeatures_k.get_support()])\n",
    "X_reduced_k.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SelectPercentile\n",
    "\n",
    "----\n",
    "Usamos los siguientes parámetros:\n",
    "\n",
    "* `score_func` = chi2.\n",
    "* `percentile` = 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El método `fit_transform` aplica el test estadístico y selecciona solo el 25% más relevante de features. \n",
    "\n",
    "En nuestro ejemplo son tres features de las 12 del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "bestfeatures_p = SelectPercentile(chi2, percentile=20)\n",
    "fit_p = bestfeatures_p.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reconstruimos un dataframe solo con las features seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BtolyYwgdvUQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced_p = pd.DataFrame(fit_p, columns = X.columns[bestfeatures_p.get_support()])\n",
    "X_reduced_p.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias y Material Adicional\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_40_Feature_Selection_Desbalance_Clases/Presentacion/img/M5_CLASE_40_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Referencias y Material Adicional\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://imbalanced-learn.org/stable/index.html\" target=\"_blank\">Imbalanced learn documentation</a>\n",
    "\n",
    "<a href=\"https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\" target=\"_blank\">Tactics to combat imbalanced classes</a>\n",
    "\n",
    "<a href=\"https://www.aprendemachinelearning.com/clasificacion-con-datos-desbalanceados/\" target=\"_blank\">Clasificación con datos desbalanceados</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5\" target=\"_blank\">5  SMOTE techniques for oversampling your imbalance data</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\" target=\"_blank\">Feature selection techniques in machine learning with Python</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2\" target=\"_blank\">The 5 Feature Selection Algorithms every Data Scientist should know</a>\n",
    "\n",
    "<a href=\"https://medium.com/@rinu.gour123/python-genetic-algorithms-with-artificial-intelligence-b8d0c7db60ac\" target=\"_blank\">Python Genetic Algorithms With Artificial Intelligence</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
